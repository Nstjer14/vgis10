Automatically generated by Mendeley Desktop 1.19.5
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Pathak2016a,
author = {Pathak, Mrunal and Srinivasu, N},
doi = {10.1007/978-981-10-2630-0},
file = {::},
isbn = {978-981-10-2629-4},
keywords = {biometrics},
pages = {137--152},
title = {{Advances in Computing Applications}},
url = {http://link.springer.com/10.1007/978-981-10-2630-0},
year = {2016}
}
@article{Kumar2016a,
abstract = {We studied the fusion of three biometric authentication modalities, namely, swiping gestures, typing patterns and the phone movement patterns observed during typing or swiping. A web browser was customized to collect the data generated from the aforementioned modalities over four to seven days in an unconstrained environment. Several features were extracted by using sliding window mechanism for each modality and analyzed by using information gain, correlation, and symmetric uncertainty. Finally, five features from windows of continuous swipes, thirty features from windows of continuously typed letters, and nine features from corresponding phone movement patterns while swiping/typing were used to build the authentication system. We evaluated the performance of each modality and their fusion over a dataset of 28 users. The feature-level fusion of swiping and the corresponding phone movement patterns achieved an authentication accuracy of 93.33{\%}, whereas, the score-level fusion of typing behaviors and the corresponding phone movement patterns achieved an authentication accuracy of 89.31{\%}. 1.},
author = {Kumar, Rajesh and Phoha, Vir V. and Serwadda, Abdul},
doi = {10.1109/BTAS.2016.7791164},
file = {::},
isbn = {9781467397339},
journal = {2016 IEEE 8th International Conference on Biometrics Theory, Applications and Systems, BTAS 2016},
title = {{Continuous authentication of smartphone users by fusing typing, swiping, and phone movement patterns}},
year = {2016}
}
@inproceedings{Yin2011,
abstract = {In this paper, the acquisition and content of a new homologous multimodal biometric database are presented. The SDUMLA-HMT database consists of face images from 7 view angles, finger vein images of 6 fingers, gait videos from 6 view angles, iris images from an iris sensor, and fingerprint images acquired with 5 different sensors. The database includes real multimodal data from 106 individuals. In addition to database description, we also present possible use of the database. The database is available to research community through http://mla.sdu.edu.cn/sdumla-hmt.html .},
author = {Yin, Yilong and Liu, Lili and Sun, Xiwei},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-25449-9_33},
file = {::},
isbn = {9783642254482},
issn = {03029743},
keywords = {Biometrics,Face,Finger vein,Fingerprint,Gait,Homologous,Iris,Multi-modal},
pages = {260--268},
title = {{SDUMLA-HMT: A multimodal biometric database}},
volume = {7098 LNCS},
year = {2011}
}
@article{Sequeira2014,
abstract = {Biometrics represents a return to a natural way of identification: testing someone by what (s)he is, instead of relying on something (s)he owns or knows seems likely to be the way forward. Biometric systems that include multiple sources of information are known as multimodal. Such systems are generally regarded as an alternative to fight a variety of problems all unimodal systems stumble upon. One of the main challenges found in the development of biometric recognition systems is the shortage of publicly available databases acquired under real unconstrained working conditions. Motivated by such need the MobBIO database was created using an Asus EeePad Transformer tablet, with mobile biometric systems in mind. The proposed database is composed by three modalities: iris, face and voice.},
author = {Sequeira, Ana F. and Monteiro, Joao C. and Rebelo, Ana and Oliveira, Helder P.},
file = {::},
isbn = {9789897580093},
journal = {9th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
number = {c},
pages = {1--14},
title = {{MobBIO: A Multimodal Database Captured with a Portable Handheld Device}},
year = {2014}
}
@article{Zhao2017b,
abstract = {Person re-identification (ReID) is an important task in video surveillance and has various applications. It is non-trivial due to complex background clutters, varying illu-mination conditions, and uncontrollable camera settings. Moreover, the person body misalignment caused by detec-tors or pose variations is sometimes too severe for feature matching across images. In this study, we propose a novel Convolutional Neural Network (CNN), called Spindle Net, based on human body region guided multi-stage feature de-composition and tree-structured competitive feature fusion. It is the first time human body structure information is con-sidered in a CNN framework to facilitate feature learning. The proposed Spindle Net brings unique advantages: 1) it separately captures semantic features from different body regions thus the macro-and micro-body features can be well aligned across images, 2) the learned region features from different semantic regions are merged with a competitive scheme and discriminative features can be well preserved. State of the art performance can be achieved on multiple datasets by large margins. We further demonstrate the ro-bustness and effectiveness of the proposed Spindle Net on our proposed dataset SenseReID without fine-tuning.},
author = {Zhao, Haiyu and Tian, Maoqing and Sun, Shuyang and Shao, Jing and Yan, Junjie and Yi, Shuai and Wang, Xiaogang and Tang, Xiaoou},
doi = {10.1109/CVPR.2017.103},
file = {::},
isbn = {978-1-5386-0457-1},
journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {907--915},
title = {{Spindle Net: Person Re-identification with Human Body Region Guided Feature Decomposition and Fusion}},
url = {http://ieeexplore.ieee.org/document/8099586/},
volume = {1},
year = {2017}
}
@article{Dessimoz2007,
abstract = {The MBioID initiative has been set up to address the following germane question: What and how biometric technologies could be deployed in identity documents in the foreseeable future? This research effort proposes to look at current and future practices and systems of establishing and using biometric identity documents (IDs) and evaluate their effectiveness in large-scale developments. The first objective of the MBioID project is to present a review document establishing the current state-of-the-art related to the use of multimodal biometrics in an IDs application. This research report gives the main definitions, properties and the framework of use related to biometrics, an overview of the main standards developed in the biometric industry and standardisation organisations to ensure interoperability, as well as some of the legal framework and the issues associated to biometrics such as privacy and personal data protection. The state-of-the-art in terms of technological development is also summarised for a range of single biometric modalities (2D and 3D face, fingerprint, iris, on-line signature and speech), chosen according to ICAO recommendations and availabilities, and for various multimodal approaches. This paper gives a summary of the main elements of that report. The second objective of the MBioID project is to propose relevant acquisition and evaluation protocols for a large-scale deployment of biometric IDs. Combined with the protocols, a multimodal database will be acquired in a realistic way, in order to be as close as possible to a real biometric IDs deployment. In this paper, the issues and solutions related to the acquisition setup are briefly presented. {\textcopyright} 2006 Elsevier Ireland Ltd. All rights reserved.},
author = {Dessimoz, Damien and Richiardi, Jonas and Champod, Christophe and Drygajlo, Andrzej},
doi = {10.1016/j.forsciint.2006.06.037},
file = {::},
issn = {03790738},
journal = {Forensic Science International},
keywords = {Acquisition protocol,Biometrics,Electronic passport,Evaluation protocol,Identity documents,Multimodality},
number = {2-3},
pages = {154--159},
pmid = {16890391},
title = {{Multimodal biometrics for identity documents ({\{}A figure is presented{\}})}},
volume = {167},
year = {2007}
}
@article{Mellakh2009a,
author = {Mellakh, A and Chaari, A and Guerfi, S and Dhose, J and Colineau, J and Lelandais, S and Petrovska-Delacr{\`{e}}taz, D and Dorizzi, B},
doi = {10.1007/978-3-642-04697-1_3},
file = {::},
isbn = {03029743; 3642046967 (ISBN); 9783642046964 (ISBN)},
issn = {03029743},
journal = {11th International Conference on Advanced Concepts for Intelligent Vision Systems, ACIVS 2009},
keywords = {2D face recognition,Appearance based,Appearance-based algorithms,Computer vision,Database,Database systems,Discriminant analysis,Evaluation campaign,Experimental protocols,Face images,Face recognition,Linear discriminant analysis,Multi-modal,Multimodal database,Principal component analysis,Training sets},
pages = {24--32},
title = {{2D face recognition in the IV2 evaluation campaign}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-70549098348{\&}partnerID=40{\&}md5=0541be27bb29a037c965f0d644b15856},
volume = {5807 LNCS},
year = {2009}
}
@article{Cheng2017a,
abstract = {This paper focuses on indoor semantic segmentation us-ing RGB-D data. Although the commonly used deconvolu-tion networks (DeconvNet) have achieved impressive results on this task, we find there is still room for improvements in two aspects. One is about the boundary segmentation. DeconvNet aggregates large context to predict the label of each pixel, inherently limiting the segmentation precision of object boundaries. The other is about RGB-D fusion. Re-cent state-of-the-art methods generally fuse RGB and depth networks with equal-weight score fusion, regardless of the varying contributions of the two modalities on delineating different categories in different scenes. To address the two problems, we first propose a locality-sensitive DeconvNet (LS-DeconvNet) to refine the boundary segmentation over each modality. LS-DeconvNet incorporates locally visual and geometric cues from the raw RGB-D data into each DeconvNet, which is able to learn to upsample the coarse convolutional maps with large context whilst recovering sharp object boundaries. Towards RGB-D fusion, we introduce a gated fusion layer to effectively combine the two LS-DeconvNets. This layer can learn to adjust the contributions of RGB and depth over each pixel for high-performance object recognition. Experiments on the large-scale SUN RGB-D dataset and the popular NYU-Depth v2 dataset show that our approach achieves new state-of-the-art results for RGB-D indoor semantic segmentation.},
author = {Cheng, Yanhua and Cai, Rui and Li, Zhiwei and Zhao, Xin and Huang, Kaiqi},
doi = {10.1109/CVPR.2017.161},
file = {::},
isbn = {978-1-5386-0457-1},
journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {1475--1483},
title = {{Locality-Sensitive Deconvolution Networks with Gated Fusion for RGB-D Indoor Semantic Segmentation}},
url = {http://ieeexplore.ieee.org/document/8099644/},
year = {2017}
}
@article{Cheng2017a,
abstract = {Sea–land segmentation and ship detection are two prevalent research domains for optical remote sensing harbor images and can find many applications in harbor supervision and management. As the spatial resolution of imaging technology improves, traditional methods struggle to perform well due to the complicated appearance and background distributions. In this paper, we unify the above two tasks into a single framework and apply the deep convolutional neural networks to predict pixelwise label for an input. Specifically, an edge aware convolutional network is proposed to parse a remote sensing harbor image into three typical objects, e.g., sea, land, and ship. Two innovations are made on top of the deep structure. First, we design a multitask model by simultaneously training the segmentation and edge detection networks. Hierarchical semantic features from the segmentation network are extracted to learn the edge network. Second, the outputs of edge pipeline are further employed to refine entire model by adding an edge aware regularization, which helps our method to yield very desirable results that are spatially consistent and well boundary located. It also benefits the segmentation of docked ships that are quite challenging for many previous methods. Experimental results on two datasets collected from Google Earth have demonstrated the effectiveness of our approach both in quantitative and qualitative performance compared with state-of-the-art methods.},
author = {Cheng, Dongcai and Meng, Gaofeng and Xiang, Shiming and Pan, Chunhong},
doi = {10.1109/JSTARS.2017.2747599},
file = {::},
issn = {21511535},
journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
keywords = {Edge aware regularization,harbor images,multitask learning,semantic segmentation},
number = {12},
pages = {5769--5783},
title = {{FusionNet: Edge Aware Deep Convolutional Networks for Semantic Segmentation of Remote Sensing Harbor Images}},
volume = {10},
year = {2017}
}
@article{Chowdhury2016a,
author = {Chowdhury, Anurag and Ghosh, Soumyadeep and Singh, Richa and Vatsa, Mayank},
doi = {10.1109/BTAS.2016.7791199},
file = {::},
isbn = {9781467397339},
journal = {2016 IEEE 8th International Conference on Biometrics Theory, Applications and Systems, BTAS 2016},
title = {{RGB-D face recognition via learning-based reconstruction}},
year = {2016}
}
@article{Wan2017a,
abstract = {State-of-the-art methods for 3D hand pose estimation from depth images require large amounts of annotated training data. We propose to model the statistical relationships of 3D hand poses and corresponding depth images using two deep generative models with a shared latent space. By design, our architecture allows for learning from unlabeled image data in a semi-supervised manner. Assuming a one-to-one mapping between a pose and a depth map, any given point in the shared latent space can be projected into both a hand pose and a corresponding depth map. Regressing the hand pose can then be done by learning a discriminator to estimate the posterior of the latent pose given some depth map. To improve generalization and to better exploit unlabeled depth maps, we jointly train a generator and a discriminator. At each iteration, the generator is updated with the back-propagated gradient from the discriminator to synthesize realistic depth maps of the articulated hand, while the discriminator benefits from an augmented training set of synthesized and unlabeled samples. The proposed discriminator network architecture is highly efficient and runs at 90 FPS on the CPU with accuracies comparable or better than state-of-art on 3 publicly available benchmarks.},
archivePrefix = {arXiv},
arxivId = {1702.03431},
author = {Wan, Chengde and Probst, Thomas and {Van Gool}, Luc and Yao, Angela},
doi = {10.1109/CVPR.2017.132},
eprint = {1702.03431},
file = {::},
journal = {Cvpr2017},
pages = {10},
title = {{Crossing Nets: Dual Generative Models with a Shared Latent Space for Hand Pose Estimation}},
url = {http://arxiv.org/abs/1702.03431},
year = {2017}
}
@article{Ho1994,
abstract = {A multiple classifier system is a powerful solution to difficult pattern$\backslash$nrecognition problems involving large class sets and noisy input$\backslash$nbecause it allows simultaneous use of arbitrary feature descriptors$\backslash$nand classification procedures. Decisions by the classifiers can$\backslash$nbe represented as rankings of classifiers and different instances$\backslash$nof a problem. The rankings can be combined by methods that either$\backslash$nreduce or rerank a given set of classes. An intersection method$\backslash$nand union method are proposed for class set reduction. Three methods$\backslash$nbased on the highest rank, the Borda count, and logistic regression$\backslash$nare proposed for class set reranking. These methods have been tested$\backslash$nin applications of degraded machine-printed characters and works$\backslash$nfrom large lexicons, resulting in substantial improvement in overall$\backslash$ncorrectness.},
author = {Ho, Tin Kam and Hull, Jonathan J and Srihari, Sargur N},
doi = {http://dx.doi.org/10.1109/34.273716},
file = {::},
isbn = {0162-8828 VO - 16},
issn = {0162-8828},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
number = {1},
pages = {66--75},
title = {{Decision Combination in Multiple Classifier Systems}},
volume = {16},
year = {1994}
}
@article{Zapata2017a,
author = {Zapata, J C and Duque, C M and Gonzalez, M E},
doi = {10.1007/978-981-10-5427-3},
file = {::},
isbn = {978-981-10-5426-6},
keywords = {biometric {\'{a}} data fusion,signals {\'{a}} signal processing},
number = {i},
pages = {721--733},
title = {{Advances in Computing and Data Sciences}},
url = {http://link.springer.com/10.1007/978-981-10-5427-3},
volume = {721},
year = {2017}
}
@article{Liu2013a,
author = {Liu, Jing and Sun, Zhenan and Tan, Tieniu},
doi = {10.1109/BTAS.2013.6712692},
file = {::},
isbn = {9781479905270},
journal = {IEEE 6th International Conference on Biometrics: Theory, Applications and Systems, BTAS 2013},
pages = {1--6},
title = {{Code-level information fusion of low-resolution iris image sequences for personal identification at a distance}},
year = {2013}
}
@article{Kauba2016,
abstract = {—Authentication based on vein patterns is a very promising biometric technique. The most important step is the accurate extraction of the vein pattern from sometimes low quality input images. A single feature extraction technique may fail to correctly extract the vein pattern, entailing bad recognition performance. One of the solutions that can be used to improve recognition results is biometric fusion. A possible fusion strategy is feature level fusion, that is the fusion of several feature extractors' outputs. In our work, we exploited the feature level fusion to improve the quality of the extracted vein patterns and thus the feature extraction accuracy. An experimental study involving different feature extraction techniques (maximum curvature, repeated line tracking, wide line detector, ...) and different fusion techniques (majority voting, weighted average, STAPLE, ...) is conducted on the UTFVP finger-vein data set. The results show that feature level fusion is able to improve the recognition accuracy in terms of the EER over the single feature extraction techniques.},
author = {Kauba, Christof and Uhl, Andreas and Piciucco, Emanuela and Maiorana, Emanuele and Campisi, Patrizio},
doi = {10.1109/BIOSIG.2016.7736908},
file = {::},
isbn = {9783885796541},
issn = {16175468},
journal = {Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)},
title = {{Advanced variants of feature level fusion for finger vein recognition}},
volume = {P-260},
year = {2016}
}
@article{Wrobel2017a,
abstract = {{\textcopyright} 2017 IEEE. In this paper, a new approach for personal identity verification using finger knuckle images and least-square contour alignment method has been proposed. A special test rig with a digital camera was prepared for acquisition the knuckle images. Next, the obtained images of finger knuckle were subjected to image processing method in order to extract the knuckle furrows from them. The verification of person was performed by comparing the furrows on the verified and the reference knuckle images. To determine the similarity between the furrows we used the least-square contour alignment method. The usability of the proposed approach was tested experimentally. Practical experiments, conducted with our database, confirmed that results obtained are promising.},
author = {Wrobel, K. and Porwik, P. and Doroz, R. and Safaverdi, H.},
doi = {10.1109/ICBAKE.2017.8090616},
file = {::},
isbn = {9781538634004},
journal = {Proceedings of 2017 International Conference on Biometrics and Kansei Engineering, ICBAKE 2017},
keywords = {biometrics,finger knuckle images,least-square contour alignment,verification},
pages = {119--122},
title = {{Person verification based on finger knuckle images and least-squares contour alignment}},
year = {2017}
}
@article{Suk2014,
abstract = {For the last decade, it has been shown that neuroimaging can be a potential tool for the diagnosis of Alzheimer's Disease (AD) and its prodromal stage, Mild Cognitive Impairment (MCI), and also fusion of different modalities can further provide the complementary information to enhance diagnostic accuracy. Here, we focus on the problems of both feature representation and fusion of multimodal information from Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET). To our best knowledge, the previous methods in the literature mostly used hand-crafted features such as cortical thickness, gray matter densities from MRI, or voxel intensities from PET, and then combined these multimodal features by simply concatenating into a long vector or transforming into a higher-dimensional kernel space. In this paper, we propose a novel method for a high-level latent and shared feature representation from neuroimaging modalities via deep learning. Specifically, we use Deep Boltzmann Machine (DBM).22Although it is clear from the context that the acronym DBM denotes "Deep Boltzmann Machine" in this paper, we would clearly indicate that DBM here is not related to "Deformation Based Morphometry"., a deep network with a restricted Boltzmann machine as a building block, to find a latent hierarchical feature representation from a 3D patch, and then devise a systematic method for a joint feature representation from the paired patches of MRI and PET with a multimodal DBM. To validate the effectiveness of the proposed method, we performed experiments on ADNI dataset and compared with the state-of-the-art methods. In three binary classification problems of AD vs. healthy Normal Control (NC), MCI vs. NC, and MCI converter vs. MCI non-converter, we obtained the maximal accuracies of 95.35{\%}, 85.67{\%}, and 74.58{\%}, respectively, outperforming the competing methods. By visual inspection of the trained model, we observed that the proposed method could hierarchically discover the complex latent patterns inherent in both MRI and PET.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Suk, Heung Il and Lee, Seong Whan and Shen, Dinggang},
doi = {10.1016/j.neuroimage.2014.06.077},
eprint = {NIHMS150003},
file = {::},
isbn = {1053-8119},
issn = {10959572},
journal = {NeuroImage},
keywords = {Alzheimer's Disease,Deep boltzmann machine,Mild cognitive impairment,Multimodal data fusion,Shared feature representation},
pages = {569--582},
pmid = {25042445},
publisher = {Elsevier Inc.},
title = {{Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2014.06.077},
volume = {101},
year = {2014}
}
@incollection{Bowyer2016b,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
annote = {Read only chapter 17 about iris and face fusion. The article gives a nice and clean overview of different multi-biometric systems as well as levels of data abstraction the data fusion can be applied on. 

The work presented performs iris and face fusion using multi-sample, multi instance, and multimodal data. it fuses the multi sample, and multi instance together, by simply finding the best match in the variations. this is done as a score level fusion. and it fuses the two modalities by rank-level fusion using Broda count.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Connaughton, Ryan and Bowyer, Kevin W and Flynn, Patrick J},
booktitle = {Handbook of Iris Recognition},
doi = {10.1007/978-1-4471-6784-6},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {978-1-4471-6782-2},
issn = {16130073},
pages = {397--415},
pmid = {25246403},
title = {{Chapter 17 Fusion of Face and Iris Biometrics}},
url = {http://link.springer.com/10.1007/978-1-4471-6784-6},
year = {2016}
}
@article{Mhaske2013a,
author = {Mhaske, V. D. and Patankar, A. J.},
doi = {10.1109/ICCIC.2013.6724125},
file = {::},
isbn = {9781479915972},
journal = {2013 IEEE International Conference on Computational Intelligence and Computing Research, IEEE ICCIC 2013},
keywords = {Biometrics,Fingerprint,Fusion,MGF,Multimodal,Palm print,ROI,Unimodal},
title = {{Multimodal biometrics by integrating fingerprint and palmprint for security}},
year = {2013}
}
@article{Fierrez-Aguilar2003a,
abstract = {The aim of this paper, regarding multimodal biometric verification, is twofold: on the one hand, some score fusion strategies reported in the literature are reviewed and, on the other hand, we compare experimentally a selection of them using as monomodal baseline experts: i) our face verification system based on a global face appearance representation scheme, ii) our minutiaebased fingerprint verification system, and iii) our on-line signature verification system based on HMM modeling of temporal functions, on the MCYT multimodal database. A new strategy is also proposed and discussed in order to generate a multimodal combined score by means of Support Vector Machine (SVM) classifiers from which user-independent and user-dependent fusion schemes are derived and evaluated.},
author = {Fierrez-Aguilar, J and Ortega-Garcia, J and Garcia-Romero, D and Gonzalez-Rodriguez, J},
file = {::},
isbn = {3-540-40302-7},
issn = {03029743},
journal = {Proc. AVBPA},
pages = {1056},
title = {{A Comparative Evaluation of Fusion Strategies for Multimodal Biometric Verification}},
year = {2003}
}
@article{Wang2009a,
abstract = {Feature-level fusion remains a challenging problem for multimodal biometrics. However, existing fusion schemes such as sum rule and weighted sum rule are inefficient in complicated condition. In this paper, we propose an efficient feature-level fusion algorithm for iris and face in parallel. The algorithm first normalizes the original features of iris and face using z-score model, and then take complex FDA as the classifier of Unitary space. The proposed algorithm is tested using CASIA iris database and two face databases (ORL database and Yale database.). Experimental results show the effectiveness of the proposed algorithm.},
author = {Wang, Zhifang and Han, Qi and Niu, Xiamu and Busch, Christoph},
doi = {10.1007/978-3-642-01513-7_38},
file = {::},
isbn = {3642015123},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Biometrics,CFDA,Feature-level,Parallel fusion,Unitary space},
number = {PART 3},
pages = {356--364},
title = {{Feature-level fusion of Iris and face for personal identification}},
volume = {5553 LNCS},
year = {2009}
}
@article{Li2017,
abstract = {This paper investigates how to integrate the complementary information from RGB and thermal (RGB-T) sources for object tracking. We propose a novel Convolutional Neural Network (ConvNet) architecture, including a two-stream ConvNet and a FusionNet, to achieve adaptive fusion of different source data for robust RGB-T tracking. Both RGB and thermal streams extract generic semantic information of the target object. In particular, the thermal stream is pre-trained on the ImageNet dataset to encode rich semantic information, and then fine-tuned using thermal images to capture the specific properties of thermal information. For adaptive fusion of different modalities while avoiding redundant noises, the FusionNet is employed to select most discriminative feature maps from the outputs of the two-stream ConvNet, and updated online to adapt to appearance variations of the target object. Finally, the object locations are efficiently predicted by applying the multi-channel correlation filter on the fused feature maps. Extensive experiments on the recently public benchmark GTOT verify the effectiveness of the proposed approach against other state-of-the-art RGB-T trackers.},
author = {Li, Chenglong and Wu, Xiaohao and Zhao, Nan and Cao, Xiaochun and Tang, Jin},
doi = {10.1016/j.neucom.2017.11.068},
file = {::},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Adaptive fusion,Convolutional neural network,Correlation filter,Object tracking,Thermal information},
pages = {78--85},
publisher = {Elsevier B.V.},
title = {{Fusing two-stream convolutional neural networks for RGB-T object tracking}},
url = {https://doi.org/10.1016/j.neucom.2017.11.068},
volume = {281},
year = {2017}
}
@article{Gopal2018a,
author = {Gopal and Srivastava, Smriti},
doi = {10.1007/s13369-017-2644-6},
file = {::},
issn = {21914281},
journal = {Arabian Journal for Science and Engineering},
keywords = {Feature-level fusion,Multimodal system,Palm–phalanges,Score-level fusion,Unimodal system},
number = {2},
pages = {543--554},
publisher = {Springer Berlin Heidelberg},
title = {{Accurate Human Recognition by Score-Level and Feature-Level Fusion Using Palm–Phalanges Print}},
volume = {43},
year = {2018}
}
@inproceedings{Sangeetha2013,
abstract = {-In a Multimodal biometric system, the effective fusion method is necessary for combining information from various single modality systems. Two biometric characteristics are considered in this study: iris and fingerprint. Multimodal biometric system needs an effective fusion scheme to combine biometric characteristics derived from one or more modalities. The score level fusion is used to combine the characteristics from diff erent biometric modalities. Fusion at the score level is a new technique, which has a high potential for e f ficient consolidation of multiple unimodal biometric matcher outputs. Support vector machine and extreme learning techniques are used in this system for recognition of biometric traits. In this, the Fingerprint-Iris system provides better performance and comparison of support vector machine and extreme learning machine based on score-level fusion methods is obtained In score-level fusion, ELM provides better performance as compare to the SVM It reduces the classification time of current system. This work is valuable and makes an e f ficient accuracy in such applications. This system can be utilized for person identification in several applications.},
author = {Sangeetha, S and Radha, N.},
booktitle = {2013 7th International Conference on Intelligent Systems and Control (ISCO)},
doi = {10.1109/ISCO.2013.6481145},
isbn = {978-1-4673-4603-0},
month = {jan},
pages = {183--188},
publisher = {IEEE},
title = {{A New Framework for IRIS and Fingerprint Recognition Using SVM Classification and Extreme Learning Machine Based on Score Level Fusion}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6481145},
year = {2013}
}
@article{Fierrez2018b,
abstract = {We provide an introduction to Multiple Classifier Systems (MCS) including basic nomenclature and describing key elements: classifier dependencies, type of classifier outputs, aggregation procedures, architecture, and types of methods. This introduction complements other existing overviews of MCS, as here we also review the most prevalent theoretical framework for MCS and discuss theoretical developments related to MCS. The introduction to MCS is then followed by a review of the application of MCS to the particular field of multimodal biometric person authentication in the last 25 years, as a prototypical area in which MCS has resulted in important achievements. This review includes general descriptions of successful MCS methods and architectures in order to facilitate the export of them to other information fusion problems. Based on the theory and framework introduced here, in the companion paper we then develop in more technical detail recent trends and developments in MCS from multimodal biometrics that incorporate context information in an adaptive way. These new MCS architectures exploit input quality measures and pattern-specific particularities that move apart from general population statistics, resulting in robust multimodal biometric systems. Similarly as in the present paper, methods in the companion paper are introduced in a general way so they can be applied to other information fusion problems as well. Finally, also in the companion paper, we discuss open challenges in biometrics and the role of MCS to advance them.},
author = {Fierrez, Julian and Morales, Aythami and Vera-Rodriguez, Ruben and Camacho, David},
doi = {10.1016/j.inffus.2017.12.003},
file = {::},
issn = {15662535},
journal = {Information Fusion},
keywords = {Adaptive,Biometrics,Classifier,Context,Fusion,Multimodal},
number = {November 2017},
pages = {57--64},
publisher = {Elsevier},
title = {{Multiple classifiers in biometrics. part 1: Fundamentals and review}},
url = {https://doi.org/10.1016/j.inffus.2017.12.003},
volume = {44},
year = {2018}
}
@article{Nguyen2010,
author = {Nguyen, Kien and Fookes, Clinton and Sridharan, Sridha},
doi = {10.1145/1852611.1852635},
file = {::},
isbn = {9781450301053},
journal = {Proceedings of the 2010 Symposium on Information and Communication Technology - SoICT '10},
keywords = {iris recognition,mbgc,signal-level fusion,super-resolution},
number = {November},
pages = {122},
title = {{Robust mean super-resolution for less cooperative NIR iris recognition at a distance and on the move}},
url = {http://portal.acm.org/citation.cfm?doid=1852611.1852635},
year = {2010}
}
@article{Fierrez2018c,
abstract = {The present paper is Part 2 in this series of two papers. In Part 1 we provided an introduction to Multiple Classifier Systems (MCS) with a focus into the fundamentals: basic nomenclature, key elements, architecture, main methods, and prevalent theory and framework. Part 1 then overviewed the application of MCS to the particular field of multimodal biometric person authentication in the last 25 years, as a prototypical area in which MCS has resulted in important achievements. Here in Part 2 we present in more technical detail recent trends and developments in MCS coming from multimodal biometrics that incorporate context information in an adaptive way. These new MCS architectures exploit input quality measures and pattern-specific particularities that move apart from general population statistics, resulting in robust multimodal biometric systems. Similarly as in Part 1, methods here are described in a general way so they can be applied to other information fusion problems as well. Finally, we also discuss here open challenges in biometrics in which MCS can play a key role.},
author = {Fierrez, Julian and Morales, Aythami and Vera-Rodriguez, Ruben and Camacho, David},
doi = {10.1016/j.inffus.2017.12.005},
file = {::},
issn = {15662535},
journal = {Information Fusion},
keywords = {Adaptive,Biometrics,Classifier,Context,Fusion,Multimodal},
number = {November 2017},
pages = {103--112},
publisher = {Elsevier},
title = {{Multiple classifiers in biometrics. Part 2: Trends and challenges}},
url = {https://doi.org/10.1016/j.inffus.2017.12.005},
volume = {44},
year = {2018}
}
@article{Al-Waisy2017a,
abstract = {Multimodal biometrie systems seek to alleviate some of the limitations of unimodal biometrie systems by combining multiple pieces of evidence of the same person in the deeision-making process. In this paper, a novel multimodal biometric identification system is proposed based on fusing the results obtained from both the face and the left and right irises using deep learning approaches. Firstly, the facial features are extracted using a Deep Belief Network (DBN) architecture consisting of 3-layers. The first two RBMs are used as features detectors, while the last one is used as a discriminative model associated with softmax units for the multi-class classification purpose. Secondly, an efficient deep learning system is employed for iris recognition, whose architecture is based on a combination of Convolutional Neural Network (CNN) and Softmax classifier to extract discriminative features from an iris image. Extensive experiments on large-scale challenging databases, including FERET, CASIA V1.0 and MMU1, and SDUMLA-HMT have demonstrated the superiority of the proposed approaches by achieving new state-of-the-art Rank-1 identification rates on all the employed databases.},
author = {Al-Waisy, A S and Qahwaji, R and Ipson, S and Al-Fahdawi, S},
doi = {10.1109/EST.2017.8090417},
file = {::},
isbn = {9781538640173},
journal = {2017 Seventh International Conference on Emerging Security Technologies (EST)},
keywords = {Databases;Face;Feature extraction;Iris recognition},
pages = {163--168},
title = {{A multimodal biometrie system for personal identification based on deep learning approaches}},
year = {2017}
}
@article{Vivek2012a,
annote = {Score level fusion?},
author = {Vivek, S. Arun and Aravinth, J. and Valarmathy, S.},
doi = {10.1109/ICPRIME.2012.6208377},
file = {::},
isbn = {978-1-4673-1039-0},
journal = {International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME-2012)},
keywords = {density based score level,error,feature extraction,fusion,gmm,likelihood ratio test,multimodal biometrics,rates,template,unimodal biometrics},
pages = {387--392},
title = {{Feature extraction for multimodal biometric and study of fusion using Gaussian mixture model}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6208377},
year = {2012}
}
@article{Biosec2007,
abstract = {The baseline corpus of a new multimodal database, acquired in the framework of the FP6 EU BioSec Integrated Project, is presented. The corpus consists of fingerprint images acquired with three different sensors, frontal face images from a webcam, iris images from an iris sensor, and voice utterances acquired both with a close-talk headset and a distant webcam microphone. The BioSec baseline corpus includes real multimodal data from 200 individuals in two acquisition sessions. In this contribution, the acquisition setup and protocol are outlined, and the contents of the corpus-including data and population statistics-are described. The database will be publicly available for research purposes by mid-2006. {\textcopyright} 2006 Pattern Recognition Society.},
author = {Fierrez, Julian and Ortega-Garcia, Javier and {Torre Toledano}, Doroteo and Gonzalez-Rodriguez, Joaquin},
doi = {10.1016/j.patcog.2006.10.014},
file = {::},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Authentication,Biometrics,Database,Face,Fingerprint,Iris,Multimodal,Performance,Verification,Voice},
number = {4},
pages = {1389--1392},
title = {{Biosec baseline corpus: A multimodal biometric database}},
volume = {40},
year = {2007}
}
@article{Schiel2002,
abstract = {In this contribution we announce and describe in detail the new multimodal corpus evolving from the publicly funded German SmartKom project. The first release of the corpus (BAS SK-P 1.0) has been finished end of 2001 and will be ready for distribution to the scientific community in July 2002. The SmartKom corpus will be the first of a new generation of Language Resources (LR) designed for a more or less complete data gathering of human-machine communication combining acoustic, visual and tactile input and output modalities. Since the funding of about EU 2 Mio for this LR is 100{\%} public, the corpus will be available without royalties via the Bavarian Archive for Speech Signals (BAS) at the University of Munich.},
author = {Schiel, Florian and Steininger, Silke and T{\"{u}}rk, Ulrich},
file = {::},
journal = {Proceedings of the 3rd Language Resources and Evaluation},
number = {34},
pages = {200--2006},
title = {{The SmartKom Multimodal Corpus at BAS}},
year = {2002}
}
@article{Ross2003,
annote = {Nice and clear overview of the four basic modules of the standard biometric system.

Very clear explainations of different aspects, but a bit limited in respect to the separation of methods into categories .},
author = {Ross, Arun and Jain, Anil},
file = {::},
isbn = {1517355931},
keywords = {biometrics,decision tree,face,fingerprints,hand geometry,linear discriminant analysis,multimodal,sum rule,verification},
number = {13},
pages = {2115--2125},
title = {{Information Fusion in Biometrics}},
volume = {24},
year = {2003}
}
@article{Khiari-Hili2017a,
author = {Khiari-Hili, Nefissa and Montagne, Christophe and Lelandais, Sylvie and Hamrouni, Kamel},
doi = {10.1109/IPTA.2016.7820954},
file = {::},
isbn = {9781467389105},
journal = {2016 6th International Conference on Image Processing Theory, Tools and Applications, IPTA 2016},
keywords = {Multimodal biometrics,authentication,dynamic weighted sum,face,iris,quality,score fusion},
pages = {1--6},
title = {{Quality dependent multimodal fusion of face and iris biometrics}},
year = {2017}
}
@article{Petrovska-Delacretaz2008a,
abstract = {Face recognition finds its place in a large number of applications. They occur in different contexts related to security, entertainment or Internet applications. Reliable face recognition is still a great challenge to computer vision and pattern recognition researchers, and new algorithms need to be evaluated on relevant databases. The publicly available IV2 database allows monomodal and multimodal experiments using face data. Known variabilities, that are critical for the performance of the biometric systems (such as pose, expression, illumination and quality) are present. The face and subface data that are acquired in this database are: 2D audio-video talking-face sequences, 2D stereoscopic data acquired with two pairs of synchronized cameras, 3D facial data acquired with a laser scanner, and iris images acquired with a portable infrared camera. The IV2 database is designed for monomodal and multimodal experiments. The quality of the acquired data is of great importance. Therefore as a first step, and in order to better evaluate the quality of the data, a first internal evaluation was conducted. Only a small amount of the total acquired data was used for this evaluation: 2D still images, 3D scans and iris images. First results show the interest of this database. In parallel to the research algorithms, open-source reference systems were also run for baseline comparisons.},
author = {Petrovska-Delacr{\'{e}}taz, D. and Lelandais, S. and Colineau, J. and Chen, L. and Dorizzi, B. and Ardabilian, M. and Krichen, E. and Mellakh, M. A. and Chaari, A. and Guerfi, S. and D'Hose, J. and Amor, B. Ben},
doi = {10.1109/BTAS.2008.4699323},
file = {::},
isbn = {9781424427307},
journal = {BTAS 2008 - IEEE 2nd International Conference on Biometrics: Theory, Applications and Systems},
pages = {3--9},
title = {{The IV2 multimodal biometric database (including Iris, 2D, 3D, stereoscopic, and talking face data), and the IV2-2007 evaluation campaign}},
volume = {00},
year = {2008}
}
@inproceedings{Eitel2015,
abstract = {Robust object recognition is a crucial ingredient of many, if not all, real-world robotics applications. This paper leverages recent progress on Convolutional Neural Networks (CNNs) and proposes a novel RGB-D architecture for object recognition. Our architecture is composed of two separate CNN processing streams - one for each modality - which are consecutively combined with a late fusion network. We focus on learning with imperfect sensor data, a typical problem in real-world robotics tasks. For accurate learning, we introduce a multi-stage training methodology and two crucial ingredients for handling depth data with CNNs. The first, an effective encoding of depth information for CNNs that enables learning without the need for large depth datasets. The second, a data augmentation scheme for robust learning with depth images by corrupting them with realistic noise patterns. We present state-of-the-art results on the RGB-D object dataset and show recognition in challenging RGB-D real-world noisy settings.},
archivePrefix = {arXiv},
arxivId = {1507.06821},
author = {Eitel, Andreas and Springenberg, Jost Tobias and Spinello, Luciano and Riedmiller, Martin and Burgard, Wolfram},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2015.7353446},
eprint = {1507.06821},
file = {::},
isbn = {9781479999941},
issn = {21530866},
month = {jul},
pages = {681--687},
title = {{Multimodal deep learning for robust RGB-D object recognition}},
url = {http://arxiv.org/abs/1507.06821},
volume = {2015-Decem},
year = {2015}
}
@article{Daia,
abstract = {Deep networks have shown impressive performance on many computer vision tasks. Recently, deep convolutional neural networks (CNNs) have been used to learn discrim-inative texture representations. One of the most successful approaches is Bilinear CNN model that explicitly captures the second order statistics within deep features. However, these networks cut off the first order information flow in the deep network and make gradient back-propagation dif-ficult. We propose an effective fusion architecture -FASON that combines second order information flow and first or-der information flow. Our method allows gradients to back-propagate through both flows freely and can be trained ef-fectively. We then build a multi-level deep architecture to exploit the first and second order information within dif-ferent convolutional layers. Experiments show that our method achieves improvements over state-of-the-art meth-ods on several benchmark datasets.},
author = {Dai, Xiyang and Ng, Joe Yue-hei and Davis, Larry S},
file = {::},
journal = {Cvpr2017},
pages = {7352--7360},
title = {{FASON : First and Second Order Information Fusion Network for Texture Recognition}}
}
@article{Phillips2009,
abstract = {The goal of the Multiple Biometrics Grand Challenge (MBGC) is to improve the performance of face and iris recognition technology from biometric samples acquired under unconstrained conditions. The MBGC is organized into three challenge problems. Each challenge problem re- laxes the acquisition constraints in different directions. In the Portal Challenge Problem, the goal is to recognize people from near-infrared (NIR) and high definition (HD) video as they walk through a portal. Iris recognition can be performed from the NIR video and face recognition from the HD video. The availability of NIR and HD modalities allows for the development of fusion algorithms. The Still Face Challenge Problem has two primary goals. The first is to improve recognition performance from frontal and off angle still face images taken under uncontrolled in- door and outdoor lighting. The second is to improve recognition perfor- mance on still frontal face images that have been resized and compressed, as is required for electronic passports. In the Video Challenge Problem, the goal is to recognize people from video in unconstrained environments. The video is unconstrained in pose, illumination, and camera angle. All three challenge problems include a large data set, experiment descrip- tions, ground truth, and scoring code.},
author = {Phillips, P. Jonathon and Flynn, Patrick J. and Beveridge, J. Ross and Scruggs, W. Todd and O'Toole, Alice J. and Bolme, David and Bowyer, Kevin W. and Draper, Bruce A. and Givens, Geof H. and Lui, Yui Man and Sahibzada, Hassan and Scallan, Joseph A. and Weimer, Samuel},
doi = {10.1007/978-3-642-01793-3_72},
file = {::},
isbn = {3642017924},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {705--714},
title = {{Overview of the multiple biometrics grand challenge}},
volume = {5558 LNCS},
year = {2009}
}
@article{Chen2005a,
abstract = {The recognition accuracy of a single biometric authentication system is often much reduced due to the environment, user mode and physiological defects. In this paper, we combine face and iris features for developing a multimode biometric approach, which is able to diminish the drawback of single biometric approach as well as to improve the performance of authentication system. We combine a face database ORL and iris database CASIA to construct a multimodal biometric experimental database with which we validate the proposed approach and evaluate the multimodal biometrics performance. The experimental results reveal the multimodal biometrics verification is much more reliable and precise than single biometric approach.},
annote = {Face and iris for authentication 
uses synthetic multimodal biometric dataset

Shows how to evaluate system. 

proves multimodal performs better.},
author = {Chen, Ching-Han and {Te Chu}, Chia},
doi = {10.1007/11608288_76},
file = {::},
isbn = {978-3-540-31621-3},
issn = {03029743},
keywords = {face,iris,multimodal biometrics,wavelet probabilistic neural},
pages = {571--580},
title = {{Fusion of Face and Iris Features for Multimodal Biometrics}},
url = {http://link.springer.com/10.1007/11608288{\_}76},
year = {2005}
}
@article{Nam2016a,
abstract = {We propose Dual Attention Networks (DANs) which jointly leverage visual and textual attention mechanisms to capture fine-grained interplay between vision and language. DANs attend to specific regions in images and words in text through multiple steps and gather essential information from both modalities. Based on this framework, we introduce two types of DANs for multimodal reasoning and matching, respectively. The reasoning model allows visual and textual attentions to steer each other during collaborative inference, which is useful for tasks such as Visual Question Answering (VQA). In addition, the matching model exploits the two attention mechanisms to estimate the similarity between images and sentences by focusing on their shared semantics. Our extensive experiments validate the effectiveness of DANs in combining vision and language, achieving the state-of-the-art performance on public benchmarks for VQA and image-text matching.},
archivePrefix = {arXiv},
arxivId = {1611.00471},
author = {Nam, Hyeonseob and Ha, Jung-Woo and Kim, Jeonghee},
doi = {10.1109/CVPR.2017.232},
eprint = {1611.00471},
file = {::},
isbn = {978-1-5386-0457-1},
issn = {1611.00471},
pages = {299--307},
title = {{Dual Attention Networks for Multimodal Reasoning and Matching}},
url = {http://arxiv.org/abs/1611.00471},
year = {2016}
}
@article{Baltrusaitis2017a,
abstract = {Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.},
archivePrefix = {arXiv},
arxivId = {1705.09406},
author = {Baltru{\v{s}}aitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
doi = {10.1109/TPAMI.2018.2798607},
eprint = {1705.09406},
file = {::},
issn = {0162-8828},
number = {c},
pages = {1--20},
title = {{Multimodal Machine Learning: A Survey and Taxonomy}},
url = {http://arxiv.org/abs/1705.09406},
volume = {8828},
year = {2017}
}
@article{Ortega-Garcia2010,
abstract = {A new multimodal biometric database designed and acquired within the framework of the European BioSecure Network of Excellence is presented. It is comprised of more than 600 individuals acquired simultaneously in three scenarios: 1) over the Internet, 2) in an office environment with desktop PC, and 3) in indoor/outdoor environments with mobile portable hardware. The three scenarios include a common part of audio/video data. Also, signature and fingerprint data have been acquired both with desktop PC and mobile portable hardware. Additionally, hand and iris data were acquired in the second scenario using desktop PC. Acquisition has been conducted by 11 European institutions. Additional features of the BioSecure Multimodal Database (BMDB) are: two acquisition sessions, several sensors in certain modalities, balanced gender and age distributions, multimodal realistic scenarios with simple and quick tasks per modality, cross-European diversity, availability of demographic data, and compatibility with other multimodal databases. The novel acquisition conditions of the BMDB allow us to perform new challenging research and evaluation of either monomodal or multimodal biometric systems, as in the recent BioSecure Multimodal Evaluation campaign. A description of this campaign including baseline results of individual modalities from the new database is also given. The database is expected to be available for research purposes through the BioSecure Association during 2008.},
author = {Ortega-Garcia, Javier and Fierrez, Julian and Alonso-Fernandez, Fernando and Galbally, Javier and Freire, Manuel R. and Gonzalez-Rodriguez, Joaquin and Garcia-Mateo, Carmen and Alba-Castro, Jose Luis and Gonzalez-Agulla, Elisardo and Otero-Muras, Enrique and Garcia-Salicetti, Sonia and Allano, Lorene and Ly-Van, Bao and Dorizzi, Bernadette and Kittler, Josef and Bourlai, Thirimachos and Poh, Norman and Deravi, Farzin and Ng, Ming N.R. and Fairhurst, Michael and Hennebert, Jean and Humm, Andreas and Tistarelli, Massimo and Brodo, Linda and Richiardi, Jonas and Drygajlo, Andrezj and Ganster, Harald and Sukno, Federico M. and Pavani, Sri Kaushik and Frangi, Alejandro and Akarun, Lale and Savran, Arman},
doi = {10.1109/TPAMI.2009.76},
file = {::},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Benchmark,Biometrics,Database,Evaluation,Face,Fingerprint,Hand,Iris.,Multimodal,Performance,Signature,Speaker,Voice},
number = {6},
pages = {1097--1111},
pmid = {20431134},
title = {{The multiscenario multienvironment biosecure multimodal database (BMDB)}},
volume = {32},
year = {2010}
}
@article{Aakerberg2017,
abstract = {Object recognition is one of the important tasks in computer vision which has found enormous applications. Depth modality is proven to provide supplementary information to the common RGB modality for object recognition. In this paper, we propose methods to improve the recognition performance of an existing deep learning based RGB-D object recognition model, namely the FusionNet proposed by Eitel et al. First, we show that encoding the depth values as colorized surface normals is beneficial, when the model is initialized with weights learned from training on ImageNet data. Additionally, we show that the RGB stream of the FusionNet model can benefit from using deeper network architectures, namely the 16-layered VGGNet, in exchange for the 8-layered CaffeNet. In combination, these changes improves the recognition performance with 2.2{\%} in comparison to the original FusionNet, when evaluating on the Washington RGB-D Object Dataset.},
author = {Aakerberg, Andreas and Nasrollahi, Kamal and Rasmussen, Christoffer B. and Moeslund, Thomas B.},
doi = {10.5220/0006511501210128},
file = {::},
isbn = {978-989-758-274-5},
journal = {Proceedings of the 9th International Joint Conference on Computational Intelligence},
keywords = {artificial vision,computer vision,convolutional neural networks,deep learning,has found enormous applications,in computer vision which,learning,object recognition is one,of the important tasks,rgb-d,surface normals,transfer},
pages = {121--128},
title = {{Depth Value Pre-Processing for Accurate Transfer Learning based RGB-D Object Recognition}},
url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0006511501210128},
year = {2017}
}
@article{Zhang2017a,
abstract = {Multimodal classification arises in many computer vi-sion tasks such as object classification and image retrieval. The idea is to utilize multiple sources (modalities) measur-ing the same instance to improve the overall performance compared to using a single source (modality). The varying characteristics exhibited by multiple modalities make it nec-essary to simultaneously learn the corresponding distance metrics. In this paper, we propose a multiple metrics learn-ing algorithm for multimodal data. Metric of each modal-ity is product of two matrices: one matrix is modality spe-cific, the other is enforced to be shared by all the modalities. The learned metrics can improve multimodal classification accuracy and experimental results on four datasets show that the proposed algorithm outperforms existing learning algorithms based on multiple metrics as well as other ap-proaches tested on these datasets. Specifically, we report 95.0{\%} object instance recognition accuracy, 89.2{\%} object category recognition accuracy on the multi-view RGB-D dataset and 52.3{\%} scene category recognition accuracy on SUN RGB-D dataset.},
author = {Zhang, Heng and Patel, Vishal M. and Chellappa, Rama},
doi = {10.1109/CVPR.2017.312},
file = {::},
isbn = {978-1-5386-0457-1},
issn = {1063-6919},
journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {2925--2933},
title = {{Hierarchical Multimodal Metric Learning for Multimodal Classification}},
url = {http://ieeexplore.ieee.org/document/8099795/},
year = {2017}
}
@article{Johnson2010a,
abstract = {Identification of individuals using biometric information has found great success in many security and law enforcement applications. Up until the present time, most research in the field has been focused on ideal conditions and most available databases are constructed in these ideal conditions. There has been a growing interest in the perfection of these technologies at a distance and in less than ideal conditions, i.e. low lighting, out-of-focus blur, off angles, etc. This paper presents a dataset consisting of face and iris videos obtained at distances of 5 to 25 feet and in conditions of varying quality. The purpose of this database is to set a standard for quality measurement in face and iris data and to provide a means for analyzing biométrie systems in less than ideal conditions. The structure of the dataset as well as a quantified metric for quality measurement based on a 25 subject subset of the dataset is presented.},
author = {Johnson, P. A. and Lopez-Meyer, P. and Sazonova, N. and Hua, F. and Schuckers, S.},
doi = {10.1109/BTAS.2010.5634513},
file = {::},
isbn = {9781424475803},
journal = {IEEE 4th International Conference on Biometrics: Theory, Applications and Systems, BTAS 2010},
title = {{Quality in face and Iris research ensemble (Q-FIRE)}},
year = {2010}
}
@article{BiosecID2008,
abstract = {A new multimodal biometric database, acquired in the framework of the BiosecurID project, is presented together with the description of the acquisition setup and protocol. The database includes eight unimodal biometric traits, namely: speech, iris, face (still images, videos of talking faces), handwritten signature and handwritten text (on-line dynamic signals, off-line scanned images), fingerprints (acquired with two different sensors), hand (palmprint, contour-geometry) and keystroking. The database comprises 400 subjects and presents features such as: realistic acquisition scenario, balanced gender and population distributions, availability of information about particular demographic groups (age, gender, handedness), acquisition of replay attacks for speech and keystroking, skilled forgeries for signatures, and compatibility with other existing databases. All these characteristics make it very useful in research and development of unimodal and multimodal biometric systems. {\textcopyright} Springer-Verlag London Limited 2009.},
author = {Fierrez, J. and Galbally, J. and Ortega-Garcia, J. and Freire, M. R. and Alonso-Fernandez, F. and Ramos, D. and Toledano, D. T. and Gonzalez-Rodriguez, J. and Siguenza, J. A. and Garrido-Salas, J. and Anguiano, E. and Gonzalez-de-Rivera, G. and Ribalda, R. and Faundez-Zanuy, M. and Ortega, J. A. and Carde{\~{n}}oso-Payo, V. and Viloria, A. and Vivaracho, C. E. and Moro, Q. I. and Igarza, J. J. and Sanchez, J. and Hernaez, I. and Orrite-Uru{\~{n}}uela, C. and Martinez-Contreras, F. and Gracia-Roche, J. J.},
doi = {10.1007/s10044-009-0151-4},
file = {::},
issn = {14337541},
journal = {Pattern Analysis and Applications},
keywords = {Biometrics,Database,Face,Fingerprint,Hand geometry,Handwriting,Iris,Keystroking,Multimodal,Palmprint,Signature,Speech},
number = {2},
pages = {235--246},
title = {{BiosecurID: A multimodal biometric database}},
volume = {13},
year = {2010}
}
