Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@incollection{Miller2012,
abstract = {Zebrafish spend the majority of their time in groups, called shoals. Shoaling behavior is complex and dynamic: fish leave and rejoin the shoal, distances between shoal-members oscillate, and the speed and polarization of the shoal change on timescales of seconds to minutes. All these features of shoals can be modified by various pharmacological and environmental—and possibly also genetic—manipulations and a thorough characterization of shoaling behavior can therefore be used as an effective assay for complex aspects of vertebrate social behavior. We present methods for acquiring and analyzing detailed trajectory data from shoals of zebrafish and demonstrate how these methods can be used to distinguish episodes of shoaling under different conditions. These methods could be further developed to create a standardized assay of shoaling behavior that will allow for an in-depth exploration of social behaviors in zebrafish.},
author = {Miller, Noam and Gerlai, Robert},
doi = {10.1007/978-1-61779-597-8_16},
pages = {217--230},
publisher = {Humana Press, Totowa, NJ},
title = {{Automated Tracking of Zebrafish Shoals and the Analysis of Shoaling Behavior}},
url = {http://link.springer.com/10.1007/978-1-61779-597-8{\_}16},
year = {2012}
}
@incollection{RahmanKhan2018,
abstract = {Long-haul travel does not constitute an obstacle for tourists to travel and is fast gaining the attention of tourists in new and unique experiences. This study was conducted to identify the long-haul travel motivation by international tourists to Penang. A total of 400 respondents participated in this survey, conducted around the tourist attractions in Penang, using cluster random sampling. However, only 370 questionnaires were only used for this research. Data were analysed using SPSS software 22 version. The findings, ‘knowledge and novelty seeking' were the main push factors that drove long-haul travel by international tourists to Penang. Meanwhile, the main pull factor that attracts long- haul travel by international tourists to Penang was its ‘culture and history'. Additionally, there were partly direct and significant relationships between socio-demographic, trip characteristics and travel motivation (push factors and pull factors). Overall, this study identified the long-haul travel motivations by international tourists to Penang based on socio-demographic, trip characteristics and travel motivation and has indirectly helped in understanding the long-haul travel market particularly for Penang and Southeast Asia. This research also suggested for an effective marketing and promotion strategy in pro- viding useful information that is the key to attract international tourists to travel long distances. Keywords:},
author = {{Rahman Khan}, Farmanur and {Sulaiman Alhewairini}, Saleh},
booktitle = {Current Trends in Cancer Management [Working Title]},
doi = {10.5772/intechopen.81517},
month = {nov},
publisher = {IntechOpen},
title = {{Zebrafish (Danio rerio) as a Model Organism}},
url = {https://www.intechopen.com/online-first/zebrafish-danio-rerio-as-a-model-organism},
year = {2018}
}
@article{HongWang2016,
abstract = {Zebrafish (Danio rerio) is one of the most widely used model organisms in collective behavior research. Multi-object tracking with high speed camera is currently the most feasible way to accurately measure their motion states for quantitative study of their collective behavior. However, due to difficulties such as their similar appearance, complex body deformation and frequent occlusions, it is a big challenge for an automated system to be able to reliably track the body geometry of each individual fish. To accomplish this task, we propose a novel fish body model that uses a chain of rectangles to represent fish body. Then in detection stage, the point of maximum curvature along fish boundary is detected and set as fish nose point. Afterwards, in tracking stage, we firstly apply Kalman filter to track fish head, then use rectangle chain fitting to fit fish body, which at the same time further judge the head tracking results and remove the incorrect ones. At last, a tracklets relinking stage further solves trajectory fragmentation due to occlusion. Experiment results show that the proposed tracking system can track a group of zebrafish with their body geometry accurately even when occlusion occurs from time to time.},
author = {{Hong Wang}, Shuo and {En Cheng}, Xi and Qian, Zhi-Ming and Liu, Ye and {Qiu Chen}, Yan},
doi = {10.1371/journal.pone.0154714},
title = {{Automated Planar Tracking the Waving Bodies of Multiple Zebrafish Swimming in Shallow Water}},
url = {https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0154714{\&}type=printable},
year = {2016}
}
@article{Rodriguez2018,
abstract = {{\textcopyright}2017 The Authors. Methods in Ecology and Evolution {\textcopyright}2017 British Ecological Society Behavioural analysis based on video recording is becoming increasingly popular within research fields such as; ecology, medicine, ecotoxicology and toxicology. However, the programs available to analyse the data, which are free of cost, user-friendly, versatile, robust, fast and provide reliable statistics for different organisms (invertebrates, vertebrates and mammals) are significantly limited. We present an automated open-source executable software (ToxTrac) for image-based tracking that can simultaneously handle several organisms monitored in a laboratory environment. We compare the performance of ToxTrac with current accessible programs on the web. The main advantages of ToxTrac are as follows: (i) no specific knowledge of the geometry of the tracked bodies is needed; (ii) processing speed, ToxTrac can operate at a rate {\textgreater}25 frames per second in HD videos using modern computers; (iii) simultaneous tracking of multiple organisms in multiple arenas; (iv) integrated distortion correction and camera calibration; (v) robust against false positives; (vi) preservation of individual identification; (vii) useful statistics and heat maps in real scale are exported in image, text and excel formats. ToxTrac can be used for high speed tracking of insects, fish, rodents or other species, and provides useful locomotor information in animal behavior experiments. Download ToxTrac here: https://toxtrac.sourceforge.io (Current version v2.61).},
author = {Rodriguez, Alvaro and Zhang, Hanqing and Klaminder, Jonatan and Brodin, Tomas and Andersson, Patrik L and Andersson, Magnus},
doi = {10.1111/2041-210X.12874},
issn = {2041210X},
journal = {Methods in Ecology and Evolution},
keywords = {Kalman filter,animal behavior,cockroach,ecology,ecotoxicology,guppy,salmon,tadpole,tracking software,zebrafish},
number = {3},
pages = {460--464},
title = {{ToxTrac: A fast and robust software for tracking organisms}},
url = {https://toxtrac.sourceforge.io},
volume = {9},
year = {2018}
}
@misc{Delcourt2018,
abstract = {In laboratory fish research, the zebrafish Danio rerio (Cyprinidae) represents the equivalent of the mouse in mammalian research. This species has become a major model for studies in developmental and behavioural genetics, neurophysiology, biomedicine, ecotoxicology, and behavioural and evolutionary ecology. To meet the need for accurate and reproducible data in both fundamental and applied sciences, it is of primary importance to be able to tag and/or recognize individual zebrafish. However, classic methods used in fish ecology and aquaculture are generally difficult to apply to such small fish. Recently, various new tagging methods have been developed. This paper presents a first review of current identification and marking methods applied to zebrafish, from external observation methods (such as skin pattern recognition, fin clipping, scale regeneration, colour and transgenic methods) to the most advanced technological developments in electronic (low- and high- radio-frequencies PIT tags, microchip) and image analysis methods (video tracking). This review aims to help researchers and zebrafish facility managers select the identification method (ID) best adapted to their needs. The main characteristics of each ID method are examined (including detection range, durability, speed and repetitiveness, ID code combination, size dependence and ethical considerations), and their pros and cons are summarized in a decision table to help select the most appropriate option for a research or management program. Finally, contextual applications of these ID methods and future developments are discussed.},
author = {Delcourt, Johann and Ovidio, Micha{\"{e}}l and Deno{\"{e}}l, Mathieu and Muller, Marc and Pendeville, H{\'{e}}l{\`{e}}ne and Deneubourg, Jean Louis and Poncin, Pascal},
booktitle = {Reviews in Fish Biology and Fisheries},
doi = {10.1007/s11160-018-9537-y},
isbn = {0123456789},
issn = {15735184},
keywords = {Animal ID,Danio rerio,Passive integrated transponder,Tagging,VIE tag,Video tracking},
number = {4},
pages = {839--864},
title = {{Individual identification and marking techniques for zebrafish}},
url = {https://doi.org/10.1007/s11160-018-9537-y},
volume = {28},
year = {2018}
}
@article{Sridhar2019,
abstract = {Automated movement tracking is essential for high-throughput quantitative analyses of the behaviour and kinematics of organisms. Automated tracking also improves replicability by avoiding observer biases and allowing reproducible workflows. However, few automated tracking programs exist that are open access, open source, and capable of tracking unmarked organisms in noisy environments. Tracktor is an image-based tracking freeware designed to perform single-object tracking in noisy environments, or multi-object tracking in uniform environments while maintaining individual identities. Tracktor is code-based but requires no coding skills other than the user being able to specify tracking parameters in a designated location, much like in a graphical user interface (GUI). The installation and use of the software is fully detailed in a user manual. Through four examples of common tracking problems, we show that Tracktor is able to track a variety of animals in diverse conditions. The main strengths of Tracktor lie in its ability to track single individuals under noisy conditions (e.g. when the object shape is distorted), its robustness to perturbations (e.g. changes in lighting conditions during the experiment), and its capacity to track multiple individuals while maintaining their identities. Additionally, summary statistics and plots allow measuring and visualizing common metrics used in the analysis of animal movement (e.g. cumulative distance, speed, acceleration, activity, time spent in specific areas, distance to neighbour, etc.). Tracktor is a versatile, reliable, easy-to-use automated tracking software that is compatible with all operating systems and provides many features not available in other existing freeware. Access Tracktor and the complete user manual here: https://github.com/vivekhsridhar/tracktor},
author = {Sridhar, Vivek Hari and Roche, Dominique G and Gingins, Simon},
doi = {10.1111/2041-210X.13166},
issn = {2041210X},
journal = {Methods in Ecology and Evolution},
keywords = {choice experiment,collective behaviour,fast-start escape response,kinematics,locomotion,video analysis software},
number = {6},
pages = {815--820},
title = {{Tracktor: Image-based automated tracking of animal movement and behaviour}},
url = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13166},
volume = {10},
year = {2019}
}
@article{Heide1999,
annote = {General notes:
Review of different eye tracking techniques. From 1999.

EOG:
Used most in clinical research
Pros:
- Non intrusive
- Does not limit field of view
- Can be used while the subject is wearing glasses/lenses
- Can be used with non cooperative subject
- Can be used while eye is closed ( E.G while asleep)

Cons:
- Sensitive to changes in light
- Often contaminated by electrical noise + blinks and eye lids.
- Horisontal range of +-40 deg
- Resolution of about 1-2 deg
- Vertical eye movements are often unreliable and require special configurations
- Torsinal eye movemens (around the line of sight) are not possible

VOG:
Pros:
- Non invasive
- 3D as well.
- Range of +- 40 deg horisontal and +- 30 deg vertical
- spatial resolution of about 0.5 deg

Cons:
Not really any that are relevant anymore},
author = {Heide, W and Koenig, E and Trillenberg, P and K{\"{o}}mpf, D and Zee, D S},
issn = {0424-8155},
journal = {Electroencephalography and clinical neurophysiology. Supplement},
pages = {223--240},
pmid = {10590990},
title = {{Electrooculography: technical standards and applications. The International Federation of Clinical Neurophysiology.}},
volume = {52},
year = {1999}
}
@article{Stewart2015a,
abstract = {Background: Expanding the spectrum of organisms to model human brain phenotypes is critical for our improved understanding of the pathobiology of neuropsychiatric disorders. Given the clear limitations of existing mammalian models, there is an urgent need for low-cost, high-throughput in-vivo technologies for drug and gene discovery. New method: Here, we introduce a new automated method for generating 3D (X,Y,Z) swim trajectories in adult zebrafish (Danio rerio), to improve their neurophenotyping. Results: Based on the Track3D module of EthoVision XT video tracking software (Noldus Information Technology), this tool enhances the efficient, high-throughput 3D analyses of zebrafish behavioral responses. Applied to adult zebrafish behavior, this 3D method is highly sensitive to various classes of psychotropic drugs, including selected psychostimulant and hallucinogenic agents. Comparison with existing Methods: Our present method offers a marked advance in the existing 2D and 3D methods of zebrafish behavioral phenotyping, minimizing research time and recording high-resolution, automatically synchronized videos with calculated, high-precision object positioning. Conclusions: Our novel approach brings practical simplicity and 'integrative' capacity to the often complex and error-prone quantification of zebrafish behavioral phenotypes. Illustrating the value of 3D swim path reconstructions for identifying experimentally-evoked phenotypic profiles, this method fosters innovative, ethologically relevant, and fully automated small molecule screens using adult zebrafish.},
author = {Stewart, Adam Michael and Grieco, Fabrizio and Tegelenbosch, Ruud A.J. and Kyzar, Evan J. and Nguyen, Michael and Kaluyeva, Aleksandra and Song, Cai and Noldus, Lucas P.J.J. and Kalueff, Allan V.},
doi = {10.1016/j.jneumeth.2015.07.023},
issn = {1872678X},
journal = {Journal of Neuroscience Methods},
keywords = {Drug discovery,High-throughput screening,Neurophenotype,Video tracking,Zebrafish},
month = {nov},
pages = {66--74},
pmid = {26238728},
title = {{A novel 3D method of locomotor analysis in adult zebrafish: Implications for automated detection of CNS drug-evoked phenotypes}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26238728 https://linkinghub.elsevier.com/retrieve/pii/S0165027015002782},
volume = {255},
year = {2015}
}
@article{Al-Waisy2017,
author = {Al-Waisy, Alaa S and Qahwaji, Rami and Ipson, Stanley and Al-Fahdawi, Shumoos and Nagem, Tarek A M},
doi = {10.1007/s10044-017-0656-1},
isbn = {0123456789},
issn = {14337541},
journal = {Pattern Analysis and Applications},
keywords = {AdaGrad method,Convolutional Neural Network,Deep learning,Iris recognition,Multimodal biometric systems,Softmax classifier},
number = {0123456789},
pages = {1--20},
publisher = {Springer London},
title = {{A multi-biometric iris recognition system based on a deep learning approach}},
url = {https://doi.org/10.1007/s10044-017-0656-1},
year = {2017}
}
@article{Uka2017,
annote = {Keynotes:

CASIA (Most used database) and IIT Delhi Iris Database used.

Hough Transofrm algorithm used to detect boundaries},
author = {Uka, Arban and Ro{\c{c}}i, Albana and Ko{\c{c}}, Oktay},
isbn = {9781509038435},
journal = {IEEE EUROCON 2017 -17th International Conference on Smart Technologies},
keywords = {encoding,equal error rate,segmentation},
number = {July},
pages = {6--8},
title = {{Improved Segmentation Algorithm and Further Optimization for Iris Recognition}},
year = {2017}
}
@article{Ma2015,
abstract = {This study presents a novel human-machine interface (HMI) based on both electrooculography (EOG) and electroencephalography (EEG). This hybrid interface works in two modes: an EOG mode recognizes eye movements such as blinks, and an EEG mode detects event related potentials (ERPs) like P300. While both eye movements and ERPs have been separately used for implementing assistive interfaces, which help patients with motor disabilities in performing daily tasks, the proposed hybrid interface integrates them together. In this way, both the eye movements and ERPs complement each other. Therefore, it can provide a better efficiency and a wider scope of application. In this study, we design a threshold algorithm that can recognize four kinds of eye movements including blink, wink, gaze, and frown. In addition, an oddball paradigm with stimuli of inverted faces is used to evoke multiple ERP components including P300, N170, and VPP. To verify the effectiveness of the proposed system, two different online experiments are carried out. One is to control a multifunctional humanoid robot, and the other is to control four mobile robots. In both experiments, the subjects can complete tasks effectively by using the proposed interface, whereas the best completion time is relatively short and very close to the one operated by hand.},
author = {Ma, Jiaxin and Zhang, Yu and Cichocki, Andrzej and Matsuno, Fumitoshi},
doi = {10.1109/TBME.2014.2369483},
isbn = {0018-9294},
issn = {15582531},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {Electroencephalogram (EEG),Electrooculogram (EOG),event-related potential (ERP),human-machine interface (HMI),robot control},
number = {3},
pages = {876--889},
pmid = {25398172},
title = {{A novel EOG/EEG hybrid human-machine interface adopting eye movements and ERPs: Application to robot control}},
volume = {62},
year = {2015}
}
@article{Romero-Ferrero2019,
abstract = {Our understanding of collective animal behavior is limited by our ability to track each of the individuals. We describe an algorithm and software, idtracker.ai, that extracts from video all trajectories with correct identities at a high accuracy for collectives of up to 100 individuals. It uses two deep networks, one detecting when animals touch or cross and another one for animal identification, trained adaptively to conditions and difficulty of the video.},
archivePrefix = {arXiv},
arxivId = {1803.04351},
author = {Romero-Ferrero, Francisco and Bergomi, Mattia G. and Hinz, Robert C. and Heras, Francisco J.H. and de Polavieja, Gonzalo G.},
doi = {10.1038/s41592-018-0295-5},
eprint = {1803.04351},
file = {:home/niclas/Documents/uni/vgis10/vgis10/Report/bib/VGIS10.bib:bib},
issn = {15487105},
journal = {Nature Methods},
month = {mar},
number = {2},
pages = {179--182},
title = {idtracker.ai: tracking all individuals in small or large collectives of unmarked animals},
url = {http://arxiv.org/abs/1803.04351},
volume = {16},
year = {2019}
}
@incollection{Bowyer2016,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-{\$}\alpha{\$}-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA}for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bowyer, Kevin W and Hollingsworth, Karen P and Flynn, Patrick J},
booktitle = {Handbook of Iris Recognition},
doi = {10.1007/978-1-4471-6784-6},
eprint = {arXiv:1011.1669v3},
isbn = {978-1-4471-6782-2},
issn = {16130073},
pages = {23--61},
pmid = {25246403},
title = {{Chapter 2 A Survey of Iris Biometrics Research: 2008–2010}},
url = {http://link.springer.com/10.1007/978-1-4471-6784-6},
year = {2016}
}
@article{Berg1991,
author = {Berg, P and Scherg, M},
journal = {Clinical Physiology and Physiological Measures},
keywords = {ERP eye ocular},
pages = {49--54},
title = {{Dipole modelling of eye activity and its application to the removal of eye artifacts from the EEG and MEG}},
volume = {12},
year = {1991}
}
@inproceedings{Gall2012,
abstract = {Object detection multi class detection Random forests object detection ...},
author = {Gall, Juergen and Razavi, Nima and {Van Gool}, Luc},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-34091-8_11},
isbn = {9783642340901},
issn = {03029743},
keywords = {Hough forest,multi-class object detection,random forest,regression forest},
pages = {243--263},
title = {{An introduction to random forests for multi-class object detection}},
url = {http://link.springer.com/10.1007/978-3-642-34091-8{\_}11},
volume = {7474 LNCS},
year = {2012}
}
@misc{Burke2016,
abstract = {Scientists use a variety of laboratory techniques to investigate the genetic cause of human diseases. Research often utilizes patients' cells or tissue samples, but to determine if a mutation in a specific gene can cause a patient's symptoms, we often need experimental animal models. While mice and rats have been common choices for modeling human diseases in the past, the use of zebrafish is rapidly gaining popularity. Does this surprise you? Let me explain.},
author = {Burke, Elizabeth},
booktitle = {I am Intramural Blog},
title = {{Why Use Zebrafish to Study Human Disease?}},
url = {https://irp.nih.gov/blog/post/2016/08/why-use-zebrafish-to-study-human-diseases},
urldate = {2019-06-04},
year = {2016}
}
@misc{Delcourt2018a,
abstract = {{\textcopyright} 2018, Springer Nature Switzerland AG. In laboratory fish research, the zebrafish Danio rerio (Cyprinidae) represents the equivalent of the mouse in mammalian research. This species has become a major model for studies in developmental and behavioural genetics, neurophysiology, biomedicine, ecotoxicology, and behavioural and evolutionary ecology. To meet the need for accurate and reproducible data in both fundamental and applied sciences, it is of primary importance to be able to tag and/or recognize individual zebrafish. However, classic methods used in fish ecology and aquaculture are generally difficult to apply to such small fish. Recently, various new tagging methods have been developed. This paper presents a first review of current identification and marking methods applied to zebrafish, from external observation methods (such as skin pattern recognition, fin clipping, scale regeneration, colour and transgenic methods) to the most advanced technological developments in electronic (low- and high- radio-frequencies PIT tags, microchip) and image analysis methods (video tracking). This review aims to help researchers and zebrafish facility managers select the identification method (ID) best adapted to their needs. The main characteristics of each ID method are examined (including detection range, durability, speed and repetitiveness, ID code combination, size dependence and ethical considerations), and their pros and cons are summarized in a decision table to help select the most appropriate option for a research or management program. Finally, contextual applications of these ID methods and future developments are discussed.},
author = {Delcourt, Johann and Ovidio, Micha{\"{e}}l and Deno{\"{e}}l, Mathieu and Muller, Marc and Pendeville, H{\'{e}}l{\`{e}}ne and Deneubourg, Jean Louis and Poncin, Pascal},
booktitle = {Reviews in Fish Biology and Fisheries},
doi = {10.1007/s11160-018-9537-y},
file = {::},
isbn = {0123456789},
issn = {15735184},
keywords = {Animal ID,Danio rerio,Passive integrated transponder,Tagging,VIE tag,Video tracking},
number = {4},
pages = {839--864},
title = {{Individual identification and marking techniques for zebrafish}},
url = {https://doi.org/10.1007/s11160-018-9537-y},
volume = {28},
year = {2018}
}
@inproceedings{Ali2017,
author = {Ali, Syed Tariq and Goyal, Kalpana and Singhai, Jyoti},
booktitle = {International Conference on Recent Innovations in Signal Processing and Embedded Systems, RISE 2017},
doi = {10.1109/RISE.2017.8378144},
isbn = {9781509047604},
keywords = {Background Modeling,Background Subtraction,Gaussian Mixture Model,Moving Object Detection},
month = {oct},
pages = {153--156},
publisher = {IEEE},
title = {{Moving object detection using self adaptive Gaussian Mixture Model for real time applications}},
url = {https://ieeexplore.ieee.org/document/8378144/},
volume = {2018-Janua},
year = {2018}
}
@article{Qian2014,
abstract = {Due to its universality, swarm behavior in nature attracts much attention of scientists from many fields. Fish schools are examples of biological communities that demonstrate swarm behavior. The detection and tracking of fish in a school are of important significance for the quantitative research on swarm behavior. However, different from other biological communities, there are three problems in the detection and tracking of fish school, that is, variable appearances, complex motion and frequent occlusion. To solve these problems, we propose an effective method of fish detection and tracking. In this method, first, the fish head region is positioned through extremum detection and ellipse fitting; second, The Kalman filtering and feature matching are used to track the target in complex motion; finally, according to the feature information obtained by the detection and tracking, the tracking problems caused by frequent occlusion are processed through trajectory linking. We apply this method to track swimming fish school of different densities. The experimental results show that the proposed method is both accurate and reliable.},
author = {Qian, Zhi Ming and Cheng, Xi En and Chen, Yan Qiu},
doi = {10.1371/journal.pone.0106506},
editor = {Deng, Z. Daniel},
file = {:home/niclas/Documents/uni/vgis10/vgis10/Report/bib/VGIS10.bib:bib},
issn = {19326203},
journal = {PLoS ONE},
month = {sep},
number = {9},
pages = {e106506},
publisher = {Public Library of Science},
title = {{Automatically Detect and track multiple fish swimming in shallow water with frequent occlusion}},
url = {https://dx.plos.org/10.1371/journal.pone.0106506},
volume = {9},
year = {2014}
}
@article{Jung2017,
abstract = {Finding the accurate position of an eye is crucial for mobile iris recognition system in order to extract the iris region quickly and correctly. Unfortunately, this is very difficult to accomplish when a person is wearing eyeglasses because of the interference from the eyeglasses. This paper proposes an eye detection method that is robust to eyeglass interference in mobile environment. The proposed method comprises two stages: eye candidate generation and eye validation. In the eye candidate generation stage, multi-scale window masks consisting of 2 × 3 subblocks are used to generate all image blocks possibly containing an eye image. In the ensuing eye validation stage, two methods are employed to determine which blocks actually contain true eye images and locate their precise positions as well: the first method searches for the glint of an NIR illuminator on the pupil region. If this first method fails, the next method computes the intensity difference between the assumed pupil and its surrounding region using multi-scale 3 × 3 window masks. Experimental results show that the proposed method detects the eye position more accurately and quickly than competing methods in the presence of interference from eyeglass frames.},
author = {Jung, Yujin and Kim, Dongik and Son, Byungjun and Kim, Jaihie},
doi = {10.1016/j.eswa.2016.09.036},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Eye detection,Eye validation,Eyeglasses,Iris detection,Iris recognition,Mobile},
pages = {178--188},
publisher = {Elsevier Ltd},
title = {{An eye detection method robust to eyeglasses for mobile iris recognition}},
url = {http://dx.doi.org/10.1016/j.eswa.2016.09.036},
volume = {67},
year = {2017}
}
@article{Rodriguez2018,
abstract = {{\textcopyright} 2017 The Authors. Methods in Ecology and Evolution {\textcopyright} 2017 British Ecological Society Behavioural analysis based on video recording is becoming increasingly popular within research fields such as; ecology, medicine, ecotoxicology and toxicology. However, the programs available to analyse the data, which are free of cost, user-friendly, versatile, robust, fast and provide reliable statistics for different organisms (invertebrates, vertebrates and mammals) are significantly limited. We present an automated open-source executable software (ToxTrac) for image-based tracking that can simultaneously handle several organisms monitored in a laboratory environment. We compare the performance of ToxTrac with current accessible programs on the web. The main advantages of ToxTrac are as follows: (i) no specific knowledge of the geometry of the tracked bodies is needed; (ii) processing speed, ToxTrac can operate at a rate {\textgreater}25 frames per second in HD videos using modern computers; (iii) simultaneous tracking of multiple organisms in multiple arenas; (iv) integrated distortion correction and camera calibration; (v) robust against false positives; (vi) preservation of individual identification; (vii) useful statistics and heat maps in real scale are exported in image, text and excel formats. ToxTrac can be used for high speed tracking of insects, fish, rodents or other species, and provides useful locomotor information in animal behavior experiments. Download ToxTrac here: https://toxtrac.sourceforge.io (Current version v2.61).},
author = {Rodriguez, Alvaro and Zhang, Hanqing and Klaminder, Jonatan and Brodin, Tomas and Andersson, Patrik L. and Andersson, Magnus},
doi = {10.1111/2041-210X.12874},
file = {::},
issn = {2041210X},
journal = {Methods in Ecology and Evolution},
keywords = {Kalman filter,animal behavior,cockroach,ecology,ecotoxicology,guppy,salmon,tadpole,tracking software,zebrafish},
number = {3},
pages = {460--464},
title = {{ToxTrac: A fast and robust software for tracking organisms}},
url = {https://toxtrac.sourceforge.io},
volume = {9},
year = {2018}
}
@misc{OpenCVTeam,
author = {{Open CV Team}},
title = {{Image Filtering — OpenCV 2.4.13.7 documentation}},
url = {https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html?highlight=gaussianblur{\#}gaussianblur},
urldate = {2019-06-05}
}
@techreport{Chen1988,
abstract = {A modified version of the fast parallel thinning algorithm proposed by Zhang and Suen is presented in this paper. It preserves the original merits such as the contour noise immunity and good effect in thinning crossing lines; and overcomes the original demerits such as the serious shrinking and line connectivity problems. {\textcopyright} 1988.},
author = {Chen, Yung Sheng and Hsu, Wen Hsing},
booktitle = {Pattern Recognition Letters},
doi = {10.1016/0167-8655(88)90124-9},
file = {::},
issn = {01678655},
keywords = {Parallel algorithm,table mapping,thinning},
number = {2},
pages = {99--106},
title = {{A modified fast parallel algorithm for thinning digital patterns}},
url = {http://agcggs680.pbworks.com/f/Zhan-Suen{\_}algorithm.pdf},
volume = {7},
year = {1988}
}
@inproceedings{Christensen2018,
abstract = {{\textcopyright} 2018 Association for Computing Machinery. The rate of evolution of surgical robotics has continuously increased since its inception. Training and experience with the robots is a key factor to successful operations but training is time-consuming and expensive, as the equipment used is expensive and has a short life-time. To explore the possibility of enabling alternative training methods for robot assisted surgery, we designed a multi-user virtual reality simulation of team training as practised in certified institutes and conducted an expert review in cooperation with a training centre for minimally invasive surgery, first nurse assistant and nurse specialist in robot surgery Jane Petersson and head surgeon at Aalborg University Hospital Johan Poulsen. To this end, a contextual study was conducted to ensure realism and accuracy of the simulation. The experts were positive about the system's future, however it was not considered sufficiently complete for use in actual surgery training at this stage. More scenarios and features would be required in future implementations to allow for near full training sessions to be performed in virtual reality.},
author = {Christensen, Nicklas H. and Falk, Frederik and Hjermitslev, Oliver G. and Nikolov, Atanas and Stjernholm, Niclas H. and Kraus, Martin and Poulsen, Johan and Petersson, Jane},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3234253.3234295},
isbn = {9781450353816},
keywords = {Minimally invasive surgery,Robot assisted surgery,Team training,Virtual reality},
title = {{Feasibility of team training in virtual reality for robot-assisted minimally invasive surgery}},
year = {2018}
}
@article{Arslan2017,
abstract = {Biometric systems may be used to create a remote access model on devices, ensure personal data protection, personalize and facilitate the access security. Biometric systems are generally used to increase the security level in addition to the previous authentication methods and they seen as a good solution. Biometry occupies an important place between the areas of daily life of the machine learning. In this study; the techniques, methods, technologies used in biometric systems are researched, machine learning techniques used biometric aplications are investigated for the security perspective, the advantages and disadvantages that these tecniques provide are given. The studies in the literature between 2010-2016 years, used algorithms, technologies, metrics, usage areas, the machine learning techniques used for different biometric systems such as face, palm prints, iris, voice, fingerprint recognition are researched and the studies made are evaluated. The level of security provided by the use of biometric systems by developed using machine learning and disadvantages that arise in the use of these systems are stated in detail in the study. Also, impact on people of biometric methods in terms of ease of use, security and usages areas are examined.},
author = {Arslan, B and Yorulmaz, E and Akca, B and Sagiroglu, S},
doi = {10.1109/ICMLA.2016.183},
isbn = {9781509061662},
journal = {Proceedings - 2016 15th IEEE International Conference on Machine Learning and Applications, ICMLA 2016},
keywords = {Biometric,Face,Fingerprint,Iris,Machine learning,Recognition,Security,Teeth,Voice},
pages = {492--497},
title = {{Security perspective of Biometric recognition and machine learning techniques}},
year = {2017}
}
@inproceedings{Khan2017b,
author = {Khan, Wasim Akram and Pant, Dibakar Raj and Adhikari, Bhisma and Manandhar, Rasana},
booktitle = {Proceeding - IEEE International Conference on Computing, Communication and Automation, ICCCA 2017},
doi = {10.1109/CCAA.2017.8229982},
isbn = {9781509064717},
keywords = {computer vision,gesture recognition,object detection,object tracking},
month = {may},
pages = {1015--1018},
publisher = {IEEE},
title = {{3D object tracking using disparity map}},
url = {http://ieeexplore.ieee.org/document/8229982/},
volume = {2017-Janua},
year = {2017}
}
@inproceedings{deepID2014,
abstract = {The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset, 99.15{\%} face verification accuracy is achieved. Compared with the best deep learning result on LFW, the error rate has been significantly reduced by 67{\%}.},
archivePrefix = {arXiv},
arxivId = {1406.4773},
author = {Sun, Yi and Wang, Xiaogang and Tang, Xiaoou},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.244},
eprint = {1406.4773},
isbn = {9781479951178},
issn = {10636919},
keywords = {deep learning,face verification},
pages = {1891--1898},
pmid = {21808091},
title = {{Deep learning face representation from predicting 10,000 classes}},
url = {http://mmlab.ie.cuhk.edu.hk/pdf/YiSun{\_}CVPR14.pdf},
year = {2014}
}
@article{Galdi2017,
abstract = {FIRE is a Fast Iris REcognition algorithm especially designed for iris recognition on mobile phones under visible-light. It is based on the combination of three classifiers exploiting the iris colour and texture information. Its limited computational time makes FIRE particularly suitable for fast user verification on mobile devices. The high parallelism of the code allows its use also on large databases. FIRE, in its first version, was submitted to the Mobile Iris CHallenge Evaluation part II held in 2016. In this paper, FIRE is further improved: a number of different techniques has been analyzed and the best performing ones have been selected for fusion at score level. Performance are assessed in terms of Recognition Rate (RR), Area Under Receiver Operating Characteristic Curve (AUC), and Equal Error Rate (EER).},
author = {Galdi, Chiara and Dugelay, Jean Luc},
doi = {10.1016/j.patrec.2017.01.023},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Fast iris recognition,MICHE DB,MICHE II,Multi-classifier,Noisy iris recognition,Visible light},
pages = {44--51},
publisher = {Elsevier B.V.},
title = {{FIRE: Fast Iris REcognition on mobile phones by combining colour and texture features}},
url = {http://dx.doi.org/10.1016/j.patrec.2017.01.023},
volume = {91},
year = {2017}
}
@article{Deng2010,
abstract = {Several Human-Machine/Computer Interfaces (HMI/HCI) had been developed in recent years. Some designs were specifically made for people with disabilities such as injured-vertebra, apoplexy or poliomyelitis, Amyotrophic Lateral Sclerosis (ALS), and Motor Neuron Disease, (MND). In this paper, we proposed an eye-movement tracking system. Based on Electro-Oculography (EOG) technology we detected the signal with different directions in eye-movements and then analyzed to understand what they represented about (e.g. horizontal direction or vertical direction). We converted the analog signal to digital signal and then used as the control signals for Human-Computer Interface (HCI). In order to make the system "robust", several applications with EOG-based HCI had been designed. Our preliminary results revealed more than 90{\%} accuracy rate for examining the eye-movement that may become a new useful human-machine user interface in the near future. {\textcopyright}2009 Elsevier Ltd. All rights reserved.},
author = {Deng, Lawrence Y and Hsu, Chun Liang and Lin, Tzu Ching and Tuan, Jui Sen and Chang, Shih Ming},
doi = {10.1016/j.eswa.2009.10.017},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Amyotrophic Lateral Sclerosis,Electro-Oculography (EOG),Eye-movement,Human-Machine/Computer Interface (HMI/HCI),Motor Neuron Disease},
number = {4},
pages = {3337--3343},
publisher = {Elsevier Ltd},
title = {{EOG-based Human-Computer Interface system development}},
url = {http://dx.doi.org/10.1016/j.eswa.2009.10.017},
volume = {37},
year = {2010}
}
@article{Gulmire2012,
author = {Gulmire, Kshamaraj and Ganorkar, Sanjay},
issn = {2278-0181},
number = {5},
pages = {1--5},
title = {{Iris Recognition Using Gabor Wavelet}},
volume = {1},
year = {2012}
}
@article{Khiari-Hili2017a,
author = {Khiari-Hili, Nefissa and Montagne, Christophe and Lelandais, Sylvie and Hamrouni, Kamel},
doi = {10.1109/IPTA.2016.7820954},
isbn = {9781467389105},
journal = {2016 6th International Conference on Image Processing Theory, Tools and Applications, IPTA 2016},
keywords = {Multimodal biometrics,authentication,dynamic weighted sum,face,iris,quality,score fusion},
pages = {1--6},
title = {{Quality dependent multimodal fusion of face and iris biometrics}},
year = {2017}
}
@article{Guo2017,
abstract = {In this paper we study face recognition using convolutional neural network. First, we introduced the basic CNN neural network architecture. Second, we modify the traditional neural network and adapt it to another database by fine tuning its parameters. Third, the network architecture is extended to the cross database problem. The CNN is first trained on a large dataset and then tested on another. Experimental results show that the proposed algorithm is suitable for building various real world applications.},
author = {Guo, Mei and Xiao, Min and Gong, Deliang},
doi = {10.1007/978-3-319-69096-4_54},
file = {::},
journal = {Part of the Advances in Intelligent Systems and Computing book series (AISC, volume 686)},
keywords = {Deep neural network {\'{A}},Face recognition {\'{A}},Image processing},
title = {{Face Recognition Using Deep Convolutional Neural Network in Cross-Database Study}},
url = {https://link-springer-com.zorac.aub.aau.dk/content/pdf/10.1007{\%}2F978-3-319-69096-4{\_}54.pdf},
year = {2017}
}
@article{Luhadiya2017,
annote = {Really good article with a summery of Iris recognition history and approaches in the introduction part.


Keynotes:

Iris database called CASIA with 756 images of 108 people.

SVM used to classify irises.

Elman Recurrent Neural Netowrk used.},
author = {Luhadiya, Ruchi and Khedkar, Anagha},
doi = {10.1109/ICAECCT.2016.7942619},
isbn = {9781509036622},
journal = {2016 IEEE International Conference on Advances in Electronics, Communication and Computer Technology, ICAECCT 2016},
keywords = {GLCM,Hough circular transform,Iris,Machine learning,Person identification,SVM},
pages = {387--392},
title = {{Iris detection for person identification using multiclass SVM}},
year = {2017}
}
@article{Saha2017,
annote = {Not a very good article. but it has some nice references.

Keypoints

It is possible to aquire iris images in multiple way including Near Infrared (NIRD). a simple lense and monochome CCD camera, Adaboost cascade iris detector.

Iris localization is done by Daugman using a 2D Gabor Filter and Fisher Linear Discriminate method.

To help localize the iris despite of eyelashes a 1D rankfilter and histogram filter can be used.},
author = {Saha, Rishmita and Kundu, Mahasweta and Dutta, Madhuparna and Majumder, Rahul and Mukherjee, Debosmita and Pramanik, Sayak and Thakur, Uttam Narendra and Mukherjee, Chiradeep},
isbn = {9781538633717},
journal = {Information Technology, Electronics and Mobile Communication Conference (IEMCON), 2017 8th IEEE Annual},
pages = {685--688},
title = {{A Brief Study on Evolution of Iris Recognition System}},
url = {http://ieeexplore.ieee.org.ezproxy.psu.edu.sa/stamp/stamp.jsp?arnumber=8117234},
year = {2017}
}
@article{Crossdata2018,
abstract = {In this paper we study face recognition using convolutional neural network. First, we introduced the basic CNN neural network architecture. Second, we modify the traditional neural network and adapt it to another database by fine tuning its parameters. Third, the network architecture is extended to the cross database problem. The CNN is first trained on a large dataset and then tested on another. Experimental results show that the proposed algorithm is suitable for building various real world applications.},
author = {Guo, Mei and Xiao, Min and Gong, Deliang},
doi = {10.1007/978-3-319-69096-4_54},
file = {::},
journal = {Advances in Intelligent Systems and Computing},
keywords = {Deep neural network {\'{A}},Face recognition {\'{A}},Image processing},
title = {{Face Recognition Using Deep Convolutional Neural Network in Cross-Database Study}},
url = {https://link-springer-com.zorac.aub.aau.dk/content/pdf/10.1007{\%}2F978-3-319-69096-4{\_}54.pdf},
year = {2017}
}
@article{sun2015,
abstract = {The state-of-the-art of face recognition has been significantly advanced by the emergence of deep learning. Very deep neural networks recently achieved great success on general object recognition because of their superb learning capacity. This motivates us to investigate their effectiveness on face recognition. This paper proposes two very deep neural network architectures, referred to as DeepID3, for face recognition. These two architectures are rebuilt from stacked convolution and inception layers proposed in VGG net and GoogLeNet to make them suitable to face recognition. Joint face identification-verification supervisory signals are added to both intermediate and final feature extraction layers during training. An ensemble of the proposed two architectures achieves 99.53{\%} LFW face verification accuracy and 96.0{\%} LFW rank-1 face identification accuracy, respectively. A further discussion of LFW face verification result is given in the end.},
archivePrefix = {arXiv},
arxivId = {1502.00873},
author = {Sun, Yi and Liang, Ding and Wang, Xiaogang and Tang, Xiaoou},
eprint = {1502.00873},
title = {{DeepID3: Face Recognition with Very Deep Neural Networks}},
url = {https://arxiv.org/pdf/1502.00873.pdf http://arxiv.org/abs/1502.00873},
year = {2015}
}
@article{Dessimoz2007,
abstract = {The MBioID initiative has been set up to address the following germane question: What and how biometric technologies could be deployed in identity documents in the foreseeable future? This research effort proposes to look at current and future practices and systems of establishing and using biometric identity documents (IDs) and evaluate their effectiveness in large-scale developments. The first objective of the MBioID project is to present a review document establishing the current state-of-the-art related to the use of multimodal biometrics in an IDs application. This research report gives the main definitions, properties and the framework of use related to biometrics, an overview of the main standards developed in the biometric industry and standardisation organisations to ensure interoperability, as well as some of the legal framework and the issues associated to biometrics such as privacy and personal data protection. The state-of-the-art in terms of technological development is also summarised for a range of single biometric modalities (2D and 3D face, fingerprint, iris, on-line signature and speech), chosen according to ICAO recommendations and availabilities, and for various multimodal approaches. This paper gives a summary of the main elements of that report. The second objective of the MBioID project is to propose relevant acquisition and evaluation protocols for a large-scale deployment of biometric IDs. Combined with the protocols, a multimodal database will be acquired in a realistic way, in order to be as close as possible to a real biometric IDs deployment. In this paper, the issues and solutions related to the acquisition setup are briefly presented. {\textcopyright} 2006 Elsevier Ireland Ltd. All rights reserved.},
author = {Dessimoz, Damien and Richiardi, Jonas and Champod, Christophe and Drygajlo, Andrzej},
doi = {10.1016/j.forsciint.2006.06.037},
issn = {03790738},
journal = {Forensic Science International},
keywords = {Acquisition protocol,Biometrics,Electronic passport,Evaluation protocol,Identity documents,Multimodality},
number = {2-3},
pages = {154--159},
pmid = {16890391},
title = {{Multimodal biometrics for identity documents ({\{}A figure is presented{\}})}},
volume = {167},
year = {2007}
}
@article{Baltrusaitis2017a,
abstract = {Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.},
archivePrefix = {arXiv},
arxivId = {1705.09406},
author = {Baltru{\v{s}}aitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
doi = {10.1109/TPAMI.2018.2798607},
eprint = {1705.09406},
issn = {0162-8828},
number = {c},
pages = {1--20},
title = {{Multimodal Machine Learning: A Survey and Taxonomy}},
url = {http://arxiv.org/abs/1705.09406},
volume = {8828},
year = {2017}
}
@article{Gopal2018a,
author = {Gopal and Srivastava, Smriti},
doi = {10.1007/s13369-017-2644-6},
issn = {21914281},
journal = {Arabian Journal for Science and Engineering},
keywords = {Feature-level fusion,Multimodal system,Palm–phalanges,Score-level fusion,Unimodal system},
number = {2},
pages = {543--554},
publisher = {Springer Berlin Heidelberg},
title = {{Accurate Human Recognition by Score-Level and Feature-Level Fusion Using Palm–Phalanges Print}},
volume = {43},
year = {2018}
}
@article{Al-Waisy2017a,
abstract = {Multimodal biometrie systems seek to alleviate some of the limitations of unimodal biometrie systems by combining multiple pieces of evidence of the same person in the deeision-making process. In this paper, a novel multimodal biometric identification system is proposed based on fusing the results obtained from both the face and the left and right irises using deep learning approaches. Firstly, the facial features are extracted using a Deep Belief Network (DBN) architecture consisting of 3-layers. The first two RBMs are used as features detectors, while the last one is used as a discriminative model associated with softmax units for the multi-class classification purpose. Secondly, an efficient deep learning system is employed for iris recognition, whose architecture is based on a combination of Convolutional Neural Network (CNN) and Softmax classifier to extract discriminative features from an iris image. Extensive experiments on large-scale challenging databases, including FERET, CASIA V1.0 and MMU1, and SDUMLA-HMT have demonstrated the superiority of the proposed approaches by achieving new state-of-the-art Rank-1 identification rates on all the employed databases.},
author = {Al-Waisy, A S and Qahwaji, R and Ipson, S and Al-Fahdawi, S},
doi = {10.1109/EST.2017.8090417},
isbn = {9781538640173},
journal = {2017 Seventh International Conference on Emerging Security Technologies (EST)},
keywords = {Databases;Face;Feature extraction;Iris recognition},
pages = {163--168},
title = {{A multimodal biometrie system for personal identification based on deep learning approaches}},
year = {2017}
}
@article{Chandel2015,
abstract = {Object tracking and detection is a classical research area in the field of computer vision from decades. Numerous kinds of applications are dependent on the area of object detection, such as advance driving assistance system, traffic surveillance, scene understanding, autonomous navigation etc. Many challenges still exist while detecting an object such as illusion, low visibility, cast shadows and most importantly occlusions of object. Occlusions occur under two categories, firstly its, self‐occlusion which means that, from a certain viewpoint, one part of an object is occluded by another part. Secondly, its inter-object occlusion which means when two objects being tracked occlude each other. We will review various occlusion handling methods that involved single and multiple cameras according to their application. In short, the objective of this paper is to deliberate in detail the problem of occlusion in object tracking and provide a concise review for the problem of occlusion handling under different categories and identify new trends.},
author = {Chandel, Himanshu and Vatta, Sonia},
doi = {10.5120/21264-3857},
issn = {09758887},
journal = {International Journal of Computer Applications},
month = {jun},
number = {10},
pages = {33--38},
title = {{Occlusion Detection and Handling: A Review}},
url = {http://research.ijcaonline.org/volume120/number10/pxc3903857.pdf},
volume = {120},
year = {2015}
}
@article{Leal-Taixe2017,
archivePrefix = {arXiv},
arxivId = {1704.02781},
author = {Leal-Taix{\'{e}}, Laura and Milan, Anton and Schindler, Konrad and Cremers, Daniel and Reid, Ian and Roth, Stefan},
eprint = {1704.02781},
file = {::},
month = {apr},
title = {{Tracking the Trackers: An Analysis of the State of the Art in Multiple Object Tracking}},
url = {https://arxiv.org/abs/1704.02781},
year = {2017}
}
@article{Cheng2017a,
abstract = {Sea–land segmentation and ship detection are two prevalent research domains for optical remote sensing harbor images and can find many applications in harbor supervision and management. As the spatial resolution of imaging technology improves, traditional methods struggle to perform well due to the complicated appearance and background distributions. In this paper, we unify the above two tasks into a single framework and apply the deep convolutional neural networks to predict pixelwise label for an input. Specifically, an edge aware convolutional network is proposed to parse a remote sensing harbor image into three typical objects, e.g., sea, land, and ship. Two innovations are made on top of the deep structure. First, we design a multitask model by simultaneously training the segmentation and edge detection networks. Hierarchical semantic features from the segmentation network are extracted to learn the edge network. Second, the outputs of edge pipeline are further employed to refine entire model by adding an edge aware regularization, which helps our method to yield very desirable results that are spatially consistent and well boundary located. It also benefits the segmentation of docked ships that are quite challenging for many previous methods. Experimental results on two datasets collected from Google Earth have demonstrated the effectiveness of our approach both in quantitative and qualitative performance compared with state-of-the-art methods.},
author = {Cheng, Dongcai and Meng, Gaofeng and Xiang, Shiming and Pan, Chunhong},
doi = {10.1109/JSTARS.2017.2747599},
issn = {21511535},
journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
keywords = {Edge aware regularization,harbor images,multitask learning,semantic segmentation},
number = {12},
pages = {5769--5783},
title = {{FusionNet: Edge Aware Deep Convolutional Networks for Semantic Segmentation of Remote Sensing Harbor Images}},
volume = {10},
year = {2017}
}
@article{Nguyen2010,
author = {Nguyen, Kien and Fookes, Clinton and Sridharan, Sridha},
doi = {10.1145/1852611.1852635},
isbn = {9781450301053},
journal = {Proceedings of the 2010 Symposium on Information and Communication Technology - SoICT '10},
keywords = {iris recognition,mbgc,signal-level fusion,super-resolution},
number = {November},
pages = {122},
title = {{Robust mean super-resolution for less cooperative NIR iris recognition at a distance and on the move}},
url = {http://portal.acm.org/citation.cfm?doid=1852611.1852635},
year = {2010}
}
@article{Furman1950,
abstract = {Axonal sprouting of excitatory neurons is frequently observed in{\$}\backslash{\$}ntemporal lobe epilepsy, but the extent to which inhibitory interneurons{\$}\backslash{\$}nundergo similar axonal reorganization remains unclear. The goal of this{\$}\backslash{\$}nstudy was to determine whether somatostatin (SOM)-expressing neurons in{\$}\backslash{\$}nstratum (s.) oriens of the hippocampus exhibit axonal sprouting beyond{\$}\backslash{\$}ntheir normal territory and innervate granule cells of the dentate gyrus{\$}\backslash{\$}nin a pilocarpine model of epilepsy. To obtain selective labeling of{\$}\backslash{\$}nSOM-expressing neurons in s. oriens, a Cre recombinase-dependent{\$}\backslash{\$}nconstruct for channelrhodopsin2 fused to enhanced yellow fluorescent{\$}\backslash{\$}nprotein (ChR2-eYFP) was virally delivered to this region in SOM-Cre{\$}\backslash{\$}nmice. In control mice, labeled axons were restricted primarily to s.{\$}\backslash{\$}nlacunosum-moleculare. However, in pilocarpine-treated animals, a rich{\$}\backslash{\$}nplexus of ChR2-eYFP-labeled fibers and boutons extended into the dentate{\$}\backslash{\$}nmolecular layer. Electron microscopy with immunogold labeling{\$}\backslash{\$}ndemonstrated labeled axon terminals that formed symmetric synapses on{\$}\backslash{\$}ndendritic profiles in this region, consistent with innervation of{\$}\backslash{\$}ngranule cells. Patterned illumination of ChR2-labeled fibers in s.{\$}\backslash{\$}nlacunosum-moleculare of CA1 and the dentate molecular layer elicited{\$}\backslash{\$}nGABAergic inhibitory responses in dentate granule cells in{\$}\backslash{\$}npilocarpine-treated mice but not in controls. Similar optical{\$}\backslash{\$}nstimulation in the dentate hilus evoked no significant responses in{\$}\backslash{\$}ngranule cells of either group of mice. These findings indicate that{\$}\backslash{\$}nunder pathological conditions, SOM/GABAergic neurons can undergo{\$}\backslash{\$}nsubstantial axonal reorganization beyond their normal territory and{\$}\backslash{\$}nestablish aberrant synaptic connections. Such reorganized circuitry{\$}\backslash{\$}ncould contribute to functional deficits in inhibition in epilepsy,{\$}\backslash{\$}ndespite the presence of numerous GABAergic terminals in the region.},
author = {Furman, Joseph M and Wuyts, Floris L and Siddiqui, Uzma and Shaikh, A N and Lord, Mary P and Wright, W D and Colegatet, Robert L and Hoffman, James E},
doi = {10.1016/B978-1-4557-0308-1.00032-7},
edition = {6},
isbn = {978-1-4665-5543-3},
issn = {2278-1021},
journal = {Reports on Progress in Physics},
keywords = {adc,analogdigitalconverter,eeg,electroencefalogram,electromyalgy,electrooculography,emg,eog,rapid eye movement,rem,sem,slow eye movement},
number = {1},
pages = {4328--4330},
publisher = {Elsevier Inc.},
title = {{The investigation of eye movements}},
url = {www.ijarcce.com http://dx.doi.org/10.1016/B978-1-4557-0308-1.00032-7},
volume = {2},
year = {1950}
}
@article{Zhang2017a,
abstract = {Multimodal classification arises in many computer vi-sion tasks such as object classification and image retrieval. The idea is to utilize multiple sources (modalities) measur-ing the same instance to improve the overall performance compared to using a single source (modality). The varying characteristics exhibited by multiple modalities make it nec-essary to simultaneously learn the corresponding distance metrics. In this paper, we propose a multiple metrics learn-ing algorithm for multimodal data. Metric of each modal-ity is product of two matrices: one matrix is modality spe-cific, the other is enforced to be shared by all the modalities. The learned metrics can improve multimodal classification accuracy and experimental results on four datasets show that the proposed algorithm outperforms existing learning algorithms based on multiple metrics as well as other ap-proaches tested on these datasets. Specifically, we report 95.0{\%} object instance recognition accuracy, 89.2{\%} object category recognition accuracy on the multi-view RGB-D dataset and 52.3{\%} scene category recognition accuracy on SUN RGB-D dataset.},
author = {Zhang, Heng and Patel, Vishal M. and Chellappa, Rama},
doi = {10.1109/CVPR.2017.312},
isbn = {978-1-5386-0457-1},
issn = {1063-6919},
journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {2925--2933},
title = {{Hierarchical Multimodal Metric Learning for Multimodal Classification}},
url = {http://ieeexplore.ieee.org/document/8099795/},
year = {2017}
}
@article{d2s,
abstract = {We introduce the Densely Segmented Supermarket (D2S) dataset, a novel benchmark for instance-aware semantic segmentation in an industrial domain. It contains 21 000 high-resolution images with pixel-wise labels of all object instances. The objects comprise groceries and everyday products from 60 categories. The benchmark is designed such that it resembles the real-world setting of an automatic checkout, inventory, or warehouse system. The training images only contain objects of a single class on a homogeneous background, while the validation and test sets are much more complex and diverse. To further benchmark the robustness of instance segmentation methods, the scenes are acquired with different lightings, rotations, and backgrounds. We ensure that there are no ambiguities in the labels and that every instance is labeled comprehensively. The annotations are pixel-precise and allow using crops of single instances for articial data augmentation. The dataset covers several challenges highly relevant in the field, such as a limited amount of training data and a high diversity in the test and validation sets. The evaluation of state-of-the-art object detection and instance segmentation methods on D2S reveals significant room for improvement.},
author = {Follmann, Patrick and B{\"{o}}ttger, Tobias and H{\"{a}}rtinger, Philipp and K{\"{o}}nig, Rebecca and Ulrich, Markus},
file = {::},
journal = {European Conference on Computer Vision (ECCV)},
keywords = {industrial application,instance segmentation,segmentation dataset},
pages = {569--585},
title = {{MVTec D2S: Densely Segmented Supermarket Dataset}},
url = {www.mvtec.com},
year = {2018}
}
@article{Hu2015,
abstract = {Deep learning, in particular Convolutional Neural Network (CNN), has achieved promising results in face recognition recently. However, it remains an open question: why CNNs work well and how to design a 'good' architecture. The existing works tend to focus on reporting CNN architectures that work well for face recognition rather than investigate the reason. In this work, we conduct an extensive evaluation of CNN-based face recognition systems (CNN-FRS) on a common ground to make our work easily reproducible. Specifically, we use public database LFW (Labeled Faces in the Wild) to train CNNs, unlike most existing CNNs trained on private databases. We propose three CNN architectures which are the first reported architectures trained using LFW data. This paper quantitatively compares the architectures of CNNs and evaluate the effect of different implementation choices. We identify several useful properties of CNN-FRS. For instance, the dimensionality of the learned features can be significantly reduced without adverse effect on face recognition accuracy. In addition, traditional metric learning method exploiting CNN-learned features is evaluated. Experiments show two crucial factors to good CNN-FRS performance are the fusion of multiple CNNs and metric learning. To make our work reproducible, source code and models will be made publicly available.},
archivePrefix = {arXiv},
arxivId = {1504.02351},
author = {Hu, Guosheng and Yang, Yongxin and Yi, Dong and Kittler, Josef and Christmas, William and Li, Stan Z and Hospedales, Timothy},
doi = {10.1109/ICCVW.2015.58},
eprint = {1504.02351},
isbn = {9780769557205},
issn = {15505499},
journal = {2015 IEEE International Conference on Computer Vision Workshop (ICCVW)},
month = {dec},
pages = {384--392},
publisher = {IEEE},
title = {{When Face Recognition Meets with Deep Learning: an Evaluation of Convolutional Neural Networks for Face Recognition}},
url = {http://ieeexplore.ieee.org/document/7406407/ http://arxiv.org/abs/1504.02351},
year = {2015}
}
@article{Zapata2017a,
author = {Zapata, J C and Duque, C M and Gonzalez, M E},
doi = {10.1007/978-981-10-5427-3},
isbn = {978-981-10-5426-6},
keywords = {biometric {\'{a}} data fusion,signals {\'{a}} signal processing},
number = {i},
pages = {721--733},
title = {{Advances in Computing and Data Sciences}},
url = {http://link.springer.com/10.1007/978-981-10-5427-3},
volume = {721},
year = {2017}
}
@inproceedings{tless,
abstract = {We introduce T-LESS, a new public dataset for estimating the 6D pose, i.e. translation and rotation, of texture-less rigid objects. The dataset features thirty industry-relevant objects with no significant texture and no discriminative color or reflectance properties. The objects exhibit symmetries and mutual similarities in shape and/or size. Compared to other datasets, a unique property is that some of the objects are parts of others. The dataset includes training and test images that were captured with three synchronized sensors, specifically a structured-light and a time-of-flight RGB-D sensor and a high-resolution RGB camera. There are approximately 39K training and 10K test images from each sensor. Additionally, two types of 3D models are provided for each object, i.e. a manually created CAD model and a semi-automatically reconstructed one. Training images depict individual objects against a black background. Test images originate from twenty test scenes having varying complexity, which increases from simple scenes with several isolated objects to very challenging ones with multiple instances of several objects and with a high amount of clutter and occlusion. The images were captured from a systematically sampled view sphere around the object/scene, and are annotated with accurate ground truth 6D poses of all modeled objects. Initial evaluation results indicate that the state of the art in 6D object pose estimation has ample room for improvement, especially in difficult cases with significant occlusion. The T-LESS dataset is available online at cmp.felk.cvut.cz/t-less.},
archivePrefix = {arXiv},
arxivId = {1701.05498},
author = {Hodaň, Tom{\'{a}}{\v{s}} and Haluza, Pavel and Obdrzalek, {\v{S}}t{\v{e}}p{\'{a}}n and Matas, Jiř{\'{i}} and Lourakis, Manolis and Zabulis, Xenophon},
booktitle = {Proceedings - 2017 IEEE Winter Conference on Applications of Computer Vision, WACV 2017},
doi = {10.1109/WACV.2017.103},
eprint = {1701.05498},
file = {::},
isbn = {9781509048229},
issn = {2472-6737},
pages = {880--888},
title = {{T-LESS: An RGB-D dataset for 6D pose estimation of texture-less objects}},
url = {http://cmp.felk.cvut.cz/{~}hodanto2/data/hodan2017tless.pdf},
year = {2017}
}
@article{HongWang2016,
abstract = {Zebrafish (Danio rerio) is one of the most widely used model organisms in collective behavior research. Multi-object tracking with high speed camera is currently the most feasible way to accurately measure their motion states for quantitative study of their collective behavior. However, due to difficulties such as their similar appearance, complex body deformation and frequent occlusions, it is a big challenge for an automated system to be able to reliably track the body geometry of each individual fish. To accomplish this task, we propose a novel fish body model that uses a chain of rectangles to represent fish body. Then in detection stage, the point of maximum curvature along fish boundary is detected and set as fish nose point. Afterwards, in tracking stage, we firstly apply Kalman filter to track fish head, then use rectangle chain fitting to fit fish body, which at the same time further judge the head tracking results and remove the incorrect ones. At last, a tracklets relinking stage further solves trajectory fragmentation due to occlusion. Experiment results show that the proposed tracking system can track a group of zebrafish with their body geometry accurately even when occlusion occurs from time to time.},
author = {{Hong Wang}, Shuo and {En Cheng}, Xi and Qian, Zhi-Ming and Liu, Ye and {Qiu Chen}, Yan},
doi = {10.1371/journal.pone.0154714},
file = {::},
title = {{Automated Planar Tracking the Waving Bodies of Multiple Zebrafish Swimming in Shallow Water}},
url = {https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0154714{\&}type=printable},
year = {2016}
}
@inproceedings{Tan2014,
abstract = {In this paper, we address the problem of object tracking in intensity images and depth data. We propose a generic framework that can be used either for tracking 2D templates in intensity images or for tracking 3D objects in depth images. To overcome problems like partial occlusions, strong illumination changes and motion blur, that notoriously make energy minimization-based tracking methods get trapped in a local minimum, we propose a learning-based method that is robust to all these problems. We use random forests to learn the relation between the parameters that defines the object's motion, and the changes they induce on the image intensities or the point cloud of the template. It follows that, to track the template when it moves, we use the changes on the image intensities or point cloud to predict the parameters of this motion. Our algorithm has an extremely fast tracking performance running at less than 2 ms per frame, and is robust to partial occlusions. Moreover, it demonstrates robustness to strong illumination changes when tracking templates using intensity images, and robustness in tracking 3D objects from arbitrary viewpoints even in the presence of motion blur that causes missing or erroneous data in depth images. Extensive experimental evaluation and comparison to the related approaches strongly demonstrates the benefits of our method.},
author = {Tan, David Joseph and Ilic, Slobodan},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.157},
isbn = {9781479951178},
issn = {10636919},
month = {jun},
pages = {1202--1209},
publisher = {IEEE},
title = {{Multi-forest tracker: A Chameleon in tracking}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909553},
year = {2014}
}
@article{Vivek2012a,
author = {Vivek, S. Arun and Aravinth, J. and Valarmathy, S.},
doi = {10.1109/ICPRIME.2012.6208377},
isbn = {978-1-4673-1039-0},
journal = {International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME-2012)},
keywords = {density based score level,error,feature extraction,fusion,gmm,likelihood ratio test,multimodal biometrics,rates,template,unimodal biometrics},
pages = {387--392},
title = {{Feature extraction for multimodal biometric and study of fusion using Gaussian mixture model}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6208377},
year = {2012}
}
@article{idtracker2014,
abstract = {Animals in groups touch each other, move in paths that cross, and interact in complex ways. current video tracking methods sometimes switch identities of unmarked individuals during these interactions. these errors propagate and result in random assignments after a few minutes unless manually corrected. We present idtracker, a multitracking algorithm that extracts a characteristic fingerprint from each animal in a video recording of a group. it then uses these fingerprints to identify every individual throughout the video. tracking by identification prevents propagation of errors, and the correct identities can be maintained indefinitely. idtracker distinguishes animals even when humans cannot, such as for size-matched siblings, and reidentifies animals after they temporarily disappear from view or across different videos. it is robust, easy to use and general. We tested it on fish (Danio rerio and Oryzias latipes), flies (Drosophila melanogaster), ants (Messor structor) and mice (Mus musculus).},
author = {P{\'{e}}rez-escudero, Alfonso and Vicente-page, Juli{\'{a}}n and Hinz, Robert C and Arganda, Sara and {De Polavieja}, Gonzalo G.},
doi = {10.1038/Nmeth.2994},
file = {:home/niclas/Documents/uni/vgis10/vgis10/Report/bib/VGIS10.bib:bib},
journal = {Nature Methods},
number = {7},
pages = {743--748},
title = {{idTracker: tracking individuals in a group by automatic identification of unmarked animals}},
url = {http://www.idtracker.es/},
volume = {11},
year = {2014}
}
@article{Lee2017,
abstract = {In recent years, the iris recognition system has been gaining increasing acceptance for applications such as access control and smartphone security. When the images of the iris are obtained under unconstrained conditions, an issue of undermined quality is caused by optical and motion blur, off-angle view (the user's eyes looking somewhere else, not into the front of the camera), specular reflection (SR) and other factors. Such noisy iris images increase intra-individual variations and, as a result, reduce the accuracy of iris recognition. A typical iris recognition system requires a near-infrared (NIR) illuminator along with an NIR camera, which are larger and more expensive than fingerprint recognition equipment. Hence, many studies have proposed methods of using iris images captured by a visible light camera without the need for an additional illuminator. In this research, we propose a new recognition method for noisy iris and ocular images by using one iris and two periocular regions, based on three convolutional neural networks (CNNs). Experiments were conducted by using the noisy iris challenge evaluation-part II (NICE.II) training dataset (selected from the university of Beira iris (UBIRIS).v2 database), mobile iris challenge evaluation (MICHE) database, and institute of automation of Chinese academy of sciences (CASIA)-Iris-Distance database. As a result, the method proposed by this study outperformed previous methods.},
author = {Lee, Min Beom and Hong, Hyung Gil and Park, Kang Ryoung},
doi = {10.3390/s17122933},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Convolutional neural network,Iris and periocular,Noisy iris and ocular image},
number = {12},
pmid = {29258217},
title = {{Noisy ocular recognition based on three convolutional neural networks}},
volume = {17},
year = {2017}
}
@inproceedings{Sangeetha2013,
abstract = {-In a Multimodal biometric system, the effective fusion method is necessary for combining information from various single modality systems. Two biometric characteristics are considered in this study: iris and fingerprint. Multimodal biometric system needs an effective fusion scheme to combine biometric characteristics derived from one or more modalities. The score level fusion is used to combine the characteristics from diff erent biometric modalities. Fusion at the score level is a new technique, which has a high potential for e f ficient consolidation of multiple unimodal biometric matcher outputs. Support vector machine and extreme learning techniques are used in this system for recognition of biometric traits. In this, the Fingerprint-Iris system provides better performance and comparison of support vector machine and extreme learning machine based on score-level fusion methods is obtained In score-level fusion, ELM provides better performance as compare to the SVM It reduces the classification time of current system. This work is valuable and makes an e f ficient accuracy in such applications. This system can be utilized for person identification in several applications.},
author = {Sangeetha, S and Radha, N.},
booktitle = {2013 7th International Conference on Intelligent Systems and Control (ISCO)},
doi = {10.1109/ISCO.2013.6481145},
isbn = {978-1-4673-4603-0},
month = {jan},
pages = {183--188},
publisher = {IEEE},
title = {{A New Framework for IRIS and Fingerprint Recognition Using SVM Classification and Extreme Learning Machine Based on Score Level Fusion}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6481145},
year = {2013}
}
@article{Rifaee2017,
author = {Rifaee, Mustafa and Abdallah, Mohammad and Okosh, Basem},
journal = {international journal of multimedia {\&} its applications},
number = {April},
title = {{A Short Survey for Iris Images Databases}},
url = {https://www.researchgate.net/publication/316093004{\_}A{\_}Short{\_}Survey{\_}for{\_}Iris{\_}Images{\_}Databases},
year = {2017}
}
@misc{Weisstein,
abstract = {Green's theorem is a vector identity which is equivalent to the curl theorem in the plane. Over a region D in the plane with boundary partialD, Green's theorem states ∮{\_}(partialD)P(x,y)dx+Q(x,y)dy=intint{\_}(D)((partialQ)/(partialx)-(partialP)/(partialy))dxdy, (1) where the left side is a line integral and the right side is a surface integral. This can also be written compactly in vector form as ∮{\_}(partialD)F{\textperiodcentered}ds=intint{\_}(D)(del xF){\textperiodcentered}da. (2) If the region D is on the...},
author = {Weisstein, Eric W.},
publisher = {Wolfram Research, Inc.},
title = {{Green's Theorem}},
url = {http://mathworld.wolfram.com/GreensTheorem.html},
urldate = {2019-06-03}
}
@article{Hqwhu,
author = {Hqwhu, E and Vndudkdq, Qvndudnrf and Jwx, Dnjxo and Wu, H G X},
keywords = {convolutional neural,deep learning,network,pupil center estimation},
title = {{Deep learning based estimation of the eye pupil center by using image patch classification}}
}
@inproceedings{Misztal2012,
abstract = {A new method for finding the rotation angle in iris images for biometric identification is presented in this paper. The proposed approach is based on Fourier descriptors analysis and algebraic properties of vector rotation in complex space. {\textcopyright} 2012 IFIP International Federation for Information Processing.},
author = {Misztal, Krzysztof and Tabor, Jacek and Saeed, Khalid},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-33260-9-11},
isbn = {9783642332593},
issn = {03029743},
keywords = {Fourier descriptors,Iris pattern recognition,rotation estimation,rotation recovery},
pages = {135--145},
title = {{A new algorithm for rotation detection in iris pattern recognition}},
volume = {7564 LNCS},
year = {2012}
}
@article{Ortega-Garcia2010,
abstract = {A new multimodal biometric database designed and acquired within the framework of the European BioSecure Network of Excellence is presented. It is comprised of more than 600 individuals acquired simultaneously in three scenarios: 1) over the Internet, 2) in an office environment with desktop PC, and 3) in indoor/outdoor environments with mobile portable hardware. The three scenarios include a common part of audio/video data. Also, signature and fingerprint data have been acquired both with desktop PC and mobile portable hardware. Additionally, hand and iris data were acquired in the second scenario using desktop PC. Acquisition has been conducted by 11 European institutions. Additional features of the BioSecure Multimodal Database (BMDB) are: two acquisition sessions, several sensors in certain modalities, balanced gender and age distributions, multimodal realistic scenarios with simple and quick tasks per modality, cross-European diversity, availability of demographic data, and compatibility with other multimodal databases. The novel acquisition conditions of the BMDB allow us to perform new challenging research and evaluation of either monomodal or multimodal biometric systems, as in the recent BioSecure Multimodal Evaluation campaign. A description of this campaign including baseline results of individual modalities from the new database is also given. The database is expected to be available for research purposes through the BioSecure Association during 2008.},
author = {Ortega-Garcia, Javier and Fierrez, Julian and Alonso-Fernandez, Fernando and Galbally, Javier and Freire, Manuel R. and Gonzalez-Rodriguez, Joaquin and Garcia-Mateo, Carmen and Alba-Castro, Jose Luis and Gonzalez-Agulla, Elisardo and Otero-Muras, Enrique and Garcia-Salicetti, Sonia and Allano, Lorene and Ly-Van, Bao and Dorizzi, Bernadette and Kittler, Josef and Bourlai, Thirimachos and Poh, Norman and Deravi, Farzin and Ng, Ming N.R. and Fairhurst, Michael and Hennebert, Jean and Humm, Andreas and Tistarelli, Massimo and Brodo, Linda and Richiardi, Jonas and Drygajlo, Andrezj and Ganster, Harald and Sukno, Federico M. and Pavani, Sri Kaushik and Frangi, Alejandro and Akarun, Lale and Savran, Arman},
doi = {10.1109/TPAMI.2009.76},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Benchmark,Biometrics,Database,Evaluation,Face,Fingerprint,Hand,Iris.,Multimodal,Performance,Signature,Speaker,Voice},
number = {6},
pages = {1097--1111},
pmid = {20431134},
title = {{The multiscenario multienvironment biosecure multimodal database (BMDB)}},
volume = {32},
year = {2010}
}
@article{Soviany2016,
abstract = {– This paper presents a design approach of a reliable authentication system for mobile applications (such as those within m-Health or m-Banking areas). This means that the biometric data processing should optimize the security performance vs. the computational complexity. The security is given by the combination of fingerprint, iris and voice features that define the multimodal pattern of an individual. The complexity reduction is supported by a reduced feature space, especially for the fingerprint and iris recognition components of the overall system.},
author = {Soviany, Sorin and Săndulescu, Virginia and Puşcoci, Sorin},
isbn = {9781509020478},
journal = {Computers and Artificial Intelligence},
keywords = {-multimodal,biometrics,data fusion,identification},
title = {{A Multimodal Biometric Identification Method for Mobile Applications Security}},
volume = {30},
year = {2016}
}
@article{Daia,
abstract = {Deep networks have shown impressive performance on many computer vision tasks. Recently, deep convolutional neural networks (CNNs) have been used to learn discrim-inative texture representations. One of the most successful approaches is Bilinear CNN model that explicitly captures the second order statistics within deep features. However, these networks cut off the first order information flow in the deep network and make gradient back-propagation dif-ficult. We propose an effective fusion architecture -FASON that combines second order information flow and first or-der information flow. Our method allows gradients to back-propagate through both flows freely and can be trained ef-fectively. We then build a multi-level deep architecture to exploit the first and second order information within dif-ferent convolutional layers. Experiments show that our method achieves improvements over state-of-the-art meth-ods on several benchmark datasets.},
author = {Dai, Xiyang and Ng, Joe Yue-hei and Davis, Larry S},
journal = {Cvpr2017},
pages = {7352--7360},
title = {{FASON : First and Second Order Information Fusion Network for Texture Recognition}}
}
@inproceedings{Sun2014,
abstract = {The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset, 99.15{\%} face verification accuracy is achieved. Compared with the best deep learning result on LFW, the error rate has been significantly reduced by 67{\%}.},
archivePrefix = {arXiv},
arxivId = {1406.4773},
author = {Sun, Yi and Wang, Xiaogang and Tang, Xiaoou},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.244},
eprint = {1406.4773},
isbn = {9781479951178},
issn = {10636919},
keywords = {deep learning,face verification},
pages = {1891--1898},
pmid = {21808091},
title = {{Deep Learning Face Representation by Joint Identification-Verification}},
url = {https://arxiv.org/pdf/1406.4773.pdf http://arxiv.org/abs/1406.4773},
year = {2014}
}
@article{Ma2015,
abstract = {This study presents a novel human-machine interface (HMI) based on both electrooculography (EOG) and electroencephalography (EEG). This hybrid interface works in two modes: an EOG mode recognizes eye movements such as blinks, and an EEG mode detects event related potentials (ERPs) like P300. While both eye movements and ERPs have been separately used for implementing assistive interfaces, which help patients with motor disabilities in performing daily tasks, the proposed hybrid interface integrates them together. In this way, both the eye movements and ERPs complement each other. Therefore, it can provide a better efficiency and a wider scope of application. In this study, we design a threshold algorithm that can recognize four kinds of eye movements including blink, wink, gaze, and frown. In addition, an oddball paradigm with stimuli of inverted faces is used to evoke multiple ERP components including P300, N170, and VPP. To verify the effectiveness of the proposed system, two different online experiments are carried out. One is to control a multifunctional humanoid robot, and the other is to control four mobile robots. In both experiments, the subjects can complete tasks effectively by using the proposed interface, whereas the best completion time is relatively short and very close to the one operated by hand.},
author = {Ma, Jiaxin and Zhang, Yu and Cichocki, Andrzej and Matsuno, Fumitoshi},
doi = {10.1109/TBME.2014.2369483},
isbn = {0018-9294},
issn = {15582531},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {Electroencephalogram (EEG),Electrooculogram (EOG),event-related potential (ERP),human-machine interface (HMI),robot control},
number = {3},
pages = {876--889},
pmid = {25398172},
title = {{A novel EOG/EEG hybrid human-machine interface adopting eye movements and ERPs: Application to robot control}},
volume = {62},
year = {2015}
}
@article{Redmon2018,
abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
archivePrefix = {arXiv},
arxivId = {1804.02767},
author = {Redmon, Joseph and Farhadi, Ali},
eprint = {1804.02767},
file = {::},
month = {apr},
title = {{YOLOv3: An Incremental Improvement}},
url = {http://arxiv.org/abs/1804.02767},
year = {2018}
}
@article{Nam2014,
abstract = {We present a novel human-machine interface, called GOM-Face , and its application to humanoid robot control. The GOM-Face bases its interfacing on three electric potentials measured on the face: 1) glossokinetic potential (GKP), which involves the tongue movement; 2) electrooculogram (EOG), which involves the eye movement; 3) electromyogram, which involves the teeth clenching. Each potential has been individually used for assistive interfacing to provide persons with limb motor disabilities or even complete quadriplegia an alternative communication channel. However, to the best of our knowledge, GOM-Face is the first interface that exploits all these potentials together. We resolved the interference between GKP and EOG by extracting discriminative features from two covariance matrices: a tongue-movement-only data matrix and eye-movement-only data matrix. With the feature extraction method, GOM-Face can detect four kinds of horizontal tongue or eye movements with an accuracy of 86.7{\%} within 2.77 s. We demonstrated the applicability of the GOM-Face to humanoid robot control: users were able to communicate with the robot by selecting from a predefined menu using the eye and tongue movements.},
author = {Nam, Yunjun and Koo, Bonkon and Cichocki, Andrzej and Choi, Seungjin},
doi = {10.1109/TBME.2013.2280900},
issn = {00189294},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {Electromyogram (EMG),electrooculogram (EOG),glossokinetic potentials (GKP),human-machine interface,multimodal interface},
number = {2},
pages = {453--462},
pmid = {24021635},
title = {{GOM-face: GKP, EOG, and EMG-based multimodal interface with application to humanoid robot control}},
volume = {61},
year = {2014}
}
@article{Sridhar2019,
abstract = {Automated movement tracking is essential for high-throughput quantitative analyses of the behaviour and kinematics of organisms. Automated tracking also improves replicability by avoiding observer biases and allowing reproducible workflows. However, few automated tracking programs exist that are open access, open source, and capable of tracking unmarked organisms in noisy environments. Tracktor is an image-based tracking freeware designed to perform single-object tracking in noisy environments, or multi-object tracking in uniform environments while maintaining individual identities. Tracktor is code-based but requires no coding skills other than the user being able to specify tracking parameters in a designated location, much like in a graphical user interface (GUI). The installation and use of the software is fully detailed in a user manual. Through four examples of common tracking problems, we show that Tracktor is able to track a variety of animals in diverse conditions. The main strengths of Tracktor lie in its ability to track single individuals under noisy conditions (e.g. when the object shape is distorted), its robustness to perturbations (e.g. changes in lighting conditions during the experiment), and its capacity to track multiple individuals while maintaining their identities. Additionally, summary statistics and plots allow measuring and visualizing common metrics used in the analysis of animal movement (e.g. cumulative distance, speed, acceleration, activity, time spent in specific areas, distance to neighbour, etc.). Tracktor is a versatile, reliable, easy-to-use automated tracking software that is compatible with all operating systems and provides many features not available in other existing freeware. Access Tracktor and the complete user manual here: https://github.com/vivekhsridhar/tracktor},
author = {Sridhar, Vivek Hari and Roche, Dominique G and Gingins, Simon},
doi = {10.1111/2041-210X.13166},
file = {::},
issn = {2041210X},
journal = {Methods in Ecology and Evolution},
keywords = {choice experiment,collective behaviour,fast-start escape response,kinematics,locomotion,video analysis software},
number = {6},
pages = {815--820},
title = {{Tracktor: Image-based automated tracking of animal movement and behaviour}},
url = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13166},
volume = {10},
year = {2019}
}
@article{Bazrafkan2017,
abstract = {With the increasing imaging and processing capabilities of today's mobile devices, user authentication using iris biometrics has become feasible. However, as the acquisition conditions become more unconstrained and as image quality is typically lower than dedicated iris acquisition systems, the accurate segmentation of iris regions is crucial for these devices. In this work, an end to end Fully Convolutional Deep Neural Network (FCDNN) design is proposed to perform the iris segmentation task for lower-quality iris images. The network design process is explained in detail, and the resulting network is trained and tuned using several large public iris datasets. A set of methods to generate and augment suitable lower quality iris images from the high-quality public databases are provided. The network is trained on Near InfraRed (NIR) images initially and later tuned on additional datasets derived from visible images. Comprehensive inter-database comparisons are provided together with results from a selection of experiments detailing the effects of different tunings of the network. Finally, the proposed model is compared with SegNet-basic, and a near-optimal tuning of the network is compared to a selection of other state-of-art iris segmentation algorithms. The results show very promising performance from the optimized Deep Neural Networks design when compared with state-of-art techniques applied to the same lower quality datasets.},
archivePrefix = {arXiv},
arxivId = {1712.02877},
author = {Bazrafkan, Shabab and Thavalengal, Shejin and Corcoran, Peter},
eprint = {1712.02877},
month = {dec},
title = {{An End to End Deep Neural Network for Iris Segmentation in Unconstraint Scenarios}},
url = {http://arxiv.org/abs/1712.02877},
year = {2017}
}
@article{Kuehlkamp2016,
abstract = {Iris recognition systems are a mature technology that is widely used throughout the world. In identification (as opposed to verification) mode, an iris to be recognized is typically matched against all N enrolled irises. This is the classic "1-to-N search". In order to improve the speed of large-scale identification, a modified "1-to-First" search has been used in some operational systems. A 1-to-First search terminates with the first below-threshold match that is found, whereas a 1-to-N search always finds the best match across all enrollments. We know of no previous studies that evaluate how the accuracy of 1-to-First search differs from that of 1-to-N search. Using a dataset of over 50,000 iris images from 2,800 different irises, we perform experiments to evaluate the relative accuracy of 1-to-First and 1-to-N search. We evaluate how the accuracy difference changes with larger numbers of enrolled irises, and with larger ranges of rotational difference allowed between iris images. We find that False Match error rate for 1-to-First is higher than for 1-to-N, and the the difference grows with larger number of enrolled irises and with larger range of rotation.},
annote = {The way that iris recognition works is that some kind of filter is applied to localize the iris. 2D Gador filter is often cited. Then that iris is checked against the whole database. This is called 1:N. They check the Hamming Distance between of the bits of the data and then choose the lowest ones as a pair. In 1:First search they do the same except there is is a threshold that that is has to be under to be accepted. If it is under that threshold it is accepted and the search is stopped. Two types of error can occour: a False Match (FM) and False Non-Match (FNM). A false match occurs when two samples from different individuals are declared by the system as a match. A false non-match is when two samples from the same individual fail to be considered as a match by the system},
archivePrefix = {arXiv},
arxivId = {1702.01167},
author = {Kuehlkamp, Andrey and Bowyer, Kevin W},
doi = {10.1109/WACV.2016.7477687},
eprint = {1702.01167},
isbn = {9781509006410},
journal = {2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016},
title = {{An analysis of 1-to-first matching in iris recognition}},
year = {2016}
}
@article{Redmon2016,
abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
archivePrefix = {arXiv},
arxivId = {1612.08242},
author = {Redmon, Joseph and Farhadi, Ali},
eprint = {1612.08242},
file = {::},
month = {dec},
title = {{YOLO9000: Better, Faster, Stronger}},
url = {http://arxiv.org/abs/1612.08242},
year = {2016}
}
@article{Cheng2017a,
abstract = {This paper focuses on indoor semantic segmentation us-ing RGB-D data. Although the commonly used deconvolu-tion networks (DeconvNet) have achieved impressive results on this task, we find there is still room for improvements in two aspects. One is about the boundary segmentation. DeconvNet aggregates large context to predict the label of each pixel, inherently limiting the segmentation precision of object boundaries. The other is about RGB-D fusion. Re-cent state-of-the-art methods generally fuse RGB and depth networks with equal-weight score fusion, regardless of the varying contributions of the two modalities on delineating different categories in different scenes. To address the two problems, we first propose a locality-sensitive DeconvNet (LS-DeconvNet) to refine the boundary segmentation over each modality. LS-DeconvNet incorporates locally visual and geometric cues from the raw RGB-D data into each DeconvNet, which is able to learn to upsample the coarse convolutional maps with large context whilst recovering sharp object boundaries. Towards RGB-D fusion, we introduce a gated fusion layer to effectively combine the two LS-DeconvNets. This layer can learn to adjust the contributions of RGB and depth over each pixel for high-performance object recognition. Experiments on the large-scale SUN RGB-D dataset and the popular NYU-Depth v2 dataset show that our approach achieves new state-of-the-art results for RGB-D indoor semantic segmentation.},
author = {Cheng, Yanhua and Cai, Rui and Li, Zhiwei and Zhao, Xin and Huang, Kaiqi},
doi = {10.1109/CVPR.2017.161},
isbn = {978-1-5386-0457-1},
journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {1475--1483},
title = {{Locality-Sensitive Deconvolution Networks with Gated Fusion for RGB-D Indoor Semantic Segmentation}},
url = {http://ieeexplore.ieee.org/document/8099644/},
year = {2017}
}
@article{DeMarsico2018,
abstract = {Mobile biometrics technologies are nowadays the new frontier for secure use of data and services, and are considered particularly important due to the massive use of handheld devices in the entire world. Among the biometric traits with potential to be used in mobile settings, the iris/ocular region is a natural candidate, even considering that further advances in the technology are required to meet the operational requirements of such ambitious environments. Aiming at promoting these advances, we organized the Mobile Iris Challenge Evaluation (MICHE)-I contest. This paper presents a comparison of the performance of the participant methods by various Figures of Merit (FoMs). A particular attention is devoted to the identification of the image covariates that are likely to cause a decrease in the performance levels of the compared algorithms. Among these factors, interoperability among different devices plays an important role. The methods (or parts of them) implemented by the analyzed approaches are classified into segmentation (S), which was the main target of MICHE-I, and recognition (R). The paper reports both the results observed for either S or R, and also for different recombinations (S+R) of such methods. Last but not least, we also present the results obtained by multi-classifier strategies.},
author = {{De Marsico}, Maria and Nappi, Michele and Narducci, Fabio and Proen{\c{c}}a, Hugo},
doi = {10.1016/j.patcog.2017.08.028},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Biometric algorithm fusion,Evaluation,Mobile Iris Recognition},
pages = {286--304},
publisher = {Elsevier Ltd},
title = {{Insights into the results of MICHE I - Mobile Iris CHallenge Evaluation}},
volume = {74},
year = {2018}
}
@article{Li2017,
abstract = {This paper investigates how to integrate the complementary information from RGB and thermal (RGB-T) sources for object tracking. We propose a novel Convolutional Neural Network (ConvNet) architecture, including a two-stream ConvNet and a FusionNet, to achieve adaptive fusion of different source data for robust RGB-T tracking. Both RGB and thermal streams extract generic semantic information of the target object. In particular, the thermal stream is pre-trained on the ImageNet dataset to encode rich semantic information, and then fine-tuned using thermal images to capture the specific properties of thermal information. For adaptive fusion of different modalities while avoiding redundant noises, the FusionNet is employed to select most discriminative feature maps from the outputs of the two-stream ConvNet, and updated online to adapt to appearance variations of the target object. Finally, the object locations are efficiently predicted by applying the multi-channel correlation filter on the fused feature maps. Extensive experiments on the recently public benchmark GTOT verify the effectiveness of the proposed approach against other state-of-the-art RGB-T trackers.},
author = {Li, Chenglong and Wu, Xiaohao and Zhao, Nan and Cao, Xiaochun and Tang, Jin},
doi = {10.1016/j.neucom.2017.11.068},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Adaptive fusion,Convolutional neural network,Correlation filter,Object tracking,Thermal information},
pages = {78--85},
publisher = {Elsevier B.V.},
title = {{Fusing two-stream convolutional neural networks for RGB-T object tracking}},
url = {https://doi.org/10.1016/j.neucom.2017.11.068},
volume = {281},
year = {2017}
}
@article{Biosec2007,
abstract = {The baseline corpus of a new multimodal database, acquired in the framework of the FP6 EU BioSec Integrated Project, is presented. The corpus consists of fingerprint images acquired with three different sensors, frontal face images from a webcam, iris images from an iris sensor, and voice utterances acquired both with a close-talk headset and a distant webcam microphone. The BioSec baseline corpus includes real multimodal data from 200 individuals in two acquisition sessions. In this contribution, the acquisition setup and protocol are outlined, and the contents of the corpus-including data and population statistics-are described. The database will be publicly available for research purposes by mid-2006. {\textcopyright} 2006 Pattern Recognition Society.},
author = {Fierrez, Julian and Ortega-Garcia, Javier and {Torre Toledano}, Doroteo and Gonzalez-Rodriguez, Joaquin},
doi = {10.1016/j.patcog.2006.10.014},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Authentication,Biometrics,Database,Face,Fingerprint,Iris,Multimodal,Performance,Verification,Voice},
number = {4},
pages = {1389--1392},
title = {{Biosec baseline corpus: A multimodal biometric database}},
volume = {40},
year = {2007}
}
@misc{Perez2018,
author = {Perez, Sarah},
booktitle = {Tech Crunch},
pages = {0--1},
title = {{Walmart pilots a grocery-picking robot to fulfill customers' online orders}},
url = {https://techcrunch.com/2018/08/03/walmart-pilots-a-grocery-picking-robot-to-fulfill-customers-online-orders/?guccounter=1},
urldate = {2019-01-04},
year = {2018}
}
@article{Xiang2017,
abstract = {Estimating the 6D pose of known objects is important for robots to interact with the real world. The problem is challenging due to the variety of objects as well as the complexity of a scene caused by clutter and occlusions between objects. In this work, we introduce PoseCNN, a new Convolutional Neural Network for 6D object pose estimation. PoseCNN estimates the 3D translation of an object by localizing its center in the image and predicting its distance from the camera. The 3D rotation of the object is estimated by regressing to a quaternion representation. We also introduce a novel loss function that enables PoseCNN to handle symmetric objects. In addition, we contribute a large scale video dataset for 6D object pose estimation named the YCB-Video dataset. Our dataset provides accurate 6D poses of 21 objects from the YCB dataset observed in 92 videos with 133,827 frames. We conduct extensive experiments on our YCB-Video dataset and the OccludedLINEMOD dataset to show that PoseCNN is highly robust to occlusions, can handle symmetric objects, and provide accurate pose estimation using only color images as input. When using depth data to further refine the poses, our approach achieves state-of-the-art results on the challenging OccludedLINEMOD dataset.},
archivePrefix = {arXiv},
arxivId = {1711.00199},
author = {Xiang, Yu and Schmidt, Tanner and Narayanan, Venkatraman and Fox, Dieter},
eprint = {1711.00199},
file = {::},
month = {nov},
title = {{PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes}},
url = {https://arxiv.org/abs/1711.00199},
year = {2017}
}
@misc{Weisstein,
author = {Weisstein, Eric W.},
title = {{Green's Theorem}},
url = {http://mathworld.wolfram.com/GreensTheorem.html},
urldate = {2019-06-03}
}
@article{Kim2016,
abstract = {The iris recognition on a mobile phone is different from the conventional iris recognition implemented on a dedicated device in that the computational power of a mobile phone and the space for placing NIR (near infrared) LED (light emitting diode) illuminators and iris camera are limited. This paper raises these issues in detail based on real implementation of an iris recognition system in a mobile phone and proposes some solutions to these issues. An experimental study was conducted to search for the relevant power and wavelength of NIR LED illuminators with their positioning on a phone for capturing a good quality iris image. Subsequently, in view of the disparity between the user's gazing point and the center of the iris camera which causes degradation of acquired iris images, an experiment was performed to locate the appropriate gazing point for good iris image capture. A fast eye detection algorithm was proposed for implementation under the mobile platform with low computational facility. The experiments were conducted on a currently released mobile phone and the results showed promising potential for adoption of iris recognition as a reliable authentication means. As a result, two 850 nm LEDs were selected for iris illumination at 1.1 cm away from the iris camera for the size of a 7 cm × 13.7 cm phone. In the performance, the recognition accuracy was 0.1{\%} EER (equal error rate) and the eye detection rate with the speed of 17.64 ms on a mobile phone was 99.4{\%}.},
annote = {Keynotes:
Contributes with a good NIR mobile algorithm that's fast on mobile phones. They also contribute with an mobile iris database of 500 images that they are willing to share for research. Could not find it online.},
author = {Kim, Dongik and Jung, Yujin and Toh, Kar Ann and Son, Byungjun and Kim, Jaihie},
doi = {10.1016/j.eswa.2016.01.050},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Handheld,Iris recognition,Mobile,Portable,Smartphone},
pages = {328--339},
publisher = {Elsevier Ltd},
title = {{An empirical study on iris recognition in a mobile phone}},
url = {http://dx.doi.org/10.1016/j.eswa.2016.01.050},
volume = {54},
year = {2016}
}
@article{Rodriguez2018a,
abstract = {1. Behavioral analysis based on video recording is becoming increasingly popular within research fields such as; ecology, medicine, ecotoxicology, and toxicology. However, the programs available to analyze the data, which are; free of cost, user-friendly, versatile, robust, fast and provide reliable statistics for different organisms (invertebrates, vertebrates and mammals) are significantly limited. 2. We present an automated open-source executable software (ToxTrac) for image-based tracking that can simultaneously handle several organisms monitored in a laboratory environment. We compare the performance of ToxTrac with current accessible programs on the web. 3. The main advantages of ToxTrac are: i) no specific knowledge of the geometry of the tracked bodies is needed; ii) processing speed, ToxTrac can operate at a rate {\textgreater}25 frames per second in HD videos using modern desktop computers; iii) simultaneous tracking of multiple organisms in multiple arenas; iv) integrated distortion correction and camera calibration; v) robust against false positives; vi) preservation of individual identification if crossing occurs; vii) useful statistics and heat maps in real scale are exported in: image, text and excel formats. 4. ToxTrac can be used for high speed tracking of insects, fish, rodents or other species, and provides useful locomotor information. We suggest using ToxTrac for future studies of animal behavior independent of research area. Download ToxTrac here: https://toxtrac.sourceforge.io},
archivePrefix = {arXiv},
arxivId = {1706.02577},
author = {Rodriguez, Alvaro and Zhang, Hanqing and Klaminder, Jonatan and Brodin, Tomas and Andersson, Patrik L. and Andersson, Magnus},
doi = {10.1111/2041-210X.12874},
eprint = {1706.02577},
file = {::},
issn = {2041210X},
journal = {Methods in Ecology and Evolution},
keywords = {Kalman filter,animal behavior,cockroach,ecology,ecotoxicology,guppy,salmon,tadpole,tracking software,zebrafish},
month = {jun},
number = {3},
pages = {460--464},
title = {{ToxTrac: A fast and robust software for tracking organisms}},
url = {http://arxiv.org/abs/1706.02577},
volume = {9},
year = {2018}
}
@article{Schiel2002,
abstract = {In this contribution we announce and describe in detail the new multimodal corpus evolving from the publicly funded German SmartKom project. The first release of the corpus (BAS SK-P 1.0) has been finished end of 2001 and will be ready for distribution to the scientific community in July 2002. The SmartKom corpus will be the first of a new generation of Language Resources (LR) designed for a more or less complete data gathering of human-machine communication combining acoustic, visual and tactile input and output modalities. Since the funding of about EU 2 Mio for this LR is 100{\%} public, the corpus will be available without royalties via the Bavarian Archive for Speech Signals (BAS) at the University of Munich.},
author = {Schiel, Florian and Steininger, Silke and T{\"{u}}rk, Ulrich},
journal = {Proceedings of the 3rd Language Resources and Evaluation},
number = {34},
pages = {200--2006},
title = {{The SmartKom Multimodal Corpus at BAS}},
year = {2002}
}
@article{Chen2005a,
abstract = {The recognition accuracy of a single biometric authentication system is often much reduced due to the environment, user mode and physiological defects. In this paper, we combine face and iris features for developing a multimode biometric approach, which is able to diminish the drawback of single biometric approach as well as to improve the performance of authentication system. We combine a face database ORL and iris database CASIA to construct a multimodal biometric experimental database with which we validate the proposed approach and evaluate the multimodal biometrics performance. The experimental results reveal the multimodal biometrics verification is much more reliable and precise than single biometric approach.},
annote = {Face and iris for authentication 
uses synthetic multimodal biometric dataset

Shows how to evaluate system. 

proves multimodal performs better.},
author = {Chen, Ching-Han and {Te Chu}, Chia},
doi = {10.1007/11608288_76},
isbn = {978-3-540-31621-3},
issn = {03029743},
keywords = {face,iris,multimodal biometrics,wavelet probabilistic neural},
pages = {571--580},
title = {{Fusion of Face and Iris Features for Multimodal Biometrics}},
url = {http://link.springer.com/10.1007/11608288{\_}76},
year = {2005}
}
@article{BiosecID2008,
abstract = {A new multimodal biometric database, acquired in the framework of the BiosecurID project, is presented together with the description of the acquisition setup and protocol. The database includes eight unimodal biometric traits, namely: speech, iris, face (still images, videos of talking faces), handwritten signature and handwritten text (on-line dynamic signals, off-line scanned images), fingerprints (acquired with two different sensors), hand (palmprint, contour-geometry) and keystroking. The database comprises 400 subjects and presents features such as: realistic acquisition scenario, balanced gender and population distributions, availability of information about particular demographic groups (age, gender, handedness), acquisition of replay attacks for speech and keystroking, skilled forgeries for signatures, and compatibility with other existing databases. All these characteristics make it very useful in research and development of unimodal and multimodal biometric systems. {\textcopyright} Springer-Verlag London Limited 2009.},
author = {Fierrez, J. and Galbally, J. and Ortega-Garcia, J. and Freire, M. R. and Alonso-Fernandez, F. and Ramos, D. and Toledano, D. T. and Gonzalez-Rodriguez, J. and Siguenza, J. A. and Garrido-Salas, J. and Anguiano, E. and Gonzalez-de-Rivera, G. and Ribalda, R. and Faundez-Zanuy, M. and Ortega, J. A. and Carde{\~{n}}oso-Payo, V. and Viloria, A. and Vivaracho, C. E. and Moro, Q. I. and Igarza, J. J. and Sanchez, J. and Hernaez, I. and Orrite-Uru{\~{n}}uela, C. and Martinez-Contreras, F. and Gracia-Roche, J. J.},
doi = {10.1007/s10044-009-0151-4},
issn = {14337541},
journal = {Pattern Analysis and Applications},
keywords = {Biometrics,Database,Face,Fingerprint,Hand geometry,Handwriting,Iris,Keystroking,Multimodal,Palmprint,Signature,Speech},
number = {2},
pages = {235--246},
title = {{BiosecurID: A multimodal biometric database}},
volume = {13},
year = {2010}
}
@article{Kupfer1984,
author = {Kupfer, D J and Ulrich, R F and Coble, P A and Jarnatt, D B and Grochocinski, V and Doman, J and Matthews, G and Borbely, A A},
journal = {Psychiatry Res},
keywords = {ANALYSIS,Depression,Human,REM,Sleep},
pages = {335--343},
title = {{Application of automated REM and slow wave analysis: I normal and depressive subjects}},
volume = {13},
year = {1984}
}
@article{Kumar2016a,
abstract = {We studied the fusion of three biometric authentication modalities, namely, swiping gestures, typing patterns and the phone movement patterns observed during typing or swiping. A web browser was customized to collect the data generated from the aforementioned modalities over four to seven days in an unconstrained environment. Several features were extracted by using sliding window mechanism for each modality and analyzed by using information gain, correlation, and symmetric uncertainty. Finally, five features from windows of continuous swipes, thirty features from windows of continuously typed letters, and nine features from corresponding phone movement patterns while swiping/typing were used to build the authentication system. We evaluated the performance of each modality and their fusion over a dataset of 28 users. The feature-level fusion of swiping and the corresponding phone movement patterns achieved an authentication accuracy of 93.33{\%}, whereas, the score-level fusion of typing behaviors and the corresponding phone movement patterns achieved an authentication accuracy of 89.31{\%}. 1.},
author = {Kumar, Rajesh and Phoha, Vir V. and Serwadda, Abdul},
doi = {10.1109/BTAS.2016.7791164},
isbn = {9781467397339},
journal = {2016 IEEE 8th International Conference on Biometrics Theory, Applications and Systems, BTAS 2016},
title = {{Continuous authentication of smartphone users by fusing typing, swiping, and phone movement patterns}},
year = {2016}
}
@inproceedings{Yin2011,
abstract = {In this paper, the acquisition and content of a new homologous multimodal biometric database are presented. The SDUMLA-HMT database consists of face images from 7 view angles, finger vein images of 6 fingers, gait videos from 6 view angles, iris images from an iris sensor, and fingerprint images acquired with 5 different sensors. The database includes real multimodal data from 106 individuals. In addition to database description, we also present possible use of the database. The database is available to research community through http://mla.sdu.edu.cn/sdumla-hmt.html .},
author = {Yin, Yilong and Liu, Lili and Sun, Xiwei},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-25449-9_33},
isbn = {9783642254482},
issn = {03029743},
keywords = {Biometrics,Face,Finger vein,Fingerprint,Gait,Homologous,Iris,Multi-modal},
pages = {260--268},
title = {{SDUMLA-HMT: A multimodal biometric database}},
volume = {7098 LNCS},
year = {2011}
}
@article{LeCun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
isbn = {9780521835688},
issn = {14764687},
journal = {Nature},
keywords = {Computer science,Mathematics and computing},
month = {may},
number = {7553},
pages = {436--444},
pmid = {10463930},
publisher = {Nature Publishing Group},
title = {{Deep learning}},
url = {http://www.nature.com/articles/nature14539},
volume = {521},
year = {2015}
}
@article{Petrovska-Delacretaz2008a,
abstract = {Face recognition finds its place in a large number of applications. They occur in different contexts related to security, entertainment or Internet applications. Reliable face recognition is still a great challenge to computer vision and pattern recognition researchers, and new algorithms need to be evaluated on relevant databases. The publicly available IV2 database allows monomodal and multimodal experiments using face data. Known variabilities, that are critical for the performance of the biometric systems (such as pose, expression, illumination and quality) are present. The face and subface data that are acquired in this database are: 2D audio-video talking-face sequences, 2D stereoscopic data acquired with two pairs of synchronized cameras, 3D facial data acquired with a laser scanner, and iris images acquired with a portable infrared camera. The IV2 database is designed for monomodal and multimodal experiments. The quality of the acquired data is of great importance. Therefore as a first step, and in order to better evaluate the quality of the data, a first internal evaluation was conducted. Only a small amount of the total acquired data was used for this evaluation: 2D still images, 3D scans and iris images. First results show the interest of this database. In parallel to the research algorithms, open-source reference systems were also run for baseline comparisons.},
author = {Petrovska-Delacr{\'{e}}taz, D. and Lelandais, S. and Colineau, J. and Chen, L. and Dorizzi, B. and Ardabilian, M. and Krichen, E. and Mellakh, M. A. and Chaari, A. and Guerfi, S. and D'Hose, J. and Amor, B. Ben},
doi = {10.1109/BTAS.2008.4699323},
isbn = {9781424427307},
journal = {BTAS 2008 - IEEE 2nd International Conference on Biometrics: Theory, Applications and Systems},
pages = {3--9},
title = {{The IV2 multimodal biometric database (including Iris, 2D, 3D, stereoscopic, and talking face data), and the IV2-2007 evaluation campaign}},
volume = {00},
year = {2008}
}
@article{Zhao2017a,
annote = {Good article describing a lot of the state of the art deep learning approaches being researched.

Keynotes:

Not much work with deep learning has been done in iris recognition.

THey Use a Fully Convolutional Network (FCN) and talk about others who ahve used a Convolutional Neural Network (CNN). They also mention a Deep Belief Net (DBN) that others have used. DeepIrisNet is also mentioned and tested with

THey have created their own loss function optimized for iris recognition called Extended Triplet Loss (ETL)

Their network is generalizable to other databases meaning that it doesn't require finetuing as many others do.

Used ND-IRIS, CASIA Iris, IITD Iris and WVU Non-Ideal Iris databases to test on.},
author = {Zhao, Zijing and Kumar, Ajay},
doi = {10.1109/ICCV.2017.411},
isbn = {978-1-5386-1032-9},
journal = {2017 IEEE International Conference on Computer Vision (ICCV)},
pages = {3829--3838},
title = {{Towards More Accurate Iris Recognition Using Deeply Learned Spatially Corresponding Features}},
url = {http://ieeexplore.ieee.org/document/8237673/},
year = {2017}
}
@misc{Vincent2018,
abstract = {How British supermarket Ocado is using robots to make online grocery shopping faster},
author = {Vincent, James},
booktitle = {The verge},
title = {{WELCOME TO THE AUTOMATED WAREHOUSE OF THE FUTURE}},
url = {https://www.theverge.com/2018/5/8/17331250/automated-warehouses-jobs-ocado-andover-amazon},
urldate = {2019-01-04},
year = {2018}
}
@article{Suk2014,
abstract = {For the last decade, it has been shown that neuroimaging can be a potential tool for the diagnosis of Alzheimer's Disease (AD) and its prodromal stage, Mild Cognitive Impairment (MCI), and also fusion of different modalities can further provide the complementary information to enhance diagnostic accuracy. Here, we focus on the problems of both feature representation and fusion of multimodal information from Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET). To our best knowledge, the previous methods in the literature mostly used hand-crafted features such as cortical thickness, gray matter densities from MRI, or voxel intensities from PET, and then combined these multimodal features by simply concatenating into a long vector or transforming into a higher-dimensional kernel space. In this paper, we propose a novel method for a high-level latent and shared feature representation from neuroimaging modalities via deep learning. Specifically, we use Deep Boltzmann Machine (DBM).22Although it is clear from the context that the acronym DBM denotes "Deep Boltzmann Machine" in this paper, we would clearly indicate that DBM here is not related to "Deformation Based Morphometry"., a deep network with a restricted Boltzmann machine as a building block, to find a latent hierarchical feature representation from a 3D patch, and then devise a systematic method for a joint feature representation from the paired patches of MRI and PET with a multimodal DBM. To validate the effectiveness of the proposed method, we performed experiments on ADNI dataset and compared with the state-of-the-art methods. In three binary classification problems of AD vs. healthy Normal Control (NC), MCI vs. NC, and MCI converter vs. MCI non-converter, we obtained the maximal accuracies of 95.35{\%}, 85.67{\%}, and 74.58{\%}, respectively, outperforming the competing methods. By visual inspection of the trained model, we observed that the proposed method could hierarchically discover the complex latent patterns inherent in both MRI and PET.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Suk, Heung Il and Lee, Seong Whan and Shen, Dinggang},
doi = {10.1016/j.neuroimage.2014.06.077},
eprint = {NIHMS150003},
isbn = {1053-8119},
issn = {10959572},
journal = {NeuroImage},
keywords = {Alzheimer's Disease,Deep boltzmann machine,Mild cognitive impairment,Multimodal data fusion,Shared feature representation},
pages = {569--582},
pmid = {25042445},
publisher = {Elsevier Inc.},
title = {{Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2014.06.077},
volume = {101},
year = {2014}
}
@article{Liu2013a,
author = {Liu, Jing and Sun, Zhenan and Tan, Tieniu},
doi = {10.1109/BTAS.2013.6712692},
isbn = {9781479905270},
journal = {IEEE 6th International Conference on Biometrics: Theory, Applications and Systems, BTAS 2013},
pages = {1--6},
title = {{Code-level information fusion of low-resolution iris image sequences for personal identification at a distance}},
year = {2013}
}
@article{Mellakh2009a,
author = {Mellakh, A and Chaari, A and Guerfi, S and Dhose, J and Colineau, J and Lelandais, S and Petrovska-Delacr{\`{e}}taz, D and Dorizzi, B},
doi = {10.1007/978-3-642-04697-1_3},
isbn = {03029743; 3642046967 (ISBN); 9783642046964 (ISBN)},
issn = {03029743},
journal = {11th International Conference on Advanced Concepts for Intelligent Vision Systems, ACIVS 2009},
keywords = {2D face recognition,Appearance based,Appearance-based algorithms,Computer vision,Database,Database systems,Discriminant analysis,Evaluation campaign,Experimental protocols,Face images,Face recognition,Linear discriminant analysis,Multi-modal,Multimodal database,Principal component analysis,Training sets},
pages = {24--32},
title = {{2D face recognition in the IV2 evaluation campaign}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-70549098348{\&}partnerID=40{\&}md5=0541be27bb29a037c965f0d644b15856},
volume = {5807 LNCS},
year = {2009}
}
@article{DelcourtMichaelOvidioMathieuDenoelMarcMullerHelenePendevilleJean-LouisDeneubourgPascalPoncin,
abstract = {In laboratory fish research, the zebrafish Danio rerio (Cyprinidae) represents the equivalent of the mouse in mammalian research. This species has become a major model for studies in developmental and behavioural genetics, neurophysiology, biomedi-cine, ecotoxicology, and behavioural and evolutionary ecology. To meet the need for accurate and reproducible data in both fundamental and applied sciences, it is of primary importance to be able to tag and/or recognize individual zebrafish. However, classic methods used in fish ecology and aquaculture are generally difficult to apply to such small fish. Recently, various new tagging methods have been developed. This paper presents a first review of current identification and marking methods applied to zebrafish, from external observation methods (such as skin pattern recognition, fin clipping, scale regen-eration, colour and transgenic methods) to the most advanced technological developments in electronic (low-and high-radio-frequencies PIT tags, micro-chip) and image analysis methods (video tracking). This review aims to help researchers and zebrafish facility managers select the identification method (ID) best adapted to their needs. The main characteristics of each ID method are examined (including detection range, durability, speed and repetitiveness, ID code combination, size dependence and ethical considerations), and their pros and cons are summarized in a decision table to help select the most appropriate option for a research or management program. Finally, contextual applications of these ID methods and future developments are discussed. Electronic supplementary material The online version of this article (https://doi.org/10.1007/s11160-018-9537-y) contains supplementary material, which is available to authorized users.},
author = {{Delcourt Micha{\"{e}}l Ovidio Mathieu Deno{\"{e}}l Marc Muller H{\'{e}}l{\`{e}}ne Pendeville Jean-Louis Deneubourg Pascal Poncin}, Johann and Delcourt, J and {Deno{\"{e}}l {\'{A}} P Poncin}, {\'{A}} M and {Ovidio {\'{A}} Poncin}, M P and Muller, M and Pendeville, H and Deneubourg, J-l},
doi = {10.1007/s11160-018-9537-y},
file = {::},
isbn = {0123456789},
keywords = {Animal ID,Danio rerio,Passive integrated transponder,Tagging,VIE tag,Video tracking},
title = {{Individual identification and marking techniques for zebrafish}},
url = {https://doi.org/10.1007/s11160-018-9537-y}
}
@article{Kulkarni2018,
abstract = {This paper presents a method for tracking multiple objects from a given video dataset. We can track many objects at a time efficiently using Kalman Filter and Optical flow algorithm. Here in this paper, we propose improved optical flow algorithm which works with high accuracy and overcomes occlusion in a video. So, improved optical flow algorithm is found to be more promising as it gives better accuracy in less computation.},
author = {Kulkarni, Anita and Rani, Elizabeth},
doi = {10.24247/ijecierdapr20181},
issn = {2249-684X},
journal = {International Journal of Electronics, Communication {\&} Instrumentation Engineering Research and Development},
keywords = {Kalman Filter,Less Computation,Multiple Objects},
number = {2},
pages = {1--6},
title = {{KALMAN Filter Based Multiple Object Tracking System}},
url = {http://tjprc.org/publishpapers/2-16-1519626065-1.IJECIERDAPR20181.pdf},
volume = {8},
year = {2018}
}
@article{Zhang1984,
author = {Zhang, T. Y. and Suen, C. Y.},
doi = {10.1145/357994.358023},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {parallel algorithm,skeletonization,thinning of digital patterns},
month = {mar},
number = {3},
pages = {236--239},
publisher = {ACM},
title = {{A fast parallel algorithm for thinning digital patterns}},
url = {http://portal.acm.org/citation.cfm?doid=357994.358023},
volume = {27},
year = {1984}
}
@article{Nielsen2015,
abstract = {Neural Networks and Deep Learning is a free online book. The book will teach you about: Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data Deep learning, a powerful set of techniques for learning in neural networks Neural networks and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing. This book will teach you many of the core concepts behind neural networks and deep learning.},
author = {Nielsen, Michael},
journal = {Determination Press},
title = {{Neural Networks and Deep Learning}},
url = {http://neuralnetworksanddeeplearning.com/index.html},
year = {2015}
}
@inproceedings{Kaur2016a,
abstract = {Abstract: Face recognition is a type of biometric software application by using which, we can analyzing, identifying or verifying digital image of the person by using the feature of the face of the person that are unique characteristics of each person. These characteristics may be physical or behavioral. The physiological characteristics as like finger print, iris scan, or face etc and behavior characteristics as like hand-writing, voice, key stroke etc. Face recognition is very useful in many areas such as military, airports, universities, ATM, and banks etc, used for the security purposes. There are many techniques or algorithms that are used features extraction in face recognition. This paper make a review of some of those methods which are used for the face recognition that are Principal Component Analysis (PCA), Back Propagation Neural Networks (BPNN), Genetic Algorithm, and LDA, SVM, Independent Component Analysis(ICA). Each method has different -2 functions that are used for the face recognition. Dimensionality is reduced by using the Eigen face approach or PCA, LDA to extract the features from images. Genetic Algorithm is based on feature selection and Back propagation Neural Network (BPNN) is used for the classification of face images.},
author = {Kaur, Gurpreet and Kanwal, Navdeep},
booktitle = {International Conference on Computing for Sustainable Global Development (INDIACom)},
isbn = {9789380544212},
pages = {2705--2710},
publisher = {IEEE},
title = {{A Comparative Review of Various Approaches for Feature Extraction in Face Recognition}},
year = {2016}
}
@article{Kalueff2013,
abstract = {Zebrafish (Danio rerio) are rapidly gaining popularity in translational neuroscience and behavioral research. Physiological similarity to mammals, ease of genetic manipulations, sensitivity to pharmacological and genetic factors, robust behavior, low cost, and potential for high-throughput screening contribute to the growing utility of zebrafish models in this field. Understanding zebrafish behavioral phenotypes provides important insights into neural pathways, physiological biomarkers, and genetic underpinnings of normal and pathological brain function. Novel zebrafish paradigms continue to appear with an encouraging pace, thus necessitating a consistent terminology and improved understanding of the behavioral repertoire. What can zebrafish 'do', and how does their altered brain function translate into behavioral actions? To help address these questions, we have developed a detailed catalog of zebrafish behaviors (Zebrafish Behavior Catalog, ZBC) that covers both larval and adult models. Representing a beginning of creating a more comprehensive ethogram of zebrafish behavior, this effort will improve interpretation of published findings, foster cross-species behavioral modeling, and encourage new groups to apply zebrafish neurobehavioral paradigms in their research. In addition, this glossary creates a framework for developing a zebrafish neurobehavioral ontology, ultimately to become part of a unified animal neurobehavioral ontology, which collectively will contribute to better integration of biological data within and across species.},
author = {Kalueff, Allan V and Gebhardt, Michael and Stewart, Adam Michael and Cachat, Jonathan M and Brimmer, Mallorie and Chawla, Jonathan S and Craddock, Cassandra and Kyzar, Evan J and Roth, Andrew and Landsman, Samuel and Gaikwad, Siddharth and Robinson, Kyle and Baatrup, Erik and Tierney, Keith and Shamchuk, Angela and Norton, William and Miller, Noam and Nicolson, Teresa and Braubach, Oliver and Gilman, Charles P and Pittman, Julian and Rosemberg, Denis B and Gerlai, Robert and Echevarria, David and Lamb, Elisabeth and Neuhauss, Stephan C.F. and Weng, Wei and Bally-Cuif, Laure and {Schneider, and the Zebrafish Neuros}, Henning},
doi = {10.1089/zeb.2012.0861},
file = {:home/niclas/Documents/uni/vgis10/vgis10/Report/bib/VGIS10.bib:bib},
issn = {1545-8547},
journal = {Zebrafish},
number = {1},
pages = {70--86},
title = {{Towards a Comprehensive Catalog of Zebrafish Behavior 1.0 and Beyond}},
url = {www.liebertpub.com http://www.liebertpub.com/doi/10.1089/zeb.2012.0861},
volume = {10},
year = {2013}
}
@article{Dhillon2009,
abstract = {This paper discusses a brain-computer interface through electrooculogram (EOG) and electromyogram (EMG) signals. In situations of disease or trauma, there may be inability to communicate with others through means such as speech or typing. Eye movement tends to be one of the last remaining active muscle capabilities for people with neurodegenerative disorders, such as amyotrophic lateral sclerosis (ALS) also known as Lou Gehrig's disease. Thus, there is a need for eye movement based systems to enable communication. To meet this need, we proposed a system to accept eye-gaze controlled navigation of a particular letter and EMG based click to enter the letter. Eye -gaze direction (angle) is obtained from EOG signals and EMG signal is recorded from eyebrow muscle activity. A virtual screen keyboard may be used to examine the usability of the proposed system.},
author = {Dhillon, Hari Singh and Singla, Rajesh and Rekhi, Navleen Singh and Jha, Rameshwar},
doi = {10.1109/ICCSIT.2009.5234951},
isbn = {9781424445196},
journal = {Proceedings - 2009 2nd IEEE International Conference on Computer Science and Information Technology, ICCSIT 2009},
keywords = {Amyotrophic lateral sclerosis (ALS),EMG,EOG,Eye-gaze,Virtual keyboard},
pages = {259--262},
title = {{EOG and EMG based virtual keyboard: A brain-computer interface}},
year = {2009}
}
@article{Galdi2017a,
author = {Galdi, Chiara and Dugelay, Jean Luc},
doi = {10.1109/ICPR.2016.7899626},
isbn = {9781509048472},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
pages = {160--164},
title = {{Fusing iris colour and texture information for fast iris recognition on mobile devices}},
year = {2017}
}
@book{Wechsler2007,
abstract = {One of the grand challenges for computational intelligence and biometrics is to understand how people process and recognize faces and to develop automated and reliable face recognition systems. Biometrics has become the major component in the complex decision making process associated with security applications. The face detection and authentication challenges addressed include cluttered environments, image variability, occlusion and disguise, and temporal changes all within open set recognition. Reliable Face Recognition Methods: System Design, Implementation and Evaluation comprehensively explores the face recognition problem while drawing inspiration from complementary disciplines such as neurosciences, statistics, signal and image processing, computer vision, and machine learning and pattern recognition. This book also examines the evolution of face recognition research and explores promising new avenues for research and development. Reliable Face Recognition Methods: System Design, Implementation and Evaluation benefits graduate-level students, researchers, and practitioners, as well as government and industry decision makers in the security arena. Ruud Bolle (IBM): "Harry Wechsler's monograph provides a thorough, up-to-date, and in-depth overview of the many advances in the area of face recognition. gives an excellent overview of the issues related to security and privacy when it comes to automated biometrics. In summary, this book has the potential to become a classic. Harry Wechsler is to be commended for undertaking the monumental task of writing this book." John Daugman (Cambridge University, UK): "The book looks excellent. The topic is timely, and the perspective-multi-disciplinary, measured, and objective - is much needed and welcome. I believe that Wechsler's latest book will make a valued contribution to this important field and will become a standard." David Zhang (Hong Kong Polytechnic University, China): "From a system view, this book shows an excellent arrangement, from data collection to face recognition, as well as system performance evaluation and error analysis. This book can serve both as an interdisciplinary text and a research reference. Each chapter provides the background and impetus for understanding the problems discussed." Stan Li (Chinese Academy of Sciences, China): "The book treats the subject of face recognition in a fairly systematic way. The book covers most relevant topics in face recognition, in a well organized way. The information is also useful for experts. The manuscript is easy to read." Tom Huang (University of Illinois, USA): "Harry Wechsler is without doubt one of the leading authorities in face recognition and related topics. We have recently seen a number of excellent edited books on Face Recognition. However, Wechsler's book is the first unified treatment of this important subject. In my opinion, Wechsler is one of only a handful of people in the world who could write such a comprehensive, unified, informative, perceptive, and authoritative book on Face Recognition. Harry Wechsler so far, is the only one of this handful who took the time and effort to realize such a project. The book certainly will have a great positive impact on the biometrics research community. But also, because it looks at Face Recognition from different perspectives, it will be welcomed by non-experts." {\textcopyright}2007 Springer Science+Business Media, LLC. All rights reserved.},
address = {Boston, MA},
author = {Wechsler, Harry},
booktitle = {Reliable Face Recognition Methods: System Design, Impementation and Evaluation},
doi = {10.1007/978-0-387-38464-1},
isbn = {038722372X},
pages = {1--329},
publisher = {Springer US},
title = {{Reliable face recognition methods: System design, implementation and evaluation}},
url = {http://link.springer.com/10.1007/978-0-387-38464-1 https://link-springer-com.zorac.aub.aau.dk/book/10.1007{\%}2F978-0-387-38464-1{\#}about},
year = {2007}
}
@article{Fierrez2018c,
abstract = {The present paper is Part 2 in this series of two papers. In Part 1 we provided an introduction to Multiple Classifier Systems (MCS) with a focus into the fundamentals: basic nomenclature, key elements, architecture, main methods, and prevalent theory and framework. Part 1 then overviewed the application of MCS to the particular field of multimodal biometric person authentication in the last 25 years, as a prototypical area in which MCS has resulted in important achievements. Here in Part 2 we present in more technical detail recent trends and developments in MCS coming from multimodal biometrics that incorporate context information in an adaptive way. These new MCS architectures exploit input quality measures and pattern-specific particularities that move apart from general population statistics, resulting in robust multimodal biometric systems. Similarly as in Part 1, methods here are described in a general way so they can be applied to other information fusion problems as well. Finally, we also discuss here open challenges in biometrics in which MCS can play a key role.},
author = {Fierrez, Julian and Morales, Aythami and Vera-Rodriguez, Ruben and Camacho, David},
doi = {10.1016/j.inffus.2017.12.005},
issn = {15662535},
journal = {Information Fusion},
keywords = {Adaptive,Biometrics,Classifier,Context,Fusion,Multimodal},
number = {November 2017},
pages = {103--112},
publisher = {Elsevier},
title = {{Multiple classifiers in biometrics. Part 2: Trends and challenges}},
url = {https://doi.org/10.1016/j.inffus.2017.12.005},
volume = {44},
year = {2018}
}
@article{Kauba2016,
abstract = {—Authentication based on vein patterns is a very promising biometric technique. The most important step is the accurate extraction of the vein pattern from sometimes low quality input images. A single feature extraction technique may fail to correctly extract the vein pattern, entailing bad recognition performance. One of the solutions that can be used to improve recognition results is biometric fusion. A possible fusion strategy is feature level fusion, that is the fusion of several feature extractors' outputs. In our work, we exploited the feature level fusion to improve the quality of the extracted vein patterns and thus the feature extraction accuracy. An experimental study involving different feature extraction techniques (maximum curvature, repeated line tracking, wide line detector, ...) and different fusion techniques (majority voting, weighted average, STAPLE, ...) is conducted on the UTFVP finger-vein data set. The results show that feature level fusion is able to improve the recognition accuracy in terms of the EER over the single feature extraction techniques.},
author = {Kauba, Christof and Uhl, Andreas and Piciucco, Emanuela and Maiorana, Emanuele and Campisi, Patrizio},
doi = {10.1109/BIOSIG.2016.7736908},
isbn = {9783885796541},
issn = {16175468},
journal = {Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)},
title = {{Advanced variants of feature level fusion for finger vein recognition}},
volume = {P-260},
year = {2016}
}
@inproceedings{Trokielewicz2016,
author = {Trokielewicz, Mateusz},
booktitle = {2016 IEEE International Conference on Identity, Security and Behavior Analysis (ISBA)},
doi = {10.1109/ISBA.2016.7477233},
isbn = {978-1-4673-9727-8},
month = {feb},
pages = {1--6},
publisher = {IEEE},
title = {{Iris recognition with a database of iris images obtained in visible light using smartphone camera}},
url = {http://ieeexplore.ieee.org/document/7477233/},
year = {2016}
}
@article{Pathak2016a,
author = {Pathak, Mrunal and Srinivasu, N},
doi = {10.1007/978-981-10-2630-0},
isbn = {978-981-10-2629-4},
keywords = {biometrics},
pages = {137--152},
title = {{Advances in Computing Applications}},
url = {http://link.springer.com/10.1007/978-981-10-2630-0},
year = {2016}
}
@article{Wang2017,
abstract = {{\textcopyright} 2016, Springer Science+Business Media New York. Tracking individuals in a fish school with video cameras is one of the most effective ways to quantitatively investigate their behavior which is of great value for biological research. However, tracking large numbers of fish with complex non-rigid deformation, similar appearance and frequent mutual occlusions is a challenge task. In this paper we propose an effective tracking method that can reliably track a large number of fish throughout the entire duration. The first step of the proposed method is to detect fish heads using a scale-space method. Data association across frames is achieved via identifying the head image pattern of each individual fish in each frame, which is accomplished by a convolutional neural network (CNN) specially tailored to suit this task. Then the prediction of the motion state and the recognition result by CNN are combined to associate detections across frames. The proposed method was tested on 5 video clips having different number of fish respectively. Experiment results show that the correctness of their identities is not affected by frequent occlusions. The proposed method outperforms two state-of-the-art fish tracking methods in terms of 7 performance metrics.},
author = {Wang, Shuo Hong and Zhao, Jing Wen and Chen, Yan Qiu},
doi = {10.1007/s11042-016-4045-3},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {Convolutional neural network,Fish school,Multi-object tracking,Tracking by identification},
month = {nov},
number = {22},
pages = {23679--23697},
publisher = {Springer US},
title = {{Robust tracking of fish schools using CNN for head identification}},
url = {http://link.springer.com/10.1007/s11042-016-4045-3},
volume = {76},
year = {2017}
}
@article{Neves2017,
abstract = {{\textcopyright}2017 IEEE. An error-correcting code (ECC) is a process of adding redundant data to a message, such that it can be recovered by a receiver even if a number of errors are introduced in transmission. Inspired by the principles of ECC, we introduce a method capable of detecting degraded features in biometric signatures by exploiting feature correlation. The main novelty is that, unlike existing biometric cryptosystems, the proposed method works directly on the biometric signature. Our approach performs a redundancy analysis of non-degraded data to build an undirected graphical model (Markov Random Field), whose energy minimization determines the sequence of degraded components of the biometric sample. Experiments carried out in different biometric traits ascertain the improvements attained when disregarding degraded features during the matching phase. Also, we stress that the proposed method is general enough to work in different classification methods, such as CNNs.},
author = {Neves, Joao and Proenca, Hugo},
doi = {10.1109/FG.2017.122},
isbn = {9781509040230},
journal = {Proceedings - 12th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2017 - 1st International Workshop on Adaptive Shot Learning for Gesture Understanding and Production, ASL4GUP 2017, Biometrics in the Wild, Bwild 2017, Heteroge},
pages = {981--986},
title = {{Exploiting Data Redundancy for Error Detection in Degraded Biometric Signatures Resulting from in the Wild Environments}},
year = {2017}
}
@article{Duguleana2010,
abstract = {This paper proposes a new approach to real-time robot controlling by integrating an Electrooculography (EOG) measuring device within human-robot interaction (HRI). Our study focuses on controlling robots using EOG for fulfilling elementary robot activities such as basic motor movements and environment interaction. A new EOG-based HRI paradigm has been developed on the specific defined problem of eye blinking. The resulted model is tested using biometric capturing equipment. We propose a simple algorithm for real-time identification and processing of signals produced by eyes during blinking phases. We present the experimental setup and the results of the experiment. We conclude by listing further research issues.},
author = {Duguleana, Mihai and Mogan, Gheorghe},
doi = {10.1007/978-3-642-11628-5_37},
isbn = {9783642116278},
issn = {18684238},
journal = {IFIP Advances in Information and Communication Technology},
keywords = {Control,Electrooculography,Eye blink,Human-robot interaction,Robot},
number = {29},
pages = {343--350},
title = {{Using eye blinking for EOG-based robot control}},
volume = {314},
year = {2010}
}
@incollection{Miller2012,
abstract = {Zebrafish spend the majority of their time in groups, called shoals. Shoaling behavior is complex and dynamic: fish leave and rejoin the shoal, distances between shoal-members oscillate, and the speed and polarization of the shoal change on timescales of seconds to minutes. All these features of shoals can be modified by various pharmacological and environmental—and possibly also genetic—manipulations and a thorough characterization of shoaling behavior can therefore be used as an effective assay for complex aspects of vertebrate social behavior. We present methods for acquiring and analyzing detailed trajectory data from shoals of zebrafish and demonstrate how these methods can be used to distinguish episodes of shoaling under different conditions. These methods could be further developed to create a standardized assay of shoaling behavior that will allow for an in-depth exploration of social behaviors in zebrafish.},
author = {Miller, Noam and Gerlai, Robert},
doi = {10.1007/978-1-61779-597-8_16},
file = {::},
pages = {217--230},
publisher = {Humana Press, Totowa, NJ},
title = {{Automated Tracking of Zebrafish Shoals and the Analysis of Shoaling Behavior}},
url = {http://link.springer.com/10.1007/978-1-61779-597-8{\_}16},
year = {2012}
}
@article{Abbas2017,
abstract = {Modern biometrics delivers an enhanced level of security by means of a “proof of property”. The design and deployment of a biometric system, however, hide many pitfalls, which, when underestimated, can lead to major security weaknesses and privacy threats. Issues of concern include biometric identity theft and privacy invasion because of the strong connection between a user and his identity. This book showcases a collection of comprehensive references on the advances of biometric security technology. It compiles a total of fourteen articles, all contributed by thirty-two eminent researchers in the field, thus providing concise and accessible coverage of not only general issues, but also state-of-the-art solutions. The book is divided into five parts: (1) Biometric Template Protection, which covers cancellable biometrics and parameter management protocol;(2) Biometric Key and Encryption, focusing on biometric key generation and visual biometric cryptography;(3) Biometric Systems Analysis, dealing with biometric system security, and privacy evaluation and assessment;(4) Privacy-Enhanced Biometric Systems, covering privacy-enhanced biometric system protocol design and implementation; and(5) Other Biometric Security Technologies.The book will be of particular interest to researchers, scholars, graduate students, engineers, practitioners and developers interested in security and privacy-related issues in biometric systems. It will also be attractive to managers of various organizations with strong security needs.},
author = {Abbas, Sherif N},
doi = {10.1007/978-3-319-47301-7},
isbn = {978-3-319-47300-0},
title = {{Biometric Security and Privacy}},
url = {http://link.springer.com/10.1007/978-3-319-47301-7},
year = {2017}
}
@misc{Olsen2018,
abstract = {H{\o}je danske l{\o}nninger, global konkurrence og stigende krav fra forbrugerne tvinger danske netbutikker til hele tiden at optimere arbejdsgangene, hvis de ikke vil g{\aa} bagud af dansen. Flere ser det robotstyrede lager som en l{\o}sning, viser en analyse fra interesseorganisationen Foreningen for Dansk Internethandel. Proshop inviterede robotterne indenfor i begyndelsen af maj.},
address = {Horsens},
author = {Olsen, Line},
booktitle = {Horsens Folkeblad},
month = {nov},
title = {{Fuldautomatisk robotlager: Nu tager det syv minutter at pakke en pakke}},
url = {https://hsfo.dk/erhverv/Fuldautomatisk-robotlager-Nu-tager-det-syv-minutter-at-pakke-en-pakke/artikel/199254},
year = {2018}
}
@article{Khan2017,
annote = {A good article that uses a new database containing smartphone iris images. Theses are taken in visible light. They use Daugmans approach to localize the iris, then they normalize it. They use wavelets on the image to extract the desired featues and then try to classify them using SVM (97{\%}), KNN (95.1{\%}) and LDA (94.28{\%}).

Keynotes:

A different study has used Sparse Reconstruction Classifier with K-means clustering

A different study obtained 99{\%} accuracy using SVM and Hamming Distance

They tried SVM, K-means, Linear Discrimintant in their own study as classifiers.},
author = {Khan, Fahim Faysal and Akif, Ahnaf and Haque, M A},
isbn = {9781538633748},
pages = {26--28},
title = {{Iris Recognition using Machine Learning from Smartphone Captured Images in Visible Light}},
year = {2017}
}
@article{Ghazi2016,
abstract = {Deep learning based approaches have been dominating the face recognition field due to the significant performance improvement they have provided on the challenging wild datasets. These approaches have been extensively tested on such unconstrained datasets, on the Labeled Faces in the Wild and YouTube Faces, to name a few. However, their capability to handle individual appearance variations caused by factors such as head pose, illumination, occlusion, and misalignment has not been thoroughly assessed till now. In this paper, we present a comprehensive study to evaluate the performance of deep learning based face representation under several conditions including the varying head pose angles, upper and lower face occlusion, changing illumination of different strengths, and misalignment due to erroneous facial feature localization. Two successful and publicly available deep learning models, namely VGG-Face and Lightened CNN have been utilized to extract face representations. The obtained results show that although deep learning provides a powerful representation for face recognition, it can still benefit from preprocessing, for example, for pose and illumination normalization in order to achieve better performance under various conditions. Particularly, if these variations are not included in the dataset used to train the deep learning model, the role of preprocessing becomes more crucial. Experimental results also show that deep learning based representation is robust to misalignment and can tolerate facial feature localization errors up to 10{\%} of the interocular distance.},
archivePrefix = {arXiv},
arxivId = {1606.02894},
author = {Ghazi, Mostafa Mehdipour and Ekenel, Hazim Kemal},
doi = {10.1109/CVPRW.2016.20},
eprint = {1606.02894},
isbn = {9781509014378},
issn = {21607516},
journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
month = {jun},
pages = {102--109},
publisher = {IEEE},
title = {{A Comprehensive Analysis of Deep Learning Based Representation for Face Recognition}},
url = {http://ieeexplore.ieee.org/document/7789510/ http://arxiv.org/abs/1606.02894},
year = {2016}
}
@article{Dolado2015,
abstract = {{\textcopyright} 2014, Psychonomic Society, Inc. Studying the collective behavior of fishes often requires tracking a great number of individuals. When many fishes move together, it is common for individuals to move so close to each other that some fishes superimpose themselves on others during one or several units of time, which impacts on tracking accuracy (i.e., loss of fish trajectories, interchange of fish identities). Type 1 occlusions arise when two fishes swim so near each other that they look like one long fish, whereas type 2 occlusions occur when the fishes' trajectories cross to create a T- or X-shaped individual. We propose an image processing method for resolving these types of occlusions when multitracking shoals in two dimensions. We assessed processing effectiveness after videorecording shoals of 20 and 40 individuals of two species that exhibit different shoal styles: zebrafish (Danio rerio) and black neon tetras (Hyphessobrycon herbertaxelrodi). Results show that, although the number of occlusions depended on both the number of individuals and the species, the method is able to effectively resolve a great deal of occlusions, irrespective of the species and the number of individuals. It also produces images that can be used in a multitracking system to detect individual fish trajectories. Compared to other methods, our approach makes it possible to study shoals with water depths similar to those seen in the natural conditions of the two species studied.},
author = {Dolado, Ruth and Gimeno, Elisabet and Beltran, Francesc S. and Quera, Vicen{\c{c}} and Pertusa, Jos{\'{e}} F.},
doi = {10.3758/s13428-014-0520-9},
file = {:home/niclas/Documents/uni/vgis10/vgis10/Report/bib/VGIS10.bib:bib},
issn = {15543528},
journal = {Behavior Research Methods},
keywords = {Collective behavior,Danio rerio,Hyphessobrycon herbertaxelrodi,Multitracking,Resolving occlusions},
month = {dec},
number = {4},
pages = {1032--1043},
publisher = {Springer US},
title = {{A method for resolving occlusions when multitracking individuals in a shoal}},
url = {http://link.springer.com/10.3758/s13428-014-0520-9},
volume = {47},
year = {2014}
}
@article{Rodriguez2017,
abstract = {{\textcopyright} 2017 The Author(s). Video analysis of animal behaviour is widely used in fields such as ecology, ecotoxicology, and evolutionary research. However, when tracking multiple animals, occlusion and crossing are problematic, especially when the identity of each individual needs to be preserved. We present a new algorithm, ToxId, which preserves the identity of multiple animals by linking trajectory segments using their intensity histogram and Hu-moments. We verify the performance and accuracy of our algorithm using video sequences with different animals and experimental conditions. The results show that our algorithm achieves state-of-the-art accuracy using an efficient approach without the need of learning processes, complex feature maps or knowledge of the animal shape. ToxId is also computationally efficient, has low memory requirements, and operates without accessing future or past frames.},
author = {Rodriguez, Alvaro and Zhang, Hanqing and Klaminder, Jonatan and Brodin, Tomas and Andersson, Magnus},
doi = {10.1038/s41598-017-15104-2},
file = {:home/niclas/Documents/uni/vgis10/vgis10/Report/bib/VGIS10.bib:bib},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
title = {{ToxId: An efficient algorithm to solve occlusions when tracking multiple animals}},
url = {https://www.nature.com/articles/s41598-017-15104-2},
volume = {7},
year = {2017}
}
@article{Elrefaei2017,
author = {Elrefaei, Lamiaa A and Hamid, Doaa H and Bayazed, Afnan A and Bushnak, Sara S and Maasher, Shaikhah Y},
doi = {10.1007/s11042-017-5049-3},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {Biometrics,Deep Sparse Filter,Hough transform,Iris recognition,Visible Light},
pages = {1--25},
publisher = {Multimedia Tools and Applications},
title = {{Developing Iris Recognition System for Smartphone Security}},
year = {2017}
}
@article{Percy,
author = {Percy, Oad},
journal = {Blekinge Institute of Technology},
pages = {1--48},
title = {{Iris localization using Daugman ' s algorithm}},
url = {http://www.diva-portal.org/smash/get/diva2:831173/FULLTEXT01.pdf}
}
@article{Fierrez-Aguilar2003a,
abstract = {The aim of this paper, regarding multimodal biometric verification, is twofold: on the one hand, some score fusion strategies reported in the literature are reviewed and, on the other hand, we compare experimentally a selection of them using as monomodal baseline experts: i) our face verification system based on a global face appearance representation scheme, ii) our minutiaebased fingerprint verification system, and iii) our on-line signature verification system based on HMM modeling of temporal functions, on the MCYT multimodal database. A new strategy is also proposed and discussed in order to generate a multimodal combined score by means of Support Vector Machine (SVM) classifiers from which user-independent and user-dependent fusion schemes are derived and evaluated.},
author = {Fierrez-Aguilar, J and Ortega-Garcia, J and Garcia-Romero, D and Gonzalez-Rodriguez, J},
isbn = {3-540-40302-7},
issn = {03029743},
journal = {Proc. AVBPA},
pages = {1056},
title = {{A Comparative Evaluation of Fusion Strategies for Multimodal Biometric Verification}},
year = {2003}
}
@article{Aakerberg2017,
abstract = {Object recognition is one of the important tasks in computer vision which has found enormous applications. Depth modality is proven to provide supplementary information to the common RGB modality for object recognition. In this paper, we propose methods to improve the recognition performance of an existing deep learning based RGB-D object recognition model, namely the FusionNet proposed by Eitel et al. First, we show that encoding the depth values as colorized surface normals is beneficial, when the model is initialized with weights learned from training on ImageNet data. Additionally, we show that the RGB stream of the FusionNet model can benefit from using deeper network architectures, namely the 16-layered VGGNet, in exchange for the 8-layered CaffeNet. In combination, these changes improves the recognition performance with 2.2{\%} in comparison to the original FusionNet, when evaluating on the Washington RGB-D Object Dataset.},
author = {Aakerberg, Andreas and Nasrollahi, Kamal and Rasmussen, Christoffer B. and Moeslund, Thomas B.},
doi = {10.5220/0006511501210128},
isbn = {978-989-758-274-5},
journal = {Proceedings of the 9th International Joint Conference on Computational Intelligence},
keywords = {artificial vision,computer vision,convolutional neural networks,deep learning,has found enormous applications,in computer vision which,learning,object recognition is one,of the important tasks,rgb-d,surface normals,transfer},
pages = {121--128},
title = {{Depth Value Pre-Processing for Accurate Transfer Learning based RGB-D Object Recognition}},
url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0006511501210128},
year = {2017}
}
@article{Bay2008,
abstract = {This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with SURF's application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline SURF's usefulness in a broad range of topics in computer vision. {\textcopyright} 2007 Elsevier Inc. All rights reserved.},
author = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and {Van Gool}, Luc},
doi = {10.1016/j.cviu.2007.09.014},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {Camera calibration,Feature description,Interest points,Local features,Object recognition},
month = {jun},
number = {3},
pages = {346--359},
publisher = {Academic Press},
title = {{Speeded-Up Robust Features (SURF)}},
url = {https://www.sciencedirect.com/science/article/pii/S1077314207001555?via{\%}3Dihub},
volume = {110},
year = {2008}
}
@article{Daugman1993,
author = {Daugman, J.G.},
doi = {10.1109/34.244676},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {11},
pages = {1148--1161},
title = {{High confidence visual recognition of persons by a test of statistical independence}},
url = {http://ieeexplore.ieee.org/document/244676/},
volume = {15},
year = {1993}
}
@article{Ross2003,
annote = {Nice and clear overview of the four basic modules of the standard biometric system.

Very clear explainations of different aspects, but a bit limited in respect to the separation of methods into categories .},
author = {Ross, Arun and Jain, Anil},
isbn = {1517355931},
keywords = {biometrics,decision tree,face,fingerprints,hand geometry,linear discriminant analysis,multimodal,sum rule,verification},
number = {13},
pages = {2115--2125},
title = {{Information Fusion in Biometrics}},
volume = {24},
year = {2003}
}
@misc{LiborMasek2003,
annote = {Open Source Matlab Iris Recognition system. Based on Daugmans approach.},
author = {{Libor Masek}, Peter Kovesi},
publisher = {The School of Computer Science and Software Engineering, The University of Western Australia},
title = {{MATLAB Source Code for a Biometric Identification System Based on Iris Patterns}},
url = {http://www.peterkovesi.com/studentprojects/libor/sourcecode.html},
year = {2003}
}
@article{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
isbn = {9781450341448},
issn = {09505849},
journal = {International Conference on Learning Representations (ICRL)},
month = {sep},
pages = {1--14},
pmid = {16873662},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2015}
}
@article{Chowdhury2016a,
author = {Chowdhury, Anurag and Ghosh, Soumyadeep and Singh, Richa and Vatsa, Mayank},
doi = {10.1109/BTAS.2016.7791199},
isbn = {9781467397339},
journal = {2016 IEEE 8th International Conference on Biometrics Theory, Applications and Systems, BTAS 2016},
title = {{RGB-D face recognition via learning-based reconstruction}},
year = {2016}
}
@article{Jund2016,
abstract = {With the increasing performance of machine learning techniques in the last few years, the computer vision and robotics communities have created a large number of datasets for benchmarking object recognition tasks. These datasets cover a large spectrum of natural images and object categories, making them not only useful as a testbed for comparing machine learning approaches, but also a great resource for bootstrapping different domain-specific perception and robotic systems. One such domain is domestic environments, where an autonomous robot has to recognize a large variety of everyday objects such as groceries. This is a challenging task due to the large variety of objects and products, and where there is great need for real-world training data that goes beyond product images available online. In this paper, we address this issue and present a dataset consisting of 5,000 images covering 25 different classes of groceries, with at least 97 images per class. We collected all images from real-world settings at different stores and apartments. In contrast to existing groceries datasets, our dataset includes a large variety of perspectives, lighting conditions, and degrees of clutter. Overall, our images contain thousands of different object instances. It is our hope that machine learning and robotics researchers find this dataset of use for training, testing, and bootstrapping their approaches. As a baseline classifier to facilitate comparison, we re-trained the CaffeNet architecture (an adaptation of the well-known AlexNet) on our dataset and achieved a mean accuracy of 78.9{\%}. We release this trained model along with the code and data splits we used in our experiments.},
archivePrefix = {arXiv},
arxivId = {1611.05799},
author = {Jund, Philipp and Abdo, Nichola and Eitel, Andreas and Burgard, Wolfram},
eprint = {1611.05799},
file = {::},
isbn = {3838361334},
month = {nov},
title = {{The Freiburg Groceries Dataset}},
url = {http://arxiv.org/abs/1611.05799},
year = {2016}
}
@article{Karpathy2016,
abstract = {These notes accompany the Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition.},
author = {Karpathy, Andrej},
journal = {Stanford University},
pages = {1--21},
title = {{Convolutional Neural Networks for Visual Recognition}},
url = {http://cs231n.github.io/classification/},
year = {2016}
}
@inproceedings{Hung2018,
author = {Hung, Pei Yuan and Chen, Kuan Yu and Hsu, Heng Wei and Wang, Ruei Hao},
booktitle = {Proceedings of 4th IEEE International Conference on Applied System Innovation 2018, ICASI 2018},
doi = {10.1109/ICASI.2018.8394526},
isbn = {9781538643426},
keywords = {3D infrared thermal imaging,autofocus,object tracking,speed-up robust features,stereo vision},
month = {apr},
pages = {1280--1283},
publisher = {IEEE},
title = {{Integration of autofocus and object tracking in an infrared stereo vision-based video surveillance system with multi-lens module}},
url = {https://ieeexplore.ieee.org/document/8394526/},
year = {2018}
}
@article{Wijesoma2005,
abstract = {Assistive robots are increasingly being used to improve the quality of the life of disabled or handicapped people. In this paper a complete system is presented that can be used by people with extremely limited peripheral mobility but having the ability for eye motor coordination. The electrooculogram signals (EOG) that results from the eye displacement in the orbit of the subject are processed in real time to interpret intent and hence generate appropriate control signals to the assistive device. The effectiveness of the proposed methodology and the algorithms are demonstrated using a mobile robot for a limited vocabulary},
author = {Wijesoma, W S and Wee, Kang Say Wee Kang Say and Wee, Ong Choon Wee Ong Choon and a.P. Balasuriya and San, Koh Tong San Koh Tong and Soon, Low Kay Soon Low Kay},
doi = {10.1109/ROBIO.2005.246316},
isbn = {0-7803-9315-5},
journal = {2005 IEEE International Conference on Robotics and Biomimetics - ROBIO},
keywords = {- electrooculography,disabled people,eoc,eye movements,severely,wheelchair},
pages = {490--494},
title = {{EOG based control of mobile assistive platforms for the severely disabled}},
year = {2005}
}
@article{Zhang2016,
author = {Zhang, Man and Zhang, Qi and Sun, Zhenan and Zhou, Shujuan and Ahmed, Nasir Uddin},
doi = {10.1109/BTAS.2016.7791191},
isbn = {9781467397339},
journal = {2016 IEEE 8th International Conference on Biometrics Theory, Applications and Systems, BTAS 2016},
title = {{The BTAS∗Competition on Mobile Iris Recognition}},
year = {2016}
}
@article{Kim2013,
author = {Kim, Myoung Ro and Yoon, Gilwon},
journal = {International Journal of Electrical, Computer, Energetic, Electronic and Communication Engineering},
keywords = {ddr game,eog,eye movement},
number = {10},
pages = {1352--1355},
title = {{Control Signal from EOG Analysis and Its Application}},
volume = {7},
year = {2013}
}
@article{Stewart2015,
abstract = {Background: Expanding the spectrum of organisms to model human brain phenotypes is critical for our improved understanding of the pathobiology of neuropsychiatric disorders. Given the clear limitations of existing mammalian models, there is an urgent need for low-cost, high-throughput in-vivo technologies for drug and gene discovery. New method: Here, we introduce a new automated method for generating 3D (X,Y,Z) swim trajectories in adult zebrafish (Danio rerio), to improve their neurophenotyping. Results: Based on the Track3D module of EthoVision XT video tracking software (Noldus Information Technology), this tool enhances the efficient, high-throughput 3D analyses of zebrafish behavioral responses. Applied to adult zebrafish behavior, this 3D method is highly sensitive to various classes of psychotropic drugs, including selected psychostimulant and hallucinogenic agents. Comparison with existing Methods: Our present method offers a marked advance in the existing 2D and 3D methods of zebrafish behavioral phenotyping, minimizing research time and recording high-resolution, automatically synchronized videos with calculated, high-precision object positioning. Conclusions: Our novel approach brings practical simplicity and 'integrative' capacity to the often complex and error-prone quantification of zebrafish behavioral phenotypes. Illustrating the value of 3D swim path reconstructions for identifying experimentally-evoked phenotypic profiles, this method fosters innovative, ethologically relevant, and fully automated small molecule screens using adult zebrafish.},
author = {Stewart, Adam Michael and Grieco, Fabrizio and Tegelenbosch, Ruud A.J. and Kyzar, Evan J and Nguyen, Michael and Kaluyeva, Aleksandra and Song, Cai and Noldus, Lucas P.J.J. and Kalueff, Allan V},
doi = {10.1016/j.jneumeth.2015.07.023},
file = {::},
issn = {1872678X},
journal = {Journal of Neuroscience Methods},
keywords = {Drug discovery,High-throughput screening,Neurophenotype,Video tracking,Zebrafish},
pages = {66--74},
title = {{A novel 3D method of locomotor analysis in adult zebrafish: Implications for automated detection of CNS drug-evoked phenotypes}},
url = {http://dx.doi.org/10.1016/j.jneumeth.2015.07.023},
volume = {255},
year = {2015}
}
@article{Ho1994,
abstract = {A multiple classifier system is a powerful solution to difficult pattern$\backslash$nrecognition problems involving large class sets and noisy input$\backslash$nbecause it allows simultaneous use of arbitrary feature descriptors$\backslash$nand classification procedures. Decisions by the classifiers can$\backslash$nbe represented as rankings of classifiers and different instances$\backslash$nof a problem. The rankings can be combined by methods that either$\backslash$nreduce or rerank a given set of classes. An intersection method$\backslash$nand union method are proposed for class set reduction. Three methods$\backslash$nbased on the highest rank, the Borda count, and logistic regression$\backslash$nare proposed for class set reranking. These methods have been tested$\backslash$nin applications of degraded machine-printed characters and works$\backslash$nfrom large lexicons, resulting in substantial improvement in overall$\backslash$ncorrectness.},
author = {Ho, Tin Kam and Hull, Jonathan J and Srihari, Sargur N},
doi = {http://dx.doi.org/10.1109/34.273716},
isbn = {0162-8828 VO - 16},
issn = {0162-8828},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
number = {1},
pages = {66--75},
title = {{Decision Combination in Multiple Classifier Systems}},
volume = {16},
year = {1994}
}
@article{Sequeira2014,
abstract = {Biometrics represents a return to a natural way of identification: testing someone by what (s)he is, instead of relying on something (s)he owns or knows seems likely to be the way forward. Biometric systems that include multiple sources of information are known as multimodal. Such systems are generally regarded as an alternative to fight a variety of problems all unimodal systems stumble upon. One of the main challenges found in the development of biometric recognition systems is the shortage of publicly available databases acquired under real unconstrained working conditions. Motivated by such need the MobBIO database was created using an Asus EeePad Transformer tablet, with mobile biometric systems in mind. The proposed database is composed by three modalities: iris, face and voice.},
author = {Sequeira, Ana F. and Monteiro, Joao C. and Rebelo, Ana and Oliveira, Helder P.},
isbn = {9789897580093},
journal = {9th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
number = {c},
pages = {1--14},
title = {{MobBIO: A Multimodal Database Captured with a Portable Handheld Device}},
year = {2014}
}
@article{Crossdata2018,
abstract = {In this paper we study face recognition using convolutional neural network. First, we introduced the basic CNN neural network architecture. Second, we modify the traditional neural network and adapt it to another database by fine tuning its parameters. Third, the network architecture is extended to the cross database problem. The CNN is first trained on a large dataset and then tested on another. Experimental results show that the proposed algorithm is suitable for building various real world applications.},
author = {Guo, Mei and Xiao, Min and Gong, Deliang},
doi = {10.1007/978-3-319-69096-4_54},
journal = {Advances in Intelligent Systems and Computing},
keywords = {Deep neural network {\'{A}},Face recognition {\'{A}},Image processing},
title = {{Face Recognition Using Deep Convolutional Neural Network in Cross-Database Study}},
url = {https://link-springer-com.zorac.aub.aau.dk/content/pdf/10.1007{\%}2F978-3-319-69096-4{\_}54.pdf},
year = {2017}
}
@techreport{Zhao,
abstract = {Due to object detection's close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles which combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy and optimization function, etc. In this paper, we provide a review on deep learning based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely Convolutional Neural Network (CNN). Then we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network based learning systems.},
archivePrefix = {arXiv},
arxivId = {1807.05511v1},
author = {Zhao, Zhong-Qiu and Zheng, Peng and Xu, Shou-Tao and Wu, Xindong},
eprint = {1807.05511v1},
file = {::},
keywords = {Index Terms-deep learning,neural network,object detection},
title = {{Object Detection with Deep Learning: A Review}},
url = {https://arxiv.org/pdf/1807.05511.pdf},
year = {2018}
}
@inproceedings{Keivani2017,
abstract = {Multiple objects detection and tracking are amongst the most important tasks in computer vision-based surveillance and activity recognition. This paper proposes a real-time multiple objects detection method and compares its performance with three existing methods. `Good Features to Track' algorithm is used to extract feature points from each frame. Based on the motion-based information, feature points corresponding to moving objects are extracted from next frame. Then, the number of moving objects in each frame is determined according to their motion-based information and position, and are later clustered using the k-means algorithm. Clustering of moving objects in this paper is performed using feature vectors made of pixels' intensities, motion magnitudes, motion directions and feature point positions. In terms of accuracy and efficiency, the proposed method is shown to be highly accurate in determining the number of moving objects and also fast in tracking them in the scene.},
author = {Keivani, Arghavan and Tapamo, Jules Raymond and Ghayoor, Farzad},
booktitle = {2017 IEEE AFRICON: Science, Technology and Innovation for Africa, AFRICON 2017},
doi = {10.1109/AFRCON.2017.8095451},
isbn = {9781538627754},
keywords = {Good Features to Track,K-means,KLT,Moving Object Detection,Object Tracking},
month = {sep},
pages = {32--37},
publisher = {IEEE},
title = {{Motion-based moving object detection and tracking using automatic K-means}},
url = {http://ieeexplore.ieee.org/document/8095451/},
year = {2017}
}
@article{Feijo2018,
abstract = {In this paper, a semi-automatic multi-object tracking method to track a group of unmarked zebrafish is proposed. This method can handle partial occlusion cases, maintaining the correct identity of each individual. For every object, we extracted a set of geometric features to be used in the two main stages of the algorithm. The first stage selected the best candidate, based both on the blobs identified in the image and the estimate generated by a Kalman Filter instance. In the second stage, if the same candidate-blob is selected by two or more instances, a blob-partitioning algorithm takes place in order to split this blob and reestablish the instances' identities. If the algorithm cannot determine the identity of a blob, a manual intervention is required. This procedure was compared against a manual labeled ground truth on four video sequences with different numbers of fish and spatial resolution. The performance of the proposed method is then compared against two well-known zebrafish tracking methods found in the literature: one that treats occlusion scenarios and one that only track fish that are not in occlusion. Based on the data set used, the proposed method outperforms the first method in correctly separating fish in occlusion, increasing its efficiency by at least 8.15{\%} of the cases. As for the second, the proposed method's overall performance outperformed the second in some of the tested videos, especially those with lower image quality, because the second method requires high-spatial resolution images, which is not a requirement for the proposed method. Yet, the proposed method was able to separate fish involved in occlusion and correctly assign its identity in up to 87.85{\%} of the cases, without accounting for user intervention.},
author = {Feij{\'{o}}, Gregory de Oliveira and Sangalli, Vicenzo Abichequer and da Silva, Isaac Newton Lima and Pinho, M{\'{a}}rcio Sarroglia},
doi = {10.1016/j.compbiomed.2018.01.011},
issn = {18790534},
journal = {Computers in Biology and Medicine},
keywords = {Digital image processing,Mathematical morphology,Multiple object tracking,Occlusion handling,Shoaling tracking},
month = {may},
pages = {79--90},
publisher = {Pergamon},
title = {{An algorithm to track laboratory zebrafish shoals}},
url = {https://www-sciencedirect-com.zorac.aub.aau.dk/science/article/pii/S0010482518300192},
volume = {96},
year = {2018}
}
@inproceedings{Girshick2014,
abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30{\%} relative to the previous best result on VOC 2012---achieving a mAP of 53.3{\%}. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\~{}}rbg/rcnn.},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {::},
isbn = {9781479951178},
issn = {10636919},
month = {nov},
pages = {580--587},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
url = {http://arxiv.org/abs/1311.2524},
year = {2014}
}
@article{Ribeiro2017,
author = {Ribeiro, Eduardo and Uhl, Andreas and Alonso-Fernandez, Fernando and Farrugia, Reuben A},
doi = {10.23919/EUSIPCO.2017.8081595},
isbn = {978-0-9928626-7-1},
journal = {2017 25th European Signal Processing Conference (EUSIPCO)},
pages = {2176--2180},
title = {{Exploring deep learning image super-resolution for iris recognition}},
url = {http://ieeexplore.ieee.org/document/8081595/},
volume = {2},
year = {2017}
}
@article{Wrobel2017a,
abstract = {{\textcopyright} 2017 IEEE. In this paper, a new approach for personal identity verification using finger knuckle images and least-square contour alignment method has been proposed. A special test rig with a digital camera was prepared for acquisition the knuckle images. Next, the obtained images of finger knuckle were subjected to image processing method in order to extract the knuckle furrows from them. The verification of person was performed by comparing the furrows on the verified and the reference knuckle images. To determine the similarity between the furrows we used the least-square contour alignment method. The usability of the proposed approach was tested experimentally. Practical experiments, conducted with our database, confirmed that results obtained are promising.},
author = {Wrobel, K. and Porwik, P. and Doroz, R. and Safaverdi, H.},
doi = {10.1109/ICBAKE.2017.8090616},
isbn = {9781538634004},
journal = {Proceedings of 2017 International Conference on Biometrics and Kansei Engineering, ICBAKE 2017},
keywords = {biometrics,finger knuckle images,least-square contour alignment,verification},
pages = {119--122},
title = {{Person verification based on finger knuckle images and least-squares contour alignment}},
year = {2017}
}
@article{Daugman2009,
abstract = {This chapter explains the iris recognition algorithms and presents results of 9.1 million comparisons among eye images from trials in Britain, the USA, Japan, and Korea. The key to iris recognition is the failure of a test of statistical independence, which involves so many degrees-of-freedom that this test is virtually guaranteed to be passed whenever the phase codes for two different eyes are compared, but to be uniquely failed when any eye's phase code is compared with another version of itself. The test of statistical independence is implemented by the simple Boolean Exclusive-OR operator (XOR) applied to the 2048 bit phase vectors that encode any two iris patterns, masked (AND'ed) by both of their corresponding mask bit vectors to prevent non iris artifacts from influencing iris comparisons. The XOR operator detects disagreement between any corresponding pair of bits, while the AND operator ensures that the compared bits are both deemed to have been uncorrupted by eyelashes, eyelids, specular reflections, or other noise. The norms of the resultant bit vector and of theAND'ed mask vectors are then measured in order to compute a fractional Hamming Distance as the measure of the dissimilarity between any two irises, whose two phase code bit vectors are denoted {\{}. codeA, codeB{\}} and whose mask bit vectors are denoted {\{}. maskA, maskB{\}}. {\textcopyright}2009 Elsevier Inc. All rights reserved.},
annote = {A chapter about how Iris Recognitions works in general. John Daugman is the creator of IrisCode, a 2D Gabor wavelet-based iris recognition algorithm that is the basis of all publicly deployed automatic iris recognition systems and which has registered more than a billion persons worldwide in government ID programs.

Keywords/phrases:
Near Infra Red (NIR) images can be used for iris capturing. 

Gabor wavelets are used for determining the inter and outer edges of an iris.

Often the iris will not be circular because an eyelid will cover it.

Hamming distance is used when comparing two irises.},
author = {Daugman, John},
doi = {10.1016/B978-0-12-374457-9.00025-1},
isbn = {9780123744579},
issn = {10518215},
journal = {The Essential Guide to Image Processing},
pages = {715--739},
pmid = {20810146},
title = {{How Iris Recognition Works}},
year = {2009}
}
@article{Jesus2017,
author = {Jesus, Rosales Banderas Jose De and Maximo, Lopez Sanchez and Raul, Pinto Elias and Gabriel, Gonzalez Serna},
doi = {10.1109/CSCI.2016.0167},
isbn = {9781509055104},
journal = {Proceedings - 2016 International Conference on Computational Science and Computational Intelligence, CSCI 2016},
keywords = {color calibration,image processing,image stabilizer,iris detection},
pages = {861--864},
title = {{Methodology for Iris Scanning through Smartphones}},
year = {2017}
}
@article{Postelnicu2012,
abstract = {This paper presents an EOG-based (Electrooculography) interface for Human Computer Interface (HCI) purposes. The solution enables the filtering of the recorded signals and identification of characteristic peak amplitudes associated with eye saccades, blinks or winks by using a classifier based on a set of fuzzy logic rules and a deterministic finite automaton. The identified eye saccades were assigned to six low-level commands for navigation purposes. An experiment study was conducted in order to check the accuracy and the performances of the proposed interface compared with three traditional input control interfaces. Experimental results show that the developed interface has good performance and can be used for online communication and control in EOG-based HCI systems or even for first-person navigation metaphors in games industry. {\textcopyright}2012 Elsevier Ltd. All rights reserved.},
author = {Postelnicu, Cristian Cezar and Girbacia, Florin and Talaba, Doru},
doi = {10.1016/j.eswa.2012.03.007},
isbn = {09574174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Deterministic finite automaton,Electrooculography,Fuzzy logic,Human computer interaction,Navigation,Virtual environment},
number = {12},
pages = {10857--10866},
publisher = {Elsevier Ltd},
title = {{EOG-based visual navigation interface development}},
url = {http://dx.doi.org/10.1016/j.eswa.2012.03.007},
volume = {39},
year = {2012}
}
@inproceedings{Or-El2015,
abstract = {The popularity of low-cost RGB-D scanners is increasing on a daily basis. Nevertheless, existing scanners often cannot capture subtle details in the environment. We present a novel method to enhance the depth map by fusing the intensity and depth information to create more detailed range profiles. The lighting model we use can handle natural scene illumination. It is integrated in a shape from shading like technique to improve the visual fidelity of the reconstructed object. Unlike previous efforts in this do- main, the detailed geometry is calculated directly, without the need to explicitly find and integrate surface normals. In addition, the proposed method operates four orders of magnitude faster than the state of the art. Qualitative and quantitative visual and statistical evidence support the im- provement in the depth obtained by the suggested method.},
author = {Or-El, Roy and Rosman, Guy and Wetzler, Aaron and Kimmel, Ron and Bruckstein, Alfred M.},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7299179},
isbn = {9781467369640},
issn = {10636919},
month = {jun},
pages = {5407--5416},
pmid = {7299179},
publisher = {IEEE},
title = {{RGBD-fusion: Real-time high precision depth recovery}},
url = {http://ieeexplore.ieee.org/document/7299179/},
volume = {07-12-June},
year = {2015}
}
@inproceedings{Eitel2015,
abstract = {Robust object recognition is a crucial ingredient of many, if not all, real-world robotics applications. This paper leverages recent progress on Convolutional Neural Networks (CNNs) and proposes a novel RGB-D architecture for object recognition. Our architecture is composed of two separate CNN processing streams - one for each modality - which are consecutively combined with a late fusion network. We focus on learning with imperfect sensor data, a typical problem in real-world robotics tasks. For accurate learning, we introduce a multi-stage training methodology and two crucial ingredients for handling depth data with CNNs. The first, an effective encoding of depth information for CNNs that enables learning without the need for large depth datasets. The second, a data augmentation scheme for robust learning with depth images by corrupting them with realistic noise patterns. We present state-of-the-art results on the RGB-D object dataset and show recognition in challenging RGB-D real-world noisy settings.},
archivePrefix = {arXiv},
arxivId = {1507.06821},
author = {Eitel, Andreas and Springenberg, Jost Tobias and Spinello, Luciano and Riedmiller, Martin and Burgard, Wolfram},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2015.7353446},
eprint = {1507.06821},
isbn = {9781479999941},
issn = {21530866},
month = {jul},
pages = {681--687},
title = {{Multimodal deep learning for robust RGB-D object recognition}},
url = {http://arxiv.org/abs/1507.06821},
volume = {2015-Decem},
year = {2015}
}
@inproceedings{Girshick2015,
abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
archivePrefix = {arXiv},
arxivId = {1504.08083v2},
author = {Girshick, Ross},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.169},
eprint = {1504.08083v2},
file = {::},
isbn = {9781467383912},
issn = {15505499},
pages = {1440--1448},
title = {{Fast R-CNN}},
url = {https://github.com/rbgirshick/},
volume = {2015 Inter},
year = {2015}
}
@article{Rattani2017,
abstract = {Ocular biometrics encompasses the imaging and use of characteristic features extracted from the eyes for personal recognition. Ocular biometric modalities in visible light have mainly focused on iris, blood vessel structures over the white of the eye (mostly due to conjunctival and episcleral layers), and periocular region around eye. Most of the existing studies on iris recognition use the near infrared spectrum. However, conjunctival vasculature and periocular regions are imaged in the visible spectrum. Iris recognition in the visible spectrum is possible for light color irides or by utilizing special illumination. Ocular recognition in the visible spectrum is an important research area due to factors such as recognition at a distance, suitability for recognition with regular RGB cameras, and adaptability to mobile devices. Further these ocular modalities can be obtained from a single RGB eye image, and then fused together for enhanced performance of the system. Despite these advantages, the state-of-the-art related to ocular biometrics in visible spectrum is not well known. This paper surveys this topic in terms of computational image enhancement, feature extraction, classification schemes and designed hardware-based acquisition set-ups. Future research directions are also enumerated to identify the path forward.},
annote = {A very detalied survey article. They go through a lot of different approaches to iris recognition. They also talk about the other biometrics that can be extracted from a VIS (Visible Spectrum) image like, moles/freckles/nevi. Other patterns can also b extraxted such as conjunctival vaslulature, periocular and retinal biometrics.

UBIRIS is one of the most used publicaly available databses for VIS iris images with noise.},
author = {Rattani, Ajita and Derakhshani, Reza},
doi = {10.1016/j.imavis.2016.11.019},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Biometrics,Conjunctival vasculature,Iris,Mobile biometrics,Ocular biometrics,Periocular biometrics,Visible spectrum},
pages = {1--16},
publisher = {Elsevier B.V.},
title = {{Ocular biometrics in the visible spectrum: A survey}},
url = {http://dx.doi.org/10.1016/j.imavis.2016.11.019},
volume = {59},
year = {2017}
}
@misc{LoligoS,
author = {{Loligo Systems}},
title = {{Loligo{\textregistered} Systems. About Us}},
url = {https://www.loligosystems.com/about-us},
urldate = {2019-03-20}
}
@article{Yilmaz2006,
abstract = {The goal of this article is to review the state-of-the-art tracking methods, classify them into different categories , and identify new trends. Object tracking, in general, is a challenging problem. Difficulties in tracking objects can arise due to abrupt object motion, changing appearance patterns of both the object and the scene, nonrigid object structures, object-to-object and object-to-scene occlusions, and camera motion. Tracking is usually performed in the context of higher-level applications that require the location and/or shape of the object in every frame. Typically, assumptions are made to constrain the tracking problem in the context of a particular application. In this survey, we categorize the tracking methods on the basis of the object and motion representations used, provide detailed descriptions of representative methods in each category, and examine their pros and cons. Moreover, we discuss the important issues related to tracking including the use of appropriate image features, selection of motion models, and detection of objects.},
author = {{Acm Reference Format: Yilmaz}, A and Javed, O and Shah, M},
doi = {10.1145/1177352.1177355},
file = {::},
journal = {ACM Comput. Surv},
keywords = {I48 [Image Processing and Computer Vision]: Scene,contour evolution,feature selection,object detection,object representation,point tracking,shape tracking},
pages = {45},
title = {{Object tracking: A survey}},
url = {http://doi.acm.org/10.1145/1177352.1177355},
volume = {38},
year = {2006}
}
@article{Sung2013,
abstract = {This study uses ZigBee wireless sensor network technology to build a Multi-purpose Electronic Score system based on the electro-oculogram (EOG). As a requirement when learning to play a musical instrument the most commonly seen scores are available in paper copies. The disadvantage of the paper music score is that pages must be turned manually by the performer while playing music. An alternative is to prepare a miniaturized copy of the score, but this makes the score difficult to read clearly. Electronic scores have recently become available and are expected to become mainstream in the future due to the rapid growth in ebook readers and tablet PCs. Tracking the upward, downward, leftward, rightward and clockwise/counterclockwise eyeball movements and eye blinking, an amplified, bandpass filtered electro-oculogram (EOG) signal is converted into digital form. This signal is further transmitted through a ZigBee wireless module on which an electronic score program is installed, including the following operating modes; go to the previous/next page, recording, tuning and tempo modes. Detected eyeball movements allow a score to be viewed conveniently while playing. In the future, the proposed technology can be applied to flexible paper, flexible displays and the like. {\textcopyright}2012 Elsevier B.V.},
author = {Sung, Wen Tsai and Chen, Jui Ho and Chang, Kuo Yi},
doi = {10.1016/j.sna.2012.11.028},
isbn = {0924-4247},
issn = {09244247},
journal = {Sensors and Actuators, A: Physical},
keywords = {EOG,Multipurpose Electronic Score,Wireless Sensor Network,ZigBee},
pages = {141--152},
publisher = {Elsevier B.V.},
title = {{ZigBee based multi-purpose electronic score design and implementation using EOG}},
url = {http://dx.doi.org/10.1016/j.sna.2012.11.028},
volume = {190},
year = {2013}
}
@incollection{RahmanKhan2018,
abstract = {Long-haul travel does not constitute an obstacle for tourists to travel and is fast gaining the attention of tourists in new and unique experiences. This study was conducted to identify the long-haul travel motivation by international tourists to Penang. A total of 400 respondents participated in this survey, conducted around the tourist attractions in Penang, using cluster random sampling. However, only 370 questionnaires were only used for this research. Data were analysed using SPSS software 22 version. The findings, ‘knowledge and novelty seeking' were the main push factors that drove long-haul travel by international tourists to Penang. Meanwhile, the main pull factor that attracts long- haul travel by international tourists to Penang was its ‘culture and history'. Additionally, there were partly direct and significant relationships between socio-demographic, trip characteristics and travel motivation (push factors and pull factors). Overall, this study identified the long-haul travel motivations by international tourists to Penang based on socio-demographic, trip characteristics and travel motivation and has indirectly helped in understanding the long-haul travel market particularly for Penang and Southeast Asia. This research also suggested for an effective marketing and promotion strategy in pro- viding useful information that is the key to attract international tourists to travel long distances. Keywords:},
author = {{Rahman Khan}, Farmanur and {Sulaiman Alhewairini}, Saleh},
booktitle = {Current Trends in Cancer Management [Working Title]},
doi = {10.5772/intechopen.81517},
file = {::},
month = {nov},
publisher = {IntechOpen},
title = {{Zebrafish (Danio rerio) as a Model Organism}},
url = {https://www.intechopen.com/online-first/zebrafish-danio-rerio-as-a-model-organism},
year = {2018}
}
@article{Chen2018,
abstract = {In this work, we address the problem of face verification, namely determining whether a pair of face images belongs to the same or different subjects. Previous works often consider solving the problem of face verification in two steps: feature extraction and face recognition, resulting in a fragmented procedure. We argue that these techniques, although working well, fail to explicitly exploit a full end-to-end framework for face verification, which has received much attention and achieved significant improvements recently. In this paper, we propose a novel Joint Bayesian guided metric learning technique for dealing with the face verification task, which well integrates the above two steps of face verification into an end-to-end convolutional neural network (CNN) architecture. In the training stage, an initial neural network, which has the similar architecture with GoogLeNet CNN model, is firstly pre-trained by optimizing classification-based objective functions on the publicly available CASIA WebFace database. Based on constructed face pairs dataset from CASIA WebFace and LFW datasets, we then fine-tune the whole network parameters under the guide of the learned knowledge, which is obtained from the highly successful Joint Bayesian model. This guided learning procedure, which can also be seen as a metric learning technique, can further update network parameters for discriminating face pairs. In the testing process, the outputs by this unified network are discriminated with a threshold value to produce the ultimate prediction for the face verification task. Comprehensive evaluations over the LFW dataset well demonstrate the encouraging face verification performance of our proposed framework.},
author = {Chen, Di and Xu, Chunyan and Yang, Jian and Qian, Jianjun and Zheng, Yuhui and Shen, Linlin},
doi = {10.1016/J.NEUCOM.2017.09.009},
issn = {0925-2312},
journal = {Neurocomputing},
month = {jan},
pages = {560--567},
publisher = {Elsevier},
title = {{Joint Bayesian guided metric learning for end-to-end face verification}},
url = {https://www-sciencedirect-com.zorac.aub.aau.dk/science/article/pii/S0925231217314807{\#}bib0016},
volume = {275},
year = {2018}
}
@misc{Delcourt2018,
abstract = {In laboratory fish research, the zebrafish Danio rerio (Cyprinidae) represents the equivalent of the mouse in mammalian research. This species has become a major model for studies in developmental and behavioural genetics, neurophysiology, biomedicine, ecotoxicology, and behavioural and evolutionary ecology. To meet the need for accurate and reproducible data in both fundamental and applied sciences, it is of primary importance to be able to tag and/or recognize individual zebrafish. However, classic methods used in fish ecology and aquaculture are generally difficult to apply to such small fish. Recently, various new tagging methods have been developed. This paper presents a first review of current identification and marking methods applied to zebrafish, from external observation methods (such as skin pattern recognition, fin clipping, scale regeneration, colour and transgenic methods) to the most advanced technological developments in electronic (low- and high- radio-frequencies PIT tags, microchip) and image analysis methods (video tracking). This review aims to help researchers and zebrafish facility managers select the identification method (ID) best adapted to their needs. The main characteristics of each ID method are examined (including detection range, durability, speed and repetitiveness, ID code combination, size dependence and ethical considerations), and their pros and cons are summarized in a decision table to help select the most appropriate option for a research or management program. Finally, contextual applications of these ID methods and future developments are discussed.},
author = {Delcourt, Johann and Ovidio, Micha{\"{e}}l and Deno{\"{e}}l, Mathieu and Muller, Marc and Pendeville, H{\'{e}}l{\`{e}}ne and Deneubourg, Jean Louis and Poncin, Pascal},
booktitle = {Reviews in Fish Biology and Fisheries},
doi = {10.1007/s11160-018-9537-y},
file = {::},
isbn = {0123456789},
issn = {15735184},
keywords = {Animal ID,Danio rerio,Passive integrated transponder,Tagging,VIE tag,Video tracking},
number = {4},
pages = {839--864},
title = {{Individual identification and marking techniques for zebrafish}},
url = {https://doi.org/10.1007/s11160-018-9537-y},
volume = {28},
year = {2018}
}
@article{Yang2017a,
abstract = {We present an end-to-end, multimodal, fully convolutional$\backslash$r$\backslash$nnetwork for extracting semantic structures from document$\backslash$r$\backslash$nimages. We consider document semantic structure$\backslash$r$\backslash$nextraction as a pixel-wise segmentation task, and propose a$\backslash$r$\backslash$nunified model that classifies pixels based not only on their$\backslash$r$\backslash$nvisual appearance, as in the traditional page segmentation$\backslash$r$\backslash$ntask, but also on the content of underlying text. Moreover,$\backslash$r$\backslash$nwe propose an efficient synthetic document generation process$\backslash$r$\backslash$nthat we use to generate pretraining data for our network.$\backslash$r$\backslash$nOnce the network is trained on a large set of synthetic$\backslash$r$\backslash$ndocuments, we fine-tune the network on unlabeled real documents$\backslash$r$\backslash$nusing a semi-supervised approach. We systematically$\backslash$r$\backslash$nstudy the optimum network architecture and show that$\backslash$r$\backslash$nboth our multimodal approach and the synthetic data pretraining$\backslash$r$\backslash$nsignificantly boost the performance.},
archivePrefix = {arXiv},
arxivId = {1706.02337},
author = {Yang, Xiao and Yumer, Ersin and Asente, Paul and Kraley, Mike and Kifer, Daniel and Giles, C. Lee},
doi = {10.1109/CVPR.2017.462},
eprint = {1706.02337},
isbn = {978-1-5386-0457-1},
journal = {Cvpr2017},
title = {{Learning to Extract Semantic Structure from Documents Using Multimodal Fully Convolutional Neural Networks}},
url = {https://arxiv.org/pdf/1706.02337.pdf},
year = {2017}
}
@inproceedings{Supreeth2018,
author = {Supreeth, H. S.G. and Patil, Chandrashekar M},
booktitle = {Proceedings of the International Conference on Inventive Communication and Computational Technologies, ICICCT 2018},
doi = {10.1109/ICICCT.2018.8473354},
isbn = {9781538619742},
keywords = {Computer Vision,Deep Learning,Foreground Detection,Object Detection,Object Tracking},
month = {apr},
pages = {1775--1780},
publisher = {IEEE},
title = {{Moving object detection and tracking using deep learning neural network and correlation filter}},
url = {https://ieeexplore.ieee.org/document/8473354/},
year = {2018}
}
@article{Arsalan2017,
author = {Arsalan, Muhammad and Hong, Hyung Gil and Naqvi, Rizwan Ali and Lee, Min Beom and Kim, Min Cheol and Kim, Dong Seop and Kim, Chan Sik and Park, Kang Ryoung},
doi = {10.3390/sym9110263},
issn = {20738994},
journal = {Symmetry},
keywords = {Biometrics,Convolutional neural network (CNN),Iris recognition,Iris segmentation},
number = {11},
title = {{Deep learning-based iris segmentation for iris recognition in visible light environment}},
volume = {9},
year = {2017}
}
@article{Dexnet3,
abstract = {Vacuum-based end effectors are widely used in industry and are often preferred over parallel-jaw and multifinger grippers due to their ability to lift objects with a single point of contact. Suction grasp planners often target planar surfaces on point clouds near the estimated centroid of an object. In this paper, we propose a compliant suction contact model that computes the quality of the seal between the suction cup and local target surface and a measure of the ability of the suction grasp to resist an external gravity wrench. To characterize grasps, we estimate robustness to perturbations in end-effector and object pose, material properties, and external wrenches. We analyze grasps across 1,500 3D object models to generate Dex-Net 3.0, a dataset of 2.8 million point clouds, suction grasps, and grasp robustness labels. We use Dex-Net 3.0 to train a Grasp Quality Convolutional Neural Network (GQ-CNN) to classify robust suction targets in point clouds containing a single object. We evaluate the resulting system in 350 physical trials on an ABB YuMi fitted with a pneumatic suction gripper. When evaluated on novel objects that we categorize as Basic (prismatic or cylindrical), Typical (more complex geometry), and Adversarial (with few available suction-grasp points) Dex-Net 3.0 achieves success rates of 98{\$}\backslash{\%}{\$}, 82{\$}\backslash{\%}{\$}, and 58{\$}\backslash{\%}{\$} respectively, improving to 81{\$}\backslash{\%}{\$} in the latter case when the training set includes only adversarial objects. Code, datasets, and supplemental material can be found at http://berkeleyautomation.github.io/dex-net .},
archivePrefix = {arXiv},
arxivId = {1709.06670},
author = {Mahler, Jeffrey and Matl, Matthew and Liu, Xinyu and Li, Albert and Gealy, David and Goldberg, Ken},
doi = {10.1109/ICRA.2018.8460887},
eprint = {1709.06670},
file = {::},
isbn = {978-1-5386-3081-5},
month = {sep},
title = {{Dex-Net 3.0: Computing Robust Robot Vacuum Suction Grasp Targets in Point Clouds using a New Analytic Model and Deep Learning}},
url = {http://arxiv.org/abs/1709.06670},
year = {2017}
}
@article{Suzuki1985,
abstract = {Two border following algorithms are proposed for the topological analysis of digitized binary images. The first one determines the surroundness relations among the borders of a binary image. Since the outer borders and the hole borders have a one-to-one correspondence to the connected components of 1-pixels and to the holes, respectively, the proposed algorithm yields a representation of a binary image, from which one can extract some sort of features without reconstructing the image. The second algorithm, which is a modified version of the first, follows only the outermost borders (i.e., the outer borders which are not surrounded by holes). These algorithms can be effectively used in component counting, shrinking, and topological structural analysis of binary images, when a sequential digital computer is used. {\textcopyright} 1985.},
author = {Suzuki, Satoshi and Be, Keiichi A.},
doi = {10.1016/0734-189X(85)90016-7},
issn = {0734189X},
journal = {Computer Vision, Graphics and Image Processing},
month = {apr},
number = {1},
pages = {32--46},
publisher = {Academic Press},
title = {{Topological structural analysis of digitized binary images by border following}},
url = {https://www-sciencedirect-com.zorac.aub.aau.dk/science/article/pii/0734189X85900167},
volume = {30},
year = {1985}
}
@article{Hidayatullah2017,
author = {Hidayatullah, Priyanto and Mengko, Tati L. E. R. and Munir, Rinaldi},
doi = {10.18178/ijmlc.2017.7.5.637},
issn = {20103700},
journal = {International Journal of Machine Learning and Computing},
month = {oct},
number = {5},
pages = {144--151},
title = {{A Survey on Multisperm Tracking for Sperm Motility Measurement}},
url = {http://www.ijmlc.org/vol7/637-C46.pdf},
volume = {7},
year = {2017}
}
@article{Karpathy2016,
abstract = {These notes accompany the Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition.},
author = {Karpathy, Andrej},
journal = {Stanford University},
pages = {1--21},
title = {{Convolutional Neural Networks for Visual Recognition}},
url = {http://cs231n.github.io/classification/},
year = {2016}
}
@book{Levin2009,
abstract = {Models are used to represent complex problems in simplified forms—physics, chemistry, and biology all make good use of models. The most familiar are the mathematical sorts that form the basis of natural science theory. In the life sciences, the concept of modeling can extend further to include experimental procedures and nonhuman subjects. For example, a neuroscientist might employ a rat running in a radial-arm maze to study working memory processes, or a mouse in an open-field test to study anxiety. The value of a model is primarily a function of its fidelity: in the case of a theoretical model, fidelity is measured in terms of predicted findings; in the case of biological models, the issue is couched in terms of validity. It is this second kind of model that concerns us in this chapter on neuroscience methods, where the challenge of model species is particularly acute because behavioral and brain processes are both extraordinarily complex, and the problem is to find species that display both interesting behavior and easily accessible neural processes. Rats and mice, unquestionably the most successful models in neuroscience, have been extremely effective in helping determine which mammalian brain regions and neurotransmitter systems involved in cognition, learning, and other varieties of behavioral function. But the invertebrate Aplysia, a marine mollusk, has also served as a molecular model of memory processes [1]. Such seemingly unrelated model species are useful to the extent that they balance external validity, simplicity, and cost. Most recently these considerations have led researchers in behavioral neuroscience to use fish, a sort of middle ground between rodents and mollusks. In this chapter we review progress in the behavioral neuroscience of the diminutive zebrafish (Danio rerio), a species that has already firmly established itself as a model of vertebrate development, and now opens new doors for the investigation of brain mechanisms. Zebrafish are sometimes identified as an alternative model (relative to classic rodent models), but the term complementary model might be more appropriate since it addresses the use of fish in addition to classic mammalian models. Some questions, such as about the role of frontal cortical and hippocampal structures in learning and memory, cannot be studied with fish since these are not evident (but see [2]). But other attributes of fish make them valuable models in behavioral neuroscience research. Developmental processes can be continuously visualized in species that have a clear chorion (egg sack). Reporter systems can highlight specific neural systems so that their proliferation, differentiation, migration, and projections can be easily discerned. Reversible genetic suppression through the morpholino technique can determine the importance of specific molecular mechanisms for neurodevelopment. Numerous mutants available also help with the evaluation of molecular mechanisms throughout life. Finally, fish are easily bred in great numbers and develop rapidly, reducing the cost of experimentation and significantly increasing research throughput—potentially, more experiments can be run in less time to answer any number of questions. The merit of fish models is now a matter of record. Zebrafish, in particular, have been well used in genetics, neuroscience, pharmacology, and toxicology (e.g., see [3–7]). The next and ongoing step is to extend the zebrafish model to pursue questions of behavioral neuroscience, an undertaking that requires valid, reliable, and efficient methods of behavioral assessment.},
author = {Levin, Edward D. and Cerutti, Daniel T.},
booktitle = {Methods of Behavior Analysis in Neuroscience},
isbn = {9781420052343},
pages = {Chapter 15},
pmid = {21204325},
publisher = {CRC Press/Taylor {\&} Francis},
title = {{Behavioral Neuroscience of Zebrafish}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21204325},
year = {2009}
}
@article{Mahler2017,
abstract = {To reduce data collection time for deep learning of robust robotic grasp plans, we explore training from a synthetic dataset of 6.7 million point clouds, grasps, and analytic grasp metrics generated from thousands of 3D models from Dex-Net 1.0 in randomized poses on a table. We use the resulting dataset, Dex-Net 2.0, to train a Grasp Quality Convolutional Neural Network (GQ-CNN) model that rapidly predicts the probability of success of grasps from depth images, where grasps are specified as the planar position, angle, and depth of a gripper relative to an RGB-D sensor. Experiments with over 1,000 trials on an ABB YuMi comparing grasp planning methods on singulated objects suggest that a GQ-CNN trained with only synthetic data from Dex-Net 2.0 can be used to plan grasps in 0.8sec with a success rate of 93{\%} on eight known objects with adversarial geometry and is 3x faster than registering point clouds to a precomputed dataset of objects and indexing grasps. The Dex-Net 2.0 grasp planner also has the highest success rate on a dataset of 10 novel rigid objects and achieves 99{\%} precision (one false positive out of 69 grasps classified as robust) on a dataset of 40 novel household objects, some of which are articulated or deformable. Code, datasets, videos, and supplementary material are available at http://berkeleyautomation.github.io/dex-net .},
archivePrefix = {arXiv},
arxivId = {1703.09312},
author = {Mahler, Jeffrey and Liang, Jacky and Niyaz, Sherdil and Laskey, Michael and Doan, Richard and Liu, Xinyu and Ojea, Juan Aparicio and Goldberg, Ken},
doi = {10.15607/RSS.2017.XIII.058},
eprint = {1703.09312},
file = {::},
isbn = {978-0-9923747-3-0},
issn = {0929-5593},
month = {mar},
pmid = {23641116},
title = {{Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics}},
url = {http://arxiv.org/abs/1703.09312},
year = {2017}
}
@book{Bowyer2016b,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-{\$}\alpha{\$}-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA}for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
annote = {Read only chapter 17 about iris and face fusion. The article gives a nice and clean overview of different multi-biometric systems as well as levels of data abstraction the data fusion can be applied on. 

The work presented performs iris and face fusion using multi-sample, multi instance, and multimodal data. it fuses the multi sample, and multi instance together, by simply finding the best match in the variations. this is done as a score level fusion. and it fuses the two modalities by rank-level fusion using Broda count.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Connaughton, Ryan and Bowyer, Kevin W and Flynn, Patrick J},
booktitle = {Handbook of Iris Recognition},
doi = {10.1007/978-1-4471-6784-6},
eprint = {arXiv:1011.1669v3},
isbn = {978-1-4471-6782-2},
issn = {16130073},
pages = {397--415},
pmid = {25246403},
title = {{Chapter 17 Fusion of Face and Iris Biometrics}},
url = {http://link.springer.com/10.1007/978-1-4471-6784-6},
year = {2016}
}
@article{Proenca2017,
abstract = {The effectiveness of current iris recognition systems de-pends on the accurate segmentation and parameterisation of the iris boundaries, as failures at this point misalign the coefficients of the biometric signatures. This paper de-scribes IRINA, an algorithm for Iris Recognition that is ro-bust against INAccurately segmented samples, which makes it a good candidate to work in poor-quality data. The pro-cess is based in the concept of " corresponding " patch be-tween pairs of images, that is used to estimate the posterior probabilities that patches regard the same biological region, even in case of segmentation errors and non-linear texture deformations. Such information enables to infer a free-form deformation field (2D registration vectors) between images, whose first and second-order statistics provide effective bio-metric discriminating power. Extensive experiments were carried out in four datasets (CASIA-IrisV3-Lamp, CASIA-IrisV4-Lamp, CASIA-IrisV4-Thousand and WVU) and show that IRINA not only achieves state-of-the-art performance in good quality data, but also handles effectively severe seg-mentation errors and large differences in pupillary dilation / constriction.},
author = {Proenca, Hugo and Neves, Joao C},
doi = {10.1109/CVPR.2017.714},
isbn = {978-1-5386-0457-1},
journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {6747--6756},
title = {{IRINA: Iris Recognition (Even) in Inaccurately Segmented Data}},
url = {http://ieeexplore.ieee.org/document/8100197/},
year = {2017}
}
@article{Redmon2015,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
archivePrefix = {arXiv},
arxivId = {1506.02640},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
eprint = {1506.02640},
file = {::},
month = {jun},
title = {{You Only Look Once: Unified, Real-Time Object Detection}},
url = {http://arxiv.org/abs/1506.02640},
year = {2015}
}
@article{Ren2017,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features-using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3] , our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1109/TPAMI.2016.2577031},
file = {::},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Object detection,convolutional neural network,region proposal},
number = {6},
pages = {1137--1149},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
url = {https://github.com/},
volume = {39},
year = {2017}
}
@inproceedings{Zhao2015a,
abstract = {This paper proposes a novel and more accurate iris segmentation framework to automatically segment iris region from the face images acquired with relaxed imaging under visible or near-infrared illumination, which provides strong feasibility for applications in surveillance, forensics and the search for missing children, etc. The proposed framework is built on a novel total-variation based formulation which uses l1 norm regularization to robustly suppress noisy texture pixels for the accurate iris localization. A series of novel and robust post processing operations are introduced to more accurately localize the limbic boundaries. Our experimental results on three publicly available databases, i.e., FRGC, UBIRIS.v2 and CASIA.v4-distance, achieve significant performance improvement in terms of iris segmentation accuracy over the state-of-the-art approaches in the literature. Besides, we have shown that using iris masks generated from the proposed approach helps to improve iris recognition performance as well. Unlike prior work, all the implementations in this paper are made publicly available to further advance research and applications in biometrics at-d-distance.},
author = {Zhao, Zijing and Kumar, Ajay},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.436},
isbn = {9781467383912},
issn = {15505499},
month = {dec},
pages = {3828--3836},
publisher = {IEEE},
title = {{An accurate iris segmentation framework under relaxed imaging constraints using total variation model}},
url = {http://ieeexplore.ieee.org/document/7410793/},
volume = {2015 Inter},
year = {2015}
}
@article{Mhaske2013a,
author = {Mhaske, V. D. and Patankar, A. J.},
doi = {10.1109/ICCIC.2013.6724125},
isbn = {9781479915972},
journal = {2013 IEEE International Conference on Computational Intelligence and Computing Research, IEEE ICCIC 2013},
keywords = {Biometrics,Fingerprint,Fusion,MGF,Multimodal,Palm print,ROI,Unimodal},
title = {{Multimodal biometrics by integrating fingerprint and palmprint for security}},
year = {2013}
}
@techreport{Pedersen2017,
abstract = {The use of aquatic animals for research purposes has been increasingly popular in recent years and this is especially true for the zebrafish (Danio rerio), which is being used for genetic, envi- ronmental and pharmaceutical studies, among others. The motion trajecto- ries of the fish can be an important metric and the studies are therefore often supported by a computer vision system, which automatically tracks the fish. However, the tracking is often only conducted on a single fish or in two dimensions, as most of this type of tracking software has been developed for mice, rats or similar. The focus of this thesis is hence to investigate the possibilities of tracking zebrafish in three dimensions. The proposed system is made of off-the-shelf hardware and consists of a stereo-vision setup with two GoPro HERO5 cameras. The zebrafish are detected in each camera using the SURF keypoint extractor after which the 3D positions are estimated. The 3D reconstruction is based on ray- tracing in combination with Snell's law, in order to account for refraction. By using a Kalman filter and Munkres algorithm it is possible to assemble tracklets with an average length of 150 frames. A final part of the system, responsible for linking the tracklets, has not been implemented in this iteration of the system.},
author = {Bengtson, Stefan Hein and Pedersen, Malte},
title = {{Tracking Zebrafish in 3D using Stereo Vision}},
year = {2017}
}
@article{Zhao2017b,
abstract = {Person re-identification (ReID) is an important task in video surveillance and has various applications. It is non-trivial due to complex background clutters, varying illu-mination conditions, and uncontrollable camera settings. Moreover, the person body misalignment caused by detec-tors or pose variations is sometimes too severe for feature matching across images. In this study, we propose a novel Convolutional Neural Network (CNN), called Spindle Net, based on human body region guided multi-stage feature de-composition and tree-structured competitive feature fusion. It is the first time human body structure information is con-sidered in a CNN framework to facilitate feature learning. The proposed Spindle Net brings unique advantages: 1) it separately captures semantic features from different body regions thus the macro-and micro-body features can be well aligned across images, 2) the learned region features from different semantic regions are merged with a competitive scheme and discriminative features can be well preserved. State of the art performance can be achieved on multiple datasets by large margins. We further demonstrate the ro-bustness and effectiveness of the proposed Spindle Net on our proposed dataset SenseReID without fine-tuning.},
author = {Zhao, Haiyu and Tian, Maoqing and Sun, Shuyang and Shao, Jing and Yan, Junjie and Yi, Shuai and Wang, Xiaogang and Tang, Xiaoou},
doi = {10.1109/CVPR.2017.103},
isbn = {978-1-5386-0457-1},
journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {907--915},
title = {{Spindle Net: Person Re-identification with Human Body Region Guided Feature Decomposition and Fusion}},
url = {http://ieeexplore.ieee.org/document/8099586/},
volume = {1},
year = {2017}
}
@inproceedings{Song2016,
abstract = {{\textcopyright} 2016 IEEE. This paper presents an online multiple object tracking (MOT) method based on tracking by detection. Tracking by detection has the inherent problems by false and miss detection. To deal with the false detection, we employed the Gaussian mixture probability hypothesis density (GM-PHD) filter because this filter is robust to noisy and random data processing containing many false observations. Thus, we revised the GM-PHD filter for visual MOT. Also, to handle miss detection, we propose a hierarchical tracking framework to associate fragmented or ID switched tracklets. Experiments with the representative dataset PETS 2009 S2L1 show that our framework are effective to decrease the errors by false and miss detection, and real-Time capability.},
author = {Song, Young Min and Jeon, Moongu},
booktitle = {2016 IEEE International Conference on Consumer Electronics-Asia, ICCE-Asia 2016},
doi = {10.1109/ICCE-Asia.2016.7804800},
isbn = {9781509027439},
keywords = {Multiple object tracking,Online,Tracking by detection},
month = {oct},
pages = {1--4},
publisher = {IEEE},
title = {{Online multiple object tracking with the hierarchically adopted GM-PHD filter using motion and appearance}},
url = {http://ieeexplore.ieee.org/document/7804800/},
year = {2017}
}
@article{Wan2017a,
abstract = {State-of-the-art methods for 3D hand pose estimation from depth images require large amounts of annotated training data. We propose to model the statistical relationships of 3D hand poses and corresponding depth images using two deep generative models with a shared latent space. By design, our architecture allows for learning from unlabeled image data in a semi-supervised manner. Assuming a one-to-one mapping between a pose and a depth map, any given point in the shared latent space can be projected into both a hand pose and a corresponding depth map. Regressing the hand pose can then be done by learning a discriminator to estimate the posterior of the latent pose given some depth map. To improve generalization and to better exploit unlabeled depth maps, we jointly train a generator and a discriminator. At each iteration, the generator is updated with the back-propagated gradient from the discriminator to synthesize realistic depth maps of the articulated hand, while the discriminator benefits from an augmented training set of synthesized and unlabeled samples. The proposed discriminator network architecture is highly efficient and runs at 90 FPS on the CPU with accuracies comparable or better than state-of-art on 3 publicly available benchmarks.},
archivePrefix = {arXiv},
arxivId = {1702.03431},
author = {Wan, Chengde and Probst, Thomas and {Van Gool}, Luc and Yao, Angela},
doi = {10.1109/CVPR.2017.132},
eprint = {1702.03431},
journal = {Cvpr2017},
pages = {10},
title = {{Crossing Nets: Dual Generative Models with a Shared Latent Space for Hand Pose Estimation}},
url = {http://arxiv.org/abs/1702.03431},
year = {2017}
}
@article{Nam2016a,
abstract = {We propose Dual Attention Networks (DANs) which jointly leverage visual and textual attention mechanisms to capture fine-grained interplay between vision and language. DANs attend to specific regions in images and words in text through multiple steps and gather essential information from both modalities. Based on this framework, we introduce two types of DANs for multimodal reasoning and matching, respectively. The reasoning model allows visual and textual attentions to steer each other during collaborative inference, which is useful for tasks such as Visual Question Answering (VQA). In addition, the matching model exploits the two attention mechanisms to estimate the similarity between images and sentences by focusing on their shared semantics. Our extensive experiments validate the effectiveness of DANs in combining vision and language, achieving the state-of-the-art performance on public benchmarks for VQA and image-text matching.},
archivePrefix = {arXiv},
arxivId = {1611.00471},
author = {Nam, Hyeonseob and Ha, Jung-Woo and Kim, Jeonghee},
doi = {10.1109/CVPR.2017.232},
eprint = {1611.00471},
isbn = {978-1-5386-0457-1},
issn = {1611.00471},
pages = {299--307},
title = {{Dual Attention Networks for Multimodal Reasoning and Matching}},
url = {http://arxiv.org/abs/1611.00471},
year = {2016}
}
@article{Abate2017,
abstract = {The increasing popularity of smartphones amongst the population laid the basis for a wide range of applications aimed at security and privacy protection. Very modern mobile devices have recently demonstrated the feasibility of using a camera sensor to access the system without typing any alphanumerical password. In this work, we present a method that implements iris recognition in the visible spectrum through unsupervised learning by means of Self Organizing Maps (SOM). The proposed method uses a SOM network to cluster iris features at pixel level. The discriminative feature map is obtained by using RGB data of the iris combined with the statistical descriptors of kurtosis and skewness. An experimental analysis on MICHE-I and UBIRISv1 datasets demonstrates the strengths and weaknesses of the algorithm, which has been specifically designed to require low processing power in compliance with the limited capability of common mobile devices.},
author = {Abate, Andrea F and Barra, Silvio and Gallo, Luigi and Narducci, Fabio},
doi = {10.1016/j.patrec.2017.02.002},
isbn = {0000000000},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Iris recognition,Mobile biometric recognition,Statistical descriptors,Unsupervised learning},
pages = {37--43},
pmid = {11341202},
publisher = {Elsevier B.V.},
title = {{Kurtosis and skewness at pixel level as input for SOM networks to iris recognition on mobile devices}},
volume = {91},
year = {2017}
}
@inproceedings{Seo2017,
abstract = {{\textcopyright} 2016 IEEE.This paper proposes a new method for robust 3D object tracking from a single RGB image when an object model is available. The proposed method is based on image alignment between consecutive frames over a 3D target object. Different from conventional methods that only rely on image intensity for the alignment, we model intensity variations using the surface normal of the object. From this model, we also define a new constraint for the pose estimation, leading to significant improvement in the tracking robustness. In experiments, we demonstrate the benefits of our method by evaluating it under challenging tracking conditions.},
author = {Seo, Byung Kuk and Wuest, Harald},
booktitle = {Adjunct Proceedings of the 2016 IEEE International Symposium on Mixed and Augmented Reality, ISMAR-Adjunct 2016},
doi = {10.1109/ISMAR-Adjunct.2016.0042},
isbn = {9781509037407},
keywords = {H.5.1 [Information Systems]: Information Interface,I.4.8 [Image Processing and Computer Vision]: Scen,and virtual realities,augmented},
month = {sep},
pages = {70--71},
publisher = {IEEE},
title = {{Robust 3D Object Tracking Using an Elaborate Motion Model}},
url = {http://ieeexplore.ieee.org/document/7836463/},
year = {2017}
}
@article{Fierrez2018b,
abstract = {We provide an introduction to Multiple Classifier Systems (MCS) including basic nomenclature and describing key elements: classifier dependencies, type of classifier outputs, aggregation procedures, architecture, and types of methods. This introduction complements other existing overviews of MCS, as here we also review the most prevalent theoretical framework for MCS and discuss theoretical developments related to MCS. The introduction to MCS is then followed by a review of the application of MCS to the particular field of multimodal biometric person authentication in the last 25 years, as a prototypical area in which MCS has resulted in important achievements. This review includes general descriptions of successful MCS methods and architectures in order to facilitate the export of them to other information fusion problems. Based on the theory and framework introduced here, in the companion paper we then develop in more technical detail recent trends and developments in MCS from multimodal biometrics that incorporate context information in an adaptive way. These new MCS architectures exploit input quality measures and pattern-specific particularities that move apart from general population statistics, resulting in robust multimodal biometric systems. Similarly as in the present paper, methods in the companion paper are introduced in a general way so they can be applied to other information fusion problems as well. Finally, also in the companion paper, we discuss open challenges in biometrics and the role of MCS to advance them.},
author = {Fierrez, Julian and Morales, Aythami and Vera-Rodriguez, Ruben and Camacho, David},
doi = {10.1016/j.inffus.2017.12.003},
issn = {15662535},
journal = {Information Fusion},
keywords = {Adaptive,Biometrics,Classifier,Context,Fusion,Multimodal},
number = {November 2017},
pages = {57--64},
publisher = {Elsevier},
title = {{Multiple classifiers in biometrics. part 1: Fundamentals and review}},
url = {https://doi.org/10.1016/j.inffus.2017.12.003},
volume = {44},
year = {2018}
}
@article{lfw2007,
abstract = {Face recognition has benefitted greatly from the many databases that have been produced to study it. Most of these databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, expression, background, camera quality, occlusion, age, and gender. While there are many applications for face recognition technol- ogy in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database is provided as an aid in studying the latter, unconstrained, face recognition problem. The database represents an initial attempt to provide a set of labeled face photographs spanning the range of conditions typically encountered by people in their everyday lives. The database exhibits natural variability in pose, lighting, focus, resolution, facial expression, age, gender, race, accessories, make-up, occlusions, background, and photographic quality. Despite this variability, the images in the database are presented in a simple and consistent format for maximum ease of use. In addition to describing the details of the database and its acquisition, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible.},
author = {Huang, Gary B and Ramesh, Manu and Berg, Tamara and Learned-Miller, Erik},
doi = {10.1.1.122.8268},
isbn = {9781628414844},
issn = {1996756X},
journal = {University of Massachusetts Amherst Technical Report},
pages = {7--49},
title = {{Labeled faces in the wild: A database for studying face recognition in unconstrained environments}},
url = {http://vis-www.cs.umass.edu/lfw/lfw.pdf},
volume = {1},
year = {2007}
}
@article{Phillips2009,
abstract = {The goal of the Multiple Biometrics Grand Challenge (MBGC) is to improve the performance of face and iris recognition technology from biometric samples acquired under unconstrained conditions. The MBGC is organized into three challenge problems. Each challenge problem re- laxes the acquisition constraints in different directions. In the Portal Challenge Problem, the goal is to recognize people from near-infrared (NIR) and high definition (HD) video as they walk through a portal. Iris recognition can be performed from the NIR video and face recognition from the HD video. The availability of NIR and HD modalities allows for the development of fusion algorithms. The Still Face Challenge Problem has two primary goals. The first is to improve recognition performance from frontal and off angle still face images taken under uncontrolled in- door and outdoor lighting. The second is to improve recognition perfor- mance on still frontal face images that have been resized and compressed, as is required for electronic passports. In the Video Challenge Problem, the goal is to recognize people from video in unconstrained environments. The video is unconstrained in pose, illumination, and camera angle. All three challenge problems include a large data set, experiment descrip- tions, ground truth, and scoring code.},
author = {Phillips, P. Jonathon and Flynn, Patrick J. and Beveridge, J. Ross and Scruggs, W. Todd and O'Toole, Alice J. and Bolme, David and Bowyer, Kevin W. and Draper, Bruce A. and Givens, Geof H. and Lui, Yui Man and Sahibzada, Hassan and Scallan, Joseph A. and Weimer, Samuel},
doi = {10.1007/978-3-642-01793-3_72},
isbn = {3642017924},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {705--714},
title = {{Overview of the multiple biometrics grand challenge}},
volume = {5558 LNCS},
year = {2009}
}
@article{Johnson2010a,
abstract = {Identification of individuals using biometric information has found great success in many security and law enforcement applications. Up until the present time, most research in the field has been focused on ideal conditions and most available databases are constructed in these ideal conditions. There has been a growing interest in the perfection of these technologies at a distance and in less than ideal conditions, i.e. low lighting, out-of-focus blur, off angles, etc. This paper presents a dataset consisting of face and iris videos obtained at distances of 5 to 25 feet and in conditions of varying quality. The purpose of this database is to set a standard for quality measurement in face and iris data and to provide a means for analyzing biométrie systems in less than ideal conditions. The structure of the dataset as well as a quantified metric for quality measurement based on a 25 subject subset of the dataset is presented.},
author = {Johnson, P. A. and Lopez-Meyer, P. and Sazonova, N. and Hua, F. and Schuckers, S.},
doi = {10.1109/BTAS.2010.5634513},
isbn = {9781424475803},
journal = {IEEE 4th International Conference on Biometrics: Theory, Applications and Systems, BTAS 2010},
title = {{Quality in face and Iris research ensemble (Q-FIRE)}},
year = {2010}
}
@article{Lv2015,
abstract = {There is an increasing interest in creating pervasive games based on emerging interaction technologies. In order to develop touch-less, interactive and augmented reality games on vision-based wearable device, a touch-less motion interaction technology is designed and evaluated in this work. Users interact with the augmented reality games with dynamic hands/feet gestures in front of the camera, which triggers the interaction event to interact with the virtual object in the scene. Three primitive augmented reality games with eleven dynamic gestures are developed based on the proposed touch-less interaction technology as proof. At last, a comparing evaluation is proposed to demonstrate the social acceptability and usability of the touch-less approach, running on a hybrid wearable framework or with Google Glass, as well as workload assessment, user's emotions and satisfaction.},
author = {Lv, Zhihan and Halawani, Alaa and Feng, Shengzhong and {Ur R{\'{e}}hman}, Shafiq and Li, Haibo},
doi = {10.1007/s00779-015-0844-1},
isbn = {16174909 (ISSN)},
issn = {16174909},
journal = {Personal and Ubiquitous Computing},
keywords = {Augmented reality game,Hand free,Pervasive game,Smartphone game,Touch-less,Wearable device},
number = {3},
pages = {551--567},
title = {{Touch-less interactive augmented reality game on vision-based wearable device}},
volume = {19},
year = {2015}
}
@article{B2017,
author = {B, Qi Wang and Su, Xia and Cai, Zhenlin and Zhang, Xiangde},
doi = {10.1007/978-3-319-69923-3},
isbn = {978-3-319-69922-6},
keywords = {joint bayesian,mobile iris recognition,ordinal measures},
number = {3},
pages = {401--410},
title = {{Biometric Recognition}},
url = {http://link.springer.com/10.1007/978-3-319-69923-3},
volume = {10568},
year = {2017}
}
@article{Krizhevsky2017a,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0 {\%} which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2 {\%} achieved by the second-best entry. 1},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
doi = {10.1145/3065386},
eprint = {1102.0183},
isbn = {9781627480031},
issn = {00010782},
journal = {Communications of the ACM},
month = {may},
number = {6},
pages = {84--90},
pmid = {7491034},
publisher = {ACM},
title = {{ImageNet classification with deep convolutional neural networks}},
url = {http://dl.acm.org/citation.cfm?doid=3098997.3065386},
volume = {60},
year = {2017}
}
@article{Hakkinen1993,
abstract = {The primary aim of the study was to determine the best electrode positions for EOG signals in vigilance studies. Two-channel recordings were conducted in analogy to the Rechtschaffen and Kales (1968) system. Twenty electrodes (10 electrode pairs) were compared. Both EOG amplitudes and amplitude asymmetries within an electrode pair were studied. The amplitude of the EOG signal is sensitive to relatively small differences in electrode position. This concerns especially distance from the eye, the direction of eye movement and the effect of the upper eye lid movement. Larger and more symmetrical EOG amplitudes were obtained for different eye movements by placing the electrodes more medially than in the conventionally used system. EOG asymmetry in different electrode positions was dependent on the eye movement direction and even on the starting and end points of a movement with equal angular degrees. Most of the data could be explained by a simple monopolar model when combined with the effects of the upper eye lid movements. The most unexpected finding was that the EOG amplitudes of the horizontal and oblique eye movements were significantly larger when the eye were moving towards an electrode than when they were moving to the opposite direction. {\textcopyright}1993.},
author = {H{\"{a}}kkinen, V and Hirvonen, K and Hasan, J and Kataja, M and V{\"{a}}rri, A and Loula, P and Eskola, H},
doi = {10.1016/0013-4694(93)90111-8},
issn = {00134694},
journal = {Electroencephalography and Clinical Neurophysiology},
keywords = {EOG,Electrode positions,Vigilance},
number = {4},
pages = {294--300},
pmid = {7682933},
title = {{The effect of small differences in electrode position on EOG signals: application to vigilance studies}},
volume = {86},
year = {1993}
}
@article{Wang2009a,
abstract = {Feature-level fusion remains a challenging problem for multimodal biometrics. However, existing fusion schemes such as sum rule and weighted sum rule are inefficient in complicated condition. In this paper, we propose an efficient feature-level fusion algorithm for iris and face in parallel. The algorithm first normalizes the original features of iris and face using z-score model, and then take complex FDA as the classifier of Unitary space. The proposed algorithm is tested using CASIA iris database and two face databases (ORL database and Yale database.). Experimental results show the effectiveness of the proposed algorithm.},
author = {Wang, Zhifang and Han, Qi and Niu, Xiamu and Busch, Christoph},
doi = {10.1007/978-3-642-01513-7_38},
isbn = {3642015123},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Biometrics,CFDA,Feature-level,Parallel fusion,Unitary space},
number = {PART 3},
pages = {356--364},
title = {{Feature-level fusion of Iris and face for personal identification}},
volume = {5553 LNCS},
year = {2009}
}
@article{Elrefaei2017,
author = {Elrefaei, Lamiaa A. and Hamid, Doaa H. and Bayazed, Afnan A. and Bushnak, Sara S. and Maasher, Shaikhah Y.},
doi = {10.1007/s11042-017-5049-3},
file = {::},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {Biometrics,Deep Sparse Filter,Hough transform,Iris recognition,Visible Light},
pages = {1--25},
publisher = {Multimedia Tools and Applications},
title = {{Developing Iris Recognition System for Smartphone Security}},
year = {2017}
}
@article{Arslan2017a,
abstract = {Biometric systems may be used to create a remote access model on devices, ensure personal data protection, personalize and facilitate the access security. Biometric systems are generally used to increase the security level in addition to the previous authentication methods and they seen as a good solution. Biometry occupies an important place between the areas of daily life of the machine learning. In this study; the techniques, methods, technologies used in biometric systems are researched, machine learning techniques used biometric aplications are investigated for the security perspective, the advantages and disadvantages that these tecniques provide are given. The studies in the literature between 2010-2016 years, used algorithms, technologies, metrics, usage areas, the machine learning techniques used for different biometric systems such as face, palm prints, iris, voice, fingerprint recognition are researched and the studies made are evaluated. The level of security provided by the use of biometric systems by developed using machine learning and disadvantages that arise in the use of these systems are stated in detail in the study. Also, impact on people of biometric methods in terms of ease of use, security and usages areas are examined.},
author = {Arslan, B. and Yorulmaz, E. and Akca, B. and Sagiroglu, S.},
doi = {10.1109/ICMLA.2016.183},
file = {::},
isbn = {9781509061662},
journal = {Proceedings - 2016 15th IEEE International Conference on Machine Learning and Applications, ICMLA 2016},
keywords = {Biometric,Face,Fingerprint,Iris,Machine learning,Recognition,Security,Teeth,Voice},
pages = {492--497},
title = {{Security perspective of Biometric recognition and machine learning techniques}},
year = {2017}
}
@article{Galdi2017,
abstract = {FIRE is a Fast Iris REcognition algorithm especially designed for iris recognition on mobile phones under visible-light. It is based on the combination of three classifiers exploiting the iris colour and texture information. Its limited computational time makes FIRE particularly suitable for fast user verification on mobile devices. The high parallelism of the code allows its use also on large databases. FIRE, in its first version, was submitted to the Mobile Iris CHallenge Evaluation part II held in 2016. In this paper, FIRE is further improved: a number of different techniques has been analyzed and the best performing ones have been selected for fusion at score level. Performance are assessed in terms of Recognition Rate (RR), Area Under Receiver Operating Characteristic Curve (AUC), and Equal Error Rate (EER).},
author = {Galdi, Chiara and Dugelay, Jean Luc},
doi = {10.1016/j.patrec.2017.01.023},
file = {::},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Fast iris recognition,MICHE DB,MICHE II,Multi-classifier,Noisy iris recognition,Visible light},
pages = {44--51},
publisher = {Elsevier B.V.},
title = {{FIRE: Fast Iris REcognition on mobile phones by combining colour and texture features}},
url = {http://dx.doi.org/10.1016/j.patrec.2017.01.023},
volume = {91},
year = {2017}
}
@inproceedings{Kaur2016,
abstract = {Abstract: Face recognition is a type of biometric software application by using which, we can analyzing, identifying or verifying digital image of the person by using the feature of the face of the person that are unique characteristics of each person. These characteristics may be physical or behavioral. The physiological characteristics as like finger print, iris scan, or face etc and behavior characteristics as like hand-writing, voice, key stroke etc. Face recognition is very useful in many areas such as military, airports, universities, ATM, and banks etc, used for the security purposes. There are many techniques or algorithms that are used features extraction in face recognition. This paper make a review of some of those methods which are used for the face recognition that are Principal Component Analysis (PCA), Back Propagation Neural Networks (BPNN), Genetic Algorithm, and LDA, SVM, Independent Component Analysis(ICA). Each method has different -2 functions that are used for the face recognition. Dimensionality is reduced by using the Eigen face approach or PCA, LDA to extract the features from images. Genetic Algorithm is based on feature selection and Back propagation Neural Network (BPNN) is used for the classification of face images.},
author = {Kaur, Gurpreet and Kanwal, Navdeep},
booktitle = {International Conference on Computing for Sustainable Global Development (INDIACom)},
isbn = {9789380544212},
pages = {2705--2710},
publisher = {IEEE},
title = {{A Comparative Review of Various Approaches for Feature Extraction in Face Recognition}},
url = {http://ieeexplore.ieee.org/document/7724754/},
year = {2016}
}
@article{lfw2007,
abstract = {Face recognition has benefitted greatly from the many databases that have been produced to study it. Most of these databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, expression, background, camera quality, occlusion, age, and gender. While there are many applications for face recognition technol- ogy in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database is provided as an aid in studying the latter, unconstrained, face recognition problem. The database represents an initial attempt to provide a set of labeled face photographs spanning the range of conditions typically encountered by people in their everyday lives. The database exhibits natural variability in pose, lighting, focus, resolution, facial expression, age, gender, race, accessories, make-up, occlusions, background, and photographic quality. Despite this variability, the images in the database are presented in a simple and consistent format for maximum ease of use. In addition to describing the details of the database and its acquisition, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible.},
author = {Huang, Gary B and Ramesh, Manu and Berg, Tamara and Learned-Miller, Erik},
doi = {10.1.1.122.8268},
file = {::},
isbn = {9781628414844},
issn = {1996756X},
journal = {University of Massachusetts Amherst Technical Report},
pages = {07--49},
title = {{Labeled faces in the wild: A database for studying face recognition in unconstrained environments}},
url = {http://vis-www.cs.umass.edu/lfw/lfw.pdf},
volume = {1},
year = {2007}
}
@article{Saha2017a,
annote = {Not a very good article. but it has some nice references.

Keypoints

It is possible to aquire iris images in multiple way including Near Infrared (NIRD). a simple lense and monochome CCD camera, Adaboost cascade iris detector.

Iris localization is done by Daugman using a 2D Gabor Filter and Fisher Linear Discriminate method.

To help localize the iris despite of eyelashes a 1D rankfilter and histogram filter can be used.},
author = {Saha, Rishmita and Kundu, Mahasweta and Dutta, Madhuparna and Majumder, Rahul and Mukherjee, Debosmita and Pramanik, Sayak and Thakur, Uttam Narendra and Mukherjee, Chiradeep},
file = {::},
isbn = {9781538633717},
journal = {Information Technology, Electronics and Mobile Communication Conference (IEMCON), 2017 8th IEEE Annual},
pages = {685--688},
title = {{A Brief Study on Evolution of Iris Recognition System}},
url = {http://ieeexplore.ieee.org.ezproxy.psu.edu.sa/stamp/stamp.jsp?arnumber=8117234},
year = {2017}
}
@article{Fierrez2018b,
abstract = {We provide an introduction to Multiple Classifier Systems (MCS) including basic nomenclature and describing key elements: classifier dependencies, type of classifier outputs, aggregation procedures, architecture, and types of methods. This introduction complements other existing overviews of MCS, as here we also review the most prevalent theoretical framework for MCS and discuss theoretical developments related to MCS. The introduction to MCS is then followed by a review of the application of MCS to the particular field of multimodal biometric person authentication in the last 25 years, as a prototypical area in which MCS has resulted in important achievements. This review includes general descriptions of successful MCS methods and architectures in order to facilitate the export of them to other information fusion problems. Based on the theory and framework introduced here, in the companion paper we then develop in more technical detail recent trends and developments in MCS from multimodal biometrics that incorporate context information in an adaptive way. These new MCS architectures exploit input quality measures and pattern-specific particularities that move apart from general population statistics, resulting in robust multimodal biometric systems. Similarly as in the present paper, methods in the companion paper are introduced in a general way so they can be applied to other information fusion problems as well. Finally, also in the companion paper, we discuss open challenges in biometrics and the role of MCS to advance them.},
author = {Fierrez, Julian and Morales, Aythami and Vera-Rodriguez, Ruben and Camacho, David},
doi = {10.1016/j.inffus.2017.12.003},
file = {::},
issn = {15662535},
journal = {Information Fusion},
keywords = {Adaptive,Biometrics,Classifier,Context,Fusion,Multimodal},
number = {November 2017},
pages = {57--64},
publisher = {Elsevier},
title = {{Multiple classifiers in biometrics. part 1: Fundamentals and review}},
url = {https://doi.org/10.1016/j.inffus.2017.12.003},
volume = {44},
year = {2018}
}
@article{Zapata2017a,
author = {Zapata, J C and Duque, C M and Gonzalez, M E},
doi = {10.1007/978-981-10-5427-3},
file = {::},
isbn = {978-981-10-5426-6},
keywords = {biometric {\'{a}} data fusion,signals {\'{a}} signal processing},
number = {i},
pages = {721--733},
title = {{Advances in Computing and Data Sciences}},
url = {http://link.springer.com/10.1007/978-981-10-5427-3},
volume = {721},
year = {2017}
}
@article{Gopal2018a,
author = {Gopal and Srivastava, Smriti},
doi = {10.1007/s13369-017-2644-6},
file = {::},
issn = {21914281},
journal = {Arabian Journal for Science and Engineering},
keywords = {Feature-level fusion,Multimodal system,Palm–phalanges,Score-level fusion,Unimodal system},
number = {2},
pages = {543--554},
publisher = {Springer Berlin Heidelberg},
title = {{Accurate Human Recognition by Score-Level and Feature-Level Fusion Using Palm–Phalanges Print}},
volume = {43},
year = {2018}
}
@article{LeCun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
file = {::},
isbn = {9780521835688},
issn = {14764687},
journal = {Nature},
keywords = {Computer science,Mathematics and computing},
month = {may},
number = {7553},
pages = {436--444},
pmid = {10463930},
publisher = {Nature Publishing Group},
title = {{Deep learning}},
url = {http://www.nature.com/articles/nature14539},
volume = {521},
year = {2015}
}
@article{Wang2009a,
abstract = {Feature-level fusion remains a challenging problem for multimodal biometrics. However, existing fusion schemes such as sum rule and weighted sum rule are inefficient in complicated condition. In this paper, we propose an efficient feature-level fusion algorithm for iris and face in parallel. The algorithm first normalizes the original features of iris and face using z-score model, and then take complex FDA as the classifier of Unitary space. The proposed algorithm is tested using CASIA iris database and two face databases (ORL database and Yale database.). Experimental results show the effectiveness of the proposed algorithm.},
author = {Wang, Zhifang and Han, Qi and Niu, Xiamu and Busch, Christoph},
doi = {10.1007/978-3-642-01513-7_38},
file = {::},
isbn = {3642015123},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Biometrics,CFDA,Feature-level,Parallel fusion,Unitary space},
number = {PART 3},
pages = {356--364},
title = {{Feature-level fusion of Iris and face for personal identification}},
volume = {5553 LNCS},
year = {2009}
}
@article{Zhao2017a,
annote = {Good article describing a lot of the state of the art deep learning approaches being researched.

Keynotes:

Not much work with deep learning has been done in iris recognition.

THey Use a Fully Convolutional Network (FCN) and talk about others who ahve used a Convolutional Neural Network (CNN). They also mention a Deep Belief Net (DBN) that others have used. DeepIrisNet is also mentioned and tested with

THey have created their own loss function optimized for iris recognition called Extended Triplet Loss (ETL)

Their network is generalizable to other databases meaning that it doesn't require finetuing as many others do.

Used ND-IRIS, CASIA Iris, IITD Iris and WVU Non-Ideal Iris databases to test on.},
author = {Zhao, Zijing and Kumar, Ajay},
doi = {10.1109/ICCV.2017.411},
file = {::},
isbn = {978-1-5386-1032-9},
journal = {2017 IEEE International Conference on Computer Vision (ICCV)},
pages = {3829--3838},
title = {{Towards More Accurate Iris Recognition Using Deeply Learned Spatially Corresponding Features}},
url = {http://ieeexplore.ieee.org/document/8237673/},
year = {2017}
}
@article{Al-Waisy2017a,
abstract = {Multimodal biometrie systems seek to alleviate some of the limitations of unimodal biometrie systems by combining multiple pieces of evidence of the same person in the deeision-making process. In this paper, a novel multimodal biometric identification system is proposed based on fusing the results obtained from both the face and the left and right irises using deep learning approaches. Firstly, the facial features are extracted using a Deep Belief Network (DBN) architecture consisting of 3-layers. The first two RBMs are used as features detectors, while the last one is used as a discriminative model associated with softmax units for the multi-class classification purpose. Secondly, an efficient deep learning system is employed for iris recognition, whose architecture is based on a combination of Convolutional Neural Network (CNN) and Softmax classifier to extract discriminative features from an iris image. Extensive experiments on large-scale challenging databases, including FERET, CASIA V1.0 and MMU1, and SDUMLA-HMT have demonstrated the superiority of the proposed approaches by achieving new state-of-the-art Rank-1 identification rates on all the employed databases.},
author = {Al-Waisy, A S and Qahwaji, R and Ipson, S and Al-Fahdawi, S},
doi = {10.1109/EST.2017.8090417},
file = {::},
isbn = {9781538640173},
journal = {2017 Seventh International Conference on Emerging Security Technologies (EST)},
keywords = {Databases;Face;Feature extraction;Iris recognition},
pages = {163--168},
title = {{A multimodal biometrie system for personal identification based on deep learning approaches}},
year = {2017}
}
@inproceedings{deepID2014,
abstract = {The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset, 99.15{\%} face verification accuracy is achieved. Compared with the best deep learning result on LFW, the error rate has been significantly reduced by 67{\%}.},
archivePrefix = {arXiv},
arxivId = {1406.4773},
author = {Sun, Yi and Wang, Xiaogang and Tang, Xiaoou},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.244},
eprint = {1406.4773},
file = {::},
isbn = {9781479951178},
issn = {10636919},
keywords = {deep learning,face verification},
pages = {1891--1898},
pmid = {21808091},
title = {{Deep learning face representation from predicting 10,000 classes}},
url = {http://mmlab.ie.cuhk.edu.hk/pdf/YiSun{\_}CVPR14.pdf},
year = {2014}
}
@article{Mellakh2009a,
author = {Mellakh, A and Chaari, A and Guerfi, S and Dhose, J and Colineau, J and Lelandais, S and Petrovska-Delacr{\`{e}}taz, D and Dorizzi, B},
doi = {10.1007/978-3-642-04697-1_3},
file = {::},
isbn = {03029743; 3642046967 (ISBN); 9783642046964 (ISBN)},
issn = {03029743},
journal = {11th International Conference on Advanced Concepts for Intelligent Vision Systems, ACIVS 2009},
keywords = {2D face recognition,Appearance based,Appearance-based algorithms,Computer vision,Database,Database systems,Discriminant analysis,Evaluation campaign,Experimental protocols,Face images,Face recognition,Linear discriminant analysis,Multi-modal,Multimodal database,Principal component analysis,Training sets},
pages = {24--32},
title = {{2D face recognition in the IV2 evaluation campaign}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-70549098348{\&}partnerID=40{\&}md5=0541be27bb29a037c965f0d644b15856},
volume = {5807 LNCS},
year = {2009}
}
@article{Pathak2016a,
author = {Pathak, Mrunal and Srinivasu, N},
doi = {10.1007/978-981-10-2630-0},
file = {::},
isbn = {978-981-10-2629-4},
keywords = {biometrics},
pages = {137--152},
title = {{Advances in Computing Applications}},
url = {http://link.springer.com/10.1007/978-981-10-2630-0},
year = {2016}
}
@article{Karpathy2016,
abstract = {These notes accompany the Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition.},
author = {Karpathy, Andrej},
journal = {Stanford University},
pages = {1--21},
title = {{Convolutional Neural Networks for Visual Recognition}},
url = {http://cs231n.github.io/classification/},
year = {2016}
}
@article{Wijesoma2005,
abstract = {Assistive robots are increasingly being used to improve the quality of the life of disabled or handicapped people. In this paper a complete system is presented that can be used by people with extremely limited peripheral mobility but having the ability for eye motor coordination. The electrooculogram signals (EOG) that results from the eye displacement in the orbit of the subject are processed in real time to interpret intent and hence generate appropriate control signals to the assistive device. The effectiveness of the proposed methodology and the algorithms are demonstrated using a mobile robot for a limited vocabulary},
author = {Wijesoma, W.S. and Wee, Kang Say Wee Kang Say and Wee, Ong Choon Wee Ong Choon and a.P. Balasuriya and San, Koh Tong San Koh Tong and Soon, Low Kay Soon Low Kay},
doi = {10.1109/ROBIO.2005.246316},
file = {::},
isbn = {0-7803-9315-5},
journal = {2005 IEEE International Conference on Robotics and Biomimetics - ROBIO},
keywords = {- electrooculography,disabled people,eoc,eye movements,severely,wheelchair},
pages = {490--494},
title = {{EOG based control of mobile assistive platforms for the severely disabled}},
year = {2005}
}
@article{Crossdata2018,
abstract = {In this paper we study face recognition using convolutional neural network. First, we introduced the basic CNN neural network architecture. Second, we modify the traditional neural network and adapt it to another database by fine tuning its parameters. Third, the network architecture is extended to the cross database problem. The CNN is first trained on a large dataset and then tested on another. Experimental results show that the proposed algorithm is suitable for building various real world applications.},
author = {Guo, Mei and Xiao, Min and Gong, Deliang},
doi = {10.1007/978-3-319-69096-4_54},
file = {::},
journal = {Advances in Intelligent Systems and Computing},
keywords = {Deep neural network {\'{A}},Face recognition {\'{A}},Image processing},
title = {{Face Recognition Using Deep Convolutional Neural Network in Cross-Database Study}},
url = {https://link-springer-com.zorac.aub.aau.dk/content/pdf/10.1007{\%}2F978-3-319-69096-4{\_}54.pdf},
year = {2017}
}
@article{Aakerberg2017,
abstract = {Object recognition is one of the important tasks in computer vision which has found enormous applications. Depth modality is proven to provide supplementary information to the common RGB modality for object recognition. In this paper, we propose methods to improve the recognition performance of an existing deep learning based RGB-D object recognition model, namely the FusionNet proposed by Eitel et al. First, we show that encoding the depth values as colorized surface normals is beneficial, when the model is initialized with weights learned from training on ImageNet data. Additionally, we show that the RGB stream of the FusionNet model can benefit from using deeper network architectures, namely the 16-layered VGGNet, in exchange for the 8-layered CaffeNet. In combination, these changes improves the recognition performance with 2.2{\%} in comparison to the original FusionNet, when evaluating on the Washington RGB-D Object Dataset.},
author = {Aakerberg, Andreas and Nasrollahi, Kamal and Rasmussen, Christoffer B. and Moeslund, Thomas B.},
doi = {10.5220/0006511501210128},
file = {::},
isbn = {978-989-758-274-5},
journal = {Proceedings of the 9th International Joint Conference on Computational Intelligence},
keywords = {artificial vision,computer vision,convolutional neural networks,deep learning,has found enormous applications,in computer vision which,learning,object recognition is one,of the important tasks,rgb-d,surface normals,transfer},
pages = {121--128},
title = {{Depth Value Pre-Processing for Accurate Transfer Learning based RGB-D Object Recognition}},
url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0006511501210128},
year = {2017}
}
@article{Cheng2017a,
abstract = {This paper focuses on indoor semantic segmentation us-ing RGB-D data. Although the commonly used deconvolu-tion networks (DeconvNet) have achieved impressive results on this task, we find there is still room for improvements in two aspects. One is about the boundary segmentation. DeconvNet aggregates large context to predict the label of each pixel, inherently limiting the segmentation precision of object boundaries. The other is about RGB-D fusion. Re-cent state-of-the-art methods generally fuse RGB and depth networks with equal-weight score fusion, regardless of the varying contributions of the two modalities on delineating different categories in different scenes. To address the two problems, we first propose a locality-sensitive DeconvNet (LS-DeconvNet) to refine the boundary segmentation over each modality. LS-DeconvNet incorporates locally visual and geometric cues from the raw RGB-D data into each DeconvNet, which is able to learn to upsample the coarse convolutional maps with large context whilst recovering sharp object boundaries. Towards RGB-D fusion, we introduce a gated fusion layer to effectively combine the two LS-DeconvNets. This layer can learn to adjust the contributions of RGB and depth over each pixel for high-performance object recognition. Experiments on the large-scale SUN RGB-D dataset and the popular NYU-Depth v2 dataset show that our approach achieves new state-of-the-art results for RGB-D indoor semantic segmentation.},
author = {Cheng, Yanhua and Cai, Rui and Li, Zhiwei and Zhao, Xin and Huang, Kaiqi},
doi = {10.1109/CVPR.2017.161},
file = {::},
isbn = {978-1-5386-0457-1},
journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {1475--1483},
title = {{Locality-Sensitive Deconvolution Networks with Gated Fusion for RGB-D Indoor Semantic Segmentation}},
url = {http://ieeexplore.ieee.org/document/8099644/},
year = {2017}
}
@inproceedings{Misztal2012,
abstract = {A new method for finding the rotation angle in iris images for biometric identification is presented in this paper. The proposed approach is based on Fourier descriptors analysis and algebraic properties of vector rotation in complex space. {\textcopyright} 2012 IFIP International Federation for Information Processing.},
author = {Misztal, Krzysztof and Tabor, Jacek and Saeed, Khalid},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-33260-9-11},
file = {::},
isbn = {9783642332593},
issn = {03029743},
keywords = {Fourier descriptors,Iris pattern recognition,rotation estimation,rotation recovery},
pages = {135--145},
title = {{A new algorithm for rotation detection in iris pattern recognition}},
volume = {7564 LNCS},
year = {2012}
}
@article{Petrovska-Delacretaz2008a,
abstract = {Face recognition finds its place in a large number of applications. They occur in different contexts related to security, entertainment or Internet applications. Reliable face recognition is still a great challenge to computer vision and pattern recognition researchers, and new algorithms need to be evaluated on relevant databases. The publicly available IV2 database allows monomodal and multimodal experiments using face data. Known variabilities, that are critical for the performance of the biometric systems (such as pose, expression, illumination and quality) are present. The face and subface data that are acquired in this database are: 2D audio-video talking-face sequences, 2D stereoscopic data acquired with two pairs of synchronized cameras, 3D facial data acquired with a laser scanner, and iris images acquired with a portable infrared camera. The IV2 database is designed for monomodal and multimodal experiments. The quality of the acquired data is of great importance. Therefore as a first step, and in order to better evaluate the quality of the data, a first internal evaluation was conducted. Only a small amount of the total acquired data was used for this evaluation: 2D still images, 3D scans and iris images. First results show the interest of this database. In parallel to the research algorithms, open-source reference systems were also run for baseline comparisons.},
author = {Petrovska-Delacr{\'{e}}taz, D. and Lelandais, S. and Colineau, J. and Chen, L. and Dorizzi, B. and Ardabilian, M. and Krichen, E. and Mellakh, M. A. and Chaari, A. and Guerfi, S. and D'Hose, J. and Amor, B. Ben},
doi = {10.1109/BTAS.2008.4699323},
file = {::},
isbn = {9781424427307},
journal = {BTAS 2008 - IEEE 2nd International Conference on Biometrics: Theory, Applications and Systems},
pages = {3--9},
title = {{The IV2 multimodal biometric database (including Iris, 2D, 3D, stereoscopic, and talking face data), and the IV2-2007 evaluation campaign}},
volume = {00},
year = {2008}
}
@article{Khiari-Hili2017a,
author = {Khiari-Hili, Nefissa and Montagne, Christophe and Lelandais, Sylvie and Hamrouni, Kamel},
doi = {10.1109/IPTA.2016.7820954},
file = {::},
isbn = {9781467389105},
journal = {2016 6th International Conference on Image Processing Theory, Tools and Applications, IPTA 2016},
keywords = {Multimodal biometrics,authentication,dynamic weighted sum,face,iris,quality,score fusion},
pages = {1--6},
title = {{Quality dependent multimodal fusion of face and iris biometrics}},
year = {2017}
}
@article{Sequeira2014,
abstract = {Biometrics represents a return to a natural way of identification: testing someone by what (s)he is, instead of relying on something (s)he owns or knows seems likely to be the way forward. Biometric systems that include multiple sources of information are known as multimodal. Such systems are generally regarded as an alternative to fight a variety of problems all unimodal systems stumble upon. One of the main challenges found in the development of biometric recognition systems is the shortage of publicly available databases acquired under real unconstrained working conditions. Motivated by such need the MobBIO database was created using an Asus EeePad Transformer tablet, with mobile biometric systems in mind. The proposed database is composed by three modalities: iris, face and voice.},
author = {Sequeira, Ana F. and Monteiro, Joao C. and Rebelo, Ana and Oliveira, Helder P.},
file = {::},
isbn = {9789897580093},
journal = {9th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
number = {c},
pages = {1--14},
title = {{MobBIO: A Multimodal Database Captured with a Portable Handheld Device}},
year = {2014}
}
@article{Hu2015,
abstract = {Deep learning, in particular Convolutional Neural Network (CNN), has achieved promising results in face recognition recently. However, it remains an open question: why CNNs work well and how to design a 'good' architecture. The existing works tend to focus on reporting CNN architectures that work well for face recognition rather than investigate the reason. In this work, we conduct an extensive evaluation of CNN-based face recognition systems (CNN-FRS) on a common ground to make our work easily reproducible. Specifically, we use public database LFW (Labeled Faces in the Wild) to train CNNs, unlike most existing CNNs trained on private databases. We propose three CNN architectures which are the first reported architectures trained using LFW data. This paper quantitatively compares the architectures of CNNs and evaluate the effect of different implementation choices. We identify several useful properties of CNN-FRS. For instance, the dimensionality of the learned features can be significantly reduced without adverse effect on face recognition accuracy. In addition, traditional metric learning method exploiting CNN-learned features is evaluated. Experiments show two crucial factors to good CNN-FRS performance are the fusion of multiple CNNs and metric learning. To make our work reproducible, source code and models will be made publicly available.},
archivePrefix = {arXiv},
arxivId = {1504.02351},
author = {Hu, Guosheng and Yang, Yongxin and Yi, Dong and Kittler, Josef and Christmas, William and Li, Stan Z. and Hospedales, Timothy},
doi = {10.1109/ICCVW.2015.58},
eprint = {1504.02351},
file = {::},
isbn = {9780769557205},
issn = {15505499},
journal = {2015 IEEE International Conference on Computer Vision Workshop (ICCVW)},
month = {dec},
pages = {384--392},
publisher = {IEEE},
title = {{When Face Recognition Meets with Deep Learning: an Evaluation of Convolutional Neural Networks for Face Recognition}},
url = {http://ieeexplore.ieee.org/document/7406407/ http://arxiv.org/abs/1504.02351},
year = {2015}
}
@article{Postelnicu2012,
abstract = {This paper presents an EOG-based (Electrooculography) interface for Human Computer Interface (HCI) purposes. The solution enables the filtering of the recorded signals and identification of characteristic peak amplitudes associated with eye saccades, blinks or winks by using a classifier based on a set of fuzzy logic rules and a deterministic finite automaton. The identified eye saccades were assigned to six low-level commands for navigation purposes. An experiment study was conducted in order to check the accuracy and the performances of the proposed interface compared with three traditional input control interfaces. Experimental results show that the developed interface has good performance and can be used for online communication and control in EOG-based HCI systems or even for first-person navigation metaphors in games industry. {\textcopyright} 2012 Elsevier Ltd. All rights reserved.},
author = {Postelnicu, Cristian Cezar and Girbacia, Florin and Talaba, Doru},
doi = {10.1016/j.eswa.2012.03.007},
file = {::},
isbn = {09574174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Deterministic finite automaton,Electrooculography,Fuzzy logic,Human computer interaction,Navigation,Virtual environment},
number = {12},
pages = {10857--10866},
publisher = {Elsevier Ltd},
title = {{EOG-based visual navigation interface development}},
url = {http://dx.doi.org/10.1016/j.eswa.2012.03.007},
volume = {39},
year = {2012}
}
@inproceedings{Trokielewicz2016,
author = {Trokielewicz, Mateusz},
booktitle = {2016 IEEE International Conference on Identity, Security and Behavior Analysis (ISBA)},
doi = {10.1109/ISBA.2016.7477233},
isbn = {978-1-4673-9727-8},
month = {feb},
pages = {1--6},
publisher = {IEEE},
title = {{Iris recognition with a database of iris images obtained in visible light using smartphone camera}},
url = {http://ieeexplore.ieee.org/document/7477233/},
year = {2016}
}
@article{Liu2013a,
author = {Liu, Jing and Sun, Zhenan and Tan, Tieniu},
doi = {10.1109/BTAS.2013.6712692},
file = {::},
isbn = {9781479905270},
journal = {IEEE 6th International Conference on Biometrics: Theory, Applications and Systems, BTAS 2013},
pages = {1--6},
title = {{Code-level information fusion of low-resolution iris image sequences for personal identification at a distance}},
year = {2013}
}
@article{BiosecID2008,
abstract = {A new multimodal biometric database, acquired in the framework of the BiosecurID project, is presented together with the description of the acquisition setup and protocol. The database includes eight unimodal biometric traits, namely: speech, iris, face (still images, videos of talking faces), handwritten signature and handwritten text (on-line dynamic signals, off-line scanned images), fingerprints (acquired with two different sensors), hand (palmprint, contour-geometry) and keystroking. The database comprises 400 subjects and presents features such as: realistic acquisition scenario, balanced gender and population distributions, availability of information about particular demographic groups (age, gender, handedness), acquisition of replay attacks for speech and keystroking, skilled forgeries for signatures, and compatibility with other existing databases. All these characteristics make it very useful in research and development of unimodal and multimodal biometric systems. {\textcopyright} Springer-Verlag London Limited 2009.},
author = {Fierrez, J. and Galbally, J. and Ortega-Garcia, J. and Freire, M. R. and Alonso-Fernandez, F. and Ramos, D. and Toledano, D. T. and Gonzalez-Rodriguez, J. and Siguenza, J. A. and Garrido-Salas, J. and Anguiano, E. and Gonzalez-de-Rivera, G. and Ribalda, R. and Faundez-Zanuy, M. and Ortega, J. A. and Carde{\~{n}}oso-Payo, V. and Viloria, A. and Vivaracho, C. E. and Moro, Q. I. and Igarza, J. J. and Sanchez, J. and Hernaez, I. and Orrite-Uru{\~{n}}uela, C. and Martinez-Contreras, F. and Gracia-Roche, J. J.},
doi = {10.1007/s10044-009-0151-4},
file = {::},
issn = {14337541},
journal = {Pattern Analysis and Applications},
keywords = {Biometrics,Database,Face,Fingerprint,Hand geometry,Handwriting,Iris,Keystroking,Multimodal,Palmprint,Signature,Speech},
number = {2},
pages = {235--246},
title = {{BiosecurID: A multimodal biometric database}},
volume = {13},
year = {2010}
}
@article{Galdi2017a,
author = {Galdi, Chiara and Dugelay, Jean Luc},
doi = {10.1109/ICPR.2016.7899626},
file = {::},
isbn = {9781509048472},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
pages = {160--164},
title = {{Fusing iris colour and texture information for fast iris recognition on mobile devices}},
year = {2017}
}
@article{Daia,
abstract = {Deep networks have shown impressive performance on many computer vision tasks. Recently, deep convolutional neural networks (CNNs) have been used to learn discrim-inative texture representations. One of the most successful approaches is Bilinear CNN model that explicitly captures the second order statistics within deep features. However, these networks cut off the first order information flow in the deep network and make gradient back-propagation dif-ficult. We propose an effective fusion architecture -FASON that combines second order information flow and first or-der information flow. Our method allows gradients to back-propagate through both flows freely and can be trained ef-fectively. We then build a multi-level deep architecture to exploit the first and second order information within dif-ferent convolutional layers. Experiments show that our method achieves improvements over state-of-the-art meth-ods on several benchmark datasets.},
author = {Dai, Xiyang and Ng, Joe Yue-hei and Davis, Larry S},
file = {::},
journal = {Cvpr2017},
pages = {7352--7360},
title = {{FASON : First and Second Order Information Fusion Network for Texture Recognition}}
}
@article{Dhillon2009,
abstract = {This paper discusses a brain-computer interface through electrooculogram (EOG) and electromyogram (EMG) signals. In situations of disease or trauma, there may be inability to communicate with others through means such as speech or typing. Eye movement tends to be one of the last remaining active muscle capabilities for people with neurodegenerative disorders, such as amyotrophic lateral sclerosis (ALS) also known as Lou Gehrig's disease. Thus, there is a need for eye movement based systems to enable communication. To meet this need, we proposed a system to accept eye-gaze controlled navigation of a particular letter and EMG based click to enter the letter. Eye -gaze direction (angle) is obtained from EOG signals and EMG signal is recorded from eyebrow muscle activity. A virtual screen keyboard may be used to examine the usability of the proposed system.},
author = {Dhillon, Hari Singh and Singla, Rajesh and Rekhi, Navleen Singh and Jha, Rameshwar},
doi = {10.1109/ICCSIT.2009.5234951},
file = {::},
isbn = {9781424445196},
journal = {Proceedings - 2009 2nd IEEE International Conference on Computer Science and Information Technology, ICCSIT 2009},
keywords = {Amyotrophic lateral sclerosis (ALS),EMG,EOG,Eye-gaze,Virtual keyboard},
pages = {259--262},
title = {{EOG and EMG based virtual keyboard: A brain-computer interface}},
year = {2009}
}
@article{Abbas2017,
abstract = {Modern biometrics delivers an enhanced level of security by means of a “proof of property”. The design and deployment of a biometric system, however, hide many pitfalls, which, when underestimated, can lead to major security weaknesses and privacy threats. Issues of concern include biometric identity theft and privacy invasion because of the strong connection between a user and his identity. This book showcases a collection of comprehensive references on the advances of biometric security technology. It compiles a total of fourteen articles, all contributed by thirty-two eminent researchers in the field, thus providing concise and accessible coverage of not only general issues, but also state-of-the-art solutions. The book is divided into five parts: (1) Biometric Template Protection, which covers cancellable biometrics and parameter management protocol;(2) Biometric Key and Encryption, focusing on biometric key generation and visual biometric cryptography;(3) Biometric Systems Analysis, dealing with biometric system security, and privacy evaluation and assessment;(4) Privacy-Enhanced Biometric Systems, covering privacy-enhanced biometric system protocol design and implementation; and(5) Other Biometric Security Technologies.The book will be of particular interest to researchers, scholars, graduate students, engineers, practitioners and developers interested in security and privacy-related issues in biometric systems. It will also be attractive to managers of various organizations with strong security needs.},
author = {Abbas, Sherif N},
doi = {10.1007/978-3-319-47301-7},
file = {::},
isbn = {978-3-319-47300-0},
title = {{Biometric Security and Privacy}},
url = {http://link.springer.com/10.1007/978-3-319-47301-7},
year = {2017}
}
@article{Furman1950,
abstract = {Axonal sprouting of excitatory neurons is frequently observed in$\backslash$ntemporal lobe epilepsy, but the extent to which inhibitory interneurons$\backslash$nundergo similar axonal reorganization remains unclear. The goal of this$\backslash$nstudy was to determine whether somatostatin (SOM)-expressing neurons in$\backslash$nstratum (s.) oriens of the hippocampus exhibit axonal sprouting beyond$\backslash$ntheir normal territory and innervate granule cells of the dentate gyrus$\backslash$nin a pilocarpine model of epilepsy. To obtain selective labeling of$\backslash$nSOM-expressing neurons in s. oriens, a Cre recombinase-dependent$\backslash$nconstruct for channelrhodopsin2 fused to enhanced yellow fluorescent$\backslash$nprotein (ChR2-eYFP) was virally delivered to this region in SOM-Cre$\backslash$nmice. In control mice, labeled axons were restricted primarily to s.$\backslash$nlacunosum-moleculare. However, in pilocarpine-treated animals, a rich$\backslash$nplexus of ChR2-eYFP-labeled fibers and boutons extended into the dentate$\backslash$nmolecular layer. Electron microscopy with immunogold labeling$\backslash$ndemonstrated labeled axon terminals that formed symmetric synapses on$\backslash$ndendritic profiles in this region, consistent with innervation of$\backslash$ngranule cells. Patterned illumination of ChR2-labeled fibers in s.$\backslash$nlacunosum-moleculare of CA1 and the dentate molecular layer elicited$\backslash$nGABAergic inhibitory responses in dentate granule cells in$\backslash$npilocarpine-treated mice but not in controls. Similar optical$\backslash$nstimulation in the dentate hilus evoked no significant responses in$\backslash$ngranule cells of either group of mice. These findings indicate that$\backslash$nunder pathological conditions, SOM/GABAergic neurons can undergo$\backslash$nsubstantial axonal reorganization beyond their normal territory and$\backslash$nestablish aberrant synaptic connections. Such reorganized circuitry$\backslash$ncould contribute to functional deficits in inhibition in epilepsy,$\backslash$ndespite the presence of numerous GABAergic terminals in the region.},
author = {Furman, Joseph M. and Wuyts, Floris L. and Siddiqui, Uzma and Shaikh, A N and Lord, Mary P. and Wright, W. D. and Colegatet, Robert L and Hoffman, James E},
doi = {10.1016/B978-1-4557-0308-1.00032-7},
edition = {6},
file = {::;::;::},
isbn = {978-1-4665-5543-3},
issn = {2278-1021},
journal = {Reports on Progress in Physics},
keywords = {adc,analogdigitalconverter,eeg,electroencefalogram,electromyalgy,electrooculography,emg,eog,rapid eye movement,rem,sem,slow eye movement},
number = {1},
pages = {4328--4330},
publisher = {Elsevier Inc.},
title = {{The investigation of eye movements}},
url = {www.ijarcce.com http://dx.doi.org/10.1016/B978-1-4557-0308-1.00032-7},
volume = {2},
year = {1950}
}
@article{DeMarsico2018,
abstract = {Mobile biometrics technologies are nowadays the new frontier for secure use of data and services, and are considered particularly important due to the massive use of handheld devices in the entire world. Among the biometric traits with potential to be used in mobile settings, the iris/ocular region is a natural candidate, even considering that further advances in the technology are required to meet the operational requirements of such ambitious environments. Aiming at promoting these advances, we organized the Mobile Iris Challenge Evaluation (MICHE)-I contest. This paper presents a comparison of the performance of the participant methods by various Figures of Merit (FoMs). A particular attention is devoted to the identification of the image covariates that are likely to cause a decrease in the performance levels of the compared algorithms. Among these factors, interoperability among different devices plays an important role. The methods (or parts of them) implemented by the analyzed approaches are classified into segmentation (S), which was the main target of MICHE-I, and recognition (R). The paper reports both the results observed for either S or R, and also for different recombinations (S+R) of such methods. Last but not least, we also present the results obtained by multi-classifier strategies.},
author = {{De Marsico}, Maria and Nappi, Michele and Narducci, Fabio and Proen{\c{c}}a, Hugo},
doi = {10.1016/j.patcog.2017.08.028},
file = {::},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Biometric algorithm fusion,Evaluation,Mobile Iris Recognition},
pages = {286--304},
publisher = {Elsevier Ltd},
title = {{Insights into the results of MICHE I - Mobile Iris CHallenge Evaluation}},
volume = {74},
year = {2018}
}
@incollection{Bowyer2016b,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
annote = {Read only chapter 17 about iris and face fusion. The article gives a nice and clean overview of different multi-biometric systems as well as levels of data abstraction the data fusion can be applied on. 

The work presented performs iris and face fusion using multi-sample, multi instance, and multimodal data. it fuses the multi sample, and multi instance together, by simply finding the best match in the variations. this is done as a score level fusion. and it fuses the two modalities by rank-level fusion using Broda count.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Connaughton, Ryan and Bowyer, Kevin W and Flynn, Patrick J},
booktitle = {Handbook of Iris Recognition},
doi = {10.1007/978-1-4471-6784-6},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {978-1-4471-6782-2},
issn = {16130073},
pages = {397--415},
pmid = {25246403},
title = {{Chapter 17 Fusion of Face and Iris Biometrics}},
url = {http://link.springer.com/10.1007/978-1-4471-6784-6},
year = {2016}
}
@article{Heide1999,
annote = {General notes:
Review of different eye tracking techniques. From 1999.

EOG:
Used most in clinical research
Pros:
- Non intrusive
- Does not limit field of view
- Can be used while the subject is wearing glasses/lenses
- Can be used with non cooperative subject
- Can be used while eye is closed ( E.G while asleep)

Cons:
- Sensitive to changes in light
- Often contaminated by electrical noise + blinks and eye lids.
- Horisontal range of +-40 deg
- Resolution of about 1-2 deg
- Vertical eye movements are often unreliable and require special configurations
- Torsinal eye movemens (around the line of sight) are not possible

VOG:
Pros:
- Non invasive
- 3D as well.
- Range of +- 40 deg horisontal and +- 30 deg vertical
- spatial resolution of about 0.5 deg

Cons:
Not really any that are relevant anymore},
author = {Heide, W and Koenig, E and Trillenberg, P and K{\"{o}}mpf, D and Zee, D S},
file = {::},
issn = {0424-8155},
journal = {Electroencephalography and clinical neurophysiology. Supplement},
pages = {223--240},
pmid = {10590990},
title = {{Electrooculography: technical standards and applications. The International Federation of Clinical Neurophysiology.}},
volume = {52},
year = {1999}
}
@article{Nam2016a,
abstract = {We propose Dual Attention Networks (DANs) which jointly leverage visual and textual attention mechanisms to capture fine-grained interplay between vision and language. DANs attend to specific regions in images and words in text through multiple steps and gather essential information from both modalities. Based on this framework, we introduce two types of DANs for multimodal reasoning and matching, respectively. The reasoning model allows visual and textual attentions to steer each other during collaborative inference, which is useful for tasks such as Visual Question Answering (VQA). In addition, the matching model exploits the two attention mechanisms to estimate the similarity between images and sentences by focusing on their shared semantics. Our extensive experiments validate the effectiveness of DANs in combining vision and language, achieving the state-of-the-art performance on public benchmarks for VQA and image-text matching.},
archivePrefix = {arXiv},
arxivId = {1611.00471},
author = {Nam, Hyeonseob and Ha, Jung-Woo and Kim, Jeonghee},
doi = {10.1109/CVPR.2017.232},
eprint = {1611.00471},
file = {::},
isbn = {978-1-5386-0457-1},
issn = {1611.00471},
pages = {299--307},
title = {{Dual Attention Networks for Multimodal Reasoning and Matching}},
url = {http://arxiv.org/abs/1611.00471},
year = {2016}
}
@article{Hqwhua,
author = {Hqwhu, E and Vndudkdq, Qvndudnrf and Jwx, Dnjxo and Wu, H G X},
file = {::},
keywords = {convolutional neural,deep learning,network,pupil center estimation},
title = {{Deep learning based estimation of the eye pupil center by using image patch classification}}
}
@inproceedings{Ghazi2016a,
abstract = {Deep learning based approaches have been dominating the face recognition field due to the significant performance improvement they have provided on the challenging wild datasets. These approaches have been extensively tested on such unconstrained datasets, on the Labeled Faces in the Wild and YouTube Faces, to name a few. However, their capability to handle individual appearance variations caused by factors such as head pose, illumination, occlusion, and misalignment has not been thoroughly assessed till now. In this paper, we present a comprehensive study to evaluate the performance of deep learning based face representation under several conditions including the varying head pose angles, upper and lower face occlusion, changing illumination of different strengths, and misalignment due to erroneous facial feature localization. Two successful and publicly available deep learning models, namely VGG-Face and Lightened CNN have been utilized to extract face representations. The obtained results show that although deep learning provides a powerful representation for face recognition, it can still benefit from preprocessing, for example, for pose and illumination normalization in order to achieve better performance under various conditions. Particularly, if these variations are not included in the dataset used to train the deep learning model, the role of preprocessing becomes more crucial. Experimental results also show that deep learning based representation is robust to misalignment and can tolerate facial feature localization errors up to 10{\%} of the interocular distance.},
author = {Ghazi, Mostafa Mehdipour and Ekenel, Hazim Kemal},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
doi = {10.1109/CVPRW.2016.20},
file = {::},
isbn = {978-1-5090-1437-8},
month = {jun},
pages = {102--109},
publisher = {IEEE},
title = {{A Comprehensive Analysis of Deep Learning Based Representation for Face Recognition}},
url = {http://ieeexplore.ieee.org/document/7789510/},
year = {2016}
}
@article{Neves2017a,
abstract = {{\textcopyright} 2017 IEEE. An error-correcting code (ECC) is a process of adding redundant data to a message, such that it can be recovered by a receiver even if a number of errors are introduced in transmission. Inspired by the principles of ECC, we introduce a method capable of detecting degraded features in biometric signatures by exploiting feature correlation. The main novelty is that, unlike existing biometric cryptosystems, the proposed method works directly on the biometric signature. Our approach performs a redundancy analysis of non-degraded data to build an undirected graphical model (Markov Random Field), whose energy minimization determines the sequence of degraded components of the biometric sample. Experiments carried out in different biometric traits ascertain the improvements attained when disregarding degraded features during the matching phase. Also, we stress that the proposed method is general enough to work in different classification methods, such as CNNs.},
author = {Neves, Joao and Proenca, Hugo},
doi = {10.1109/FG.2017.122},
file = {::},
isbn = {9781509040230},
journal = {Proceedings - 12th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2017 - 1st International Workshop on Adaptive Shot Learning for Gesture Understanding and Production, ASL4GUP 2017, Biometrics in the Wild, Bwild 2017, Heteroge},
pages = {981--986},
title = {{Exploiting Data Redundancy for Error Detection in Degraded Biometric Signatures Resulting from in the Wild Environments}},
year = {2017}
}
@article{Chen2005a,
abstract = {The recognition accuracy of a single biometric authentication system is often much reduced due to the environment, user mode and physiological defects. In this paper, we combine face and iris features for developing a multimode biometric approach, which is able to diminish the drawback of single biometric approach as well as to improve the performance of authentication system. We combine a face database ORL and iris database CASIA to construct a multimodal biometric experimental database with which we validate the proposed approach and evaluate the multimodal biometrics performance. The experimental results reveal the multimodal biometrics verification is much more reliable and precise than single biometric approach.},
annote = {Face and iris for authentication 
uses synthetic multimodal biometric dataset

Shows how to evaluate system. 

proves multimodal performs better.},
author = {Chen, Ching-Han and {Te Chu}, Chia},
doi = {10.1007/11608288_76},
file = {::},
isbn = {978-3-540-31621-3},
issn = {03029743},
keywords = {face,iris,multimodal biometrics,wavelet probabilistic neural},
pages = {571--580},
title = {{Fusion of Face and Iris Features for Multimodal Biometrics}},
url = {http://link.springer.com/10.1007/11608288{\_}76},
year = {2005}
}
@inproceedings{Sun2014,
abstract = {The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset, 99.15{\%} face verification accuracy is achieved. Compared with the best deep learning result on LFW, the error rate has been significantly reduced by 67{\%}.},
archivePrefix = {arXiv},
arxivId = {1406.4773},
author = {Sun, Yi and Wang, Xiaogang and Tang, Xiaoou},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.244},
eprint = {1406.4773},
file = {::},
isbn = {9781479951178},
issn = {10636919},
keywords = {deep learning,face verification},
pages = {1891--1898},
pmid = {21808091},
title = {{Deep Learning Face Representation by Joint Identification-Verification}},
url = {https://arxiv.org/pdf/1406.4773.pdf http://arxiv.org/abs/1406.4773},
year = {2014}
}
@article{Biosec2007,
abstract = {The baseline corpus of a new multimodal database, acquired in the framework of the FP6 EU BioSec Integrated Project, is presented. The corpus consists of fingerprint images acquired with three different sensors, frontal face images from a webcam, iris images from an iris sensor, and voice utterances acquired both with a close-talk headset and a distant webcam microphone. The BioSec baseline corpus includes real multimodal data from 200 individuals in two acquisition sessions. In this contribution, the acquisition setup and protocol are outlined, and the contents of the corpus-including data and population statistics-are described. The database will be publicly available for research purposes by mid-2006. {\textcopyright} 2006 Pattern Recognition Society.},
author = {Fierrez, Julian and Ortega-Garcia, Javier and {Torre Toledano}, Doroteo and Gonzalez-Rodriguez, Joaquin},
doi = {10.1016/j.patcog.2006.10.014},
file = {::},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Authentication,Biometrics,Database,Face,Fingerprint,Iris,Multimodal,Performance,Verification,Voice},
number = {4},
pages = {1389--1392},
title = {{Biosec baseline corpus: A multimodal biometric database}},
volume = {40},
year = {2007}
}
@incollection{Bowyer2016,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bowyer, Kevin W and Hollingsworth, Karen P and Flynn, Patrick J},
booktitle = {Handbook of Iris Recognition},
doi = {10.1007/978-1-4471-6784-6},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {978-1-4471-6782-2},
issn = {16130073},
pages = {23--61},
pmid = {25246403},
title = {{Chapter 2 A Survey of Iris Biometrics Research: 2008–2010}},
url = {http://link.springer.com/10.1007/978-1-4471-6784-6},
year = {2016}
}
@article{Wan2017a,
abstract = {State-of-the-art methods for 3D hand pose estimation from depth images require large amounts of annotated training data. We propose to model the statistical relationships of 3D hand poses and corresponding depth images using two deep generative models with a shared latent space. By design, our architecture allows for learning from unlabeled image data in a semi-supervised manner. Assuming a one-to-one mapping between a pose and a depth map, any given point in the shared latent space can be projected into both a hand pose and a corresponding depth map. Regressing the hand pose can then be done by learning a discriminator to estimate the posterior of the latent pose given some depth map. To improve generalization and to better exploit unlabeled depth maps, we jointly train a generator and a discriminator. At each iteration, the generator is updated with the back-propagated gradient from the discriminator to synthesize realistic depth maps of the articulated hand, while the discriminator benefits from an augmented training set of synthesized and unlabeled samples. The proposed discriminator network architecture is highly efficient and runs at 90 FPS on the CPU with accuracies comparable or better than state-of-art on 3 publicly available benchmarks.},
archivePrefix = {arXiv},
arxivId = {1702.03431},
author = {Wan, Chengde and Probst, Thomas and {Van Gool}, Luc and Yao, Angela},
doi = {10.1109/CVPR.2017.132},
eprint = {1702.03431},
file = {::},
journal = {Cvpr2017},
pages = {10},
title = {{Crossing Nets: Dual Generative Models with a Shared Latent Space for Hand Pose Estimation}},
url = {http://arxiv.org/abs/1702.03431},
year = {2017}
}
@article{Lv2015,
abstract = {There is an increasing interest in creating pervasive games based on emerging interaction technologies. In order to develop touch-less, interactive and augmented reality games on vision-based wearable device, a touch-less motion interaction technology is designed and evaluated in this work. Users interact with the augmented reality games with dynamic hands/feet gestures in front of the camera, which triggers the interaction event to interact with the virtual object in the scene. Three primitive augmented reality games with eleven dynamic gestures are developed based on the proposed touch-less interaction technology as proof. At last, a comparing evaluation is proposed to demonstrate the social acceptability and usability of the touch-less approach, running on a hybrid wearable framework or with Google Glass, as well as workload assessment, user's emotions and satisfaction.},
author = {Lv, Zhihan and Halawani, Alaa and Feng, Shengzhong and {Ur R{\'{e}}hman}, Shafiq and Li, Haibo},
doi = {10.1007/s00779-015-0844-1},
file = {::},
isbn = {16174909 (ISSN)},
issn = {16174909},
journal = {Personal and Ubiquitous Computing},
keywords = {Augmented reality game,Hand free,Pervasive game,Smartphone game,Touch-less,Wearable device},
number = {3},
pages = {551--567},
title = {{Touch-less interactive augmented reality game on vision-based wearable device}},
volume = {19},
year = {2015}
}
@article{B2017,
author = {B, Qi Wang and Su, Xia and Cai, Zhenlin and Zhang, Xiangde},
doi = {10.1007/978-3-319-69923-3},
file = {::},
isbn = {978-3-319-69922-6},
keywords = {joint bayesian,mobile iris recognition,ordinal measures},
number = {3},
pages = {401--410},
title = {{Biometric Recognition}},
url = {http://link.springer.com/10.1007/978-3-319-69923-3},
volume = {10568},
year = {2017}
}
@article{Mhaske2013a,
author = {Mhaske, V. D. and Patankar, A. J.},
doi = {10.1109/ICCIC.2013.6724125},
file = {::},
isbn = {9781479915972},
journal = {2013 IEEE International Conference on Computational Intelligence and Computing Research, IEEE ICCIC 2013},
keywords = {Biometrics,Fingerprint,Fusion,MGF,Multimodal,Palm print,ROI,Unimodal},
title = {{Multimodal biometrics by integrating fingerprint and palmprint for security}},
year = {2013}
}
@article{Cheng2017a,
abstract = {Sea–land segmentation and ship detection are two prevalent research domains for optical remote sensing harbor images and can find many applications in harbor supervision and management. As the spatial resolution of imaging technology improves, traditional methods struggle to perform well due to the complicated appearance and background distributions. In this paper, we unify the above two tasks into a single framework and apply the deep convolutional neural networks to predict pixelwise label for an input. Specifically, an edge aware convolutional network is proposed to parse a remote sensing harbor image into three typical objects, e.g., sea, land, and ship. Two innovations are made on top of the deep structure. First, we design a multitask model by simultaneously training the segmentation and edge detection networks. Hierarchical semantic features from the segmentation network are extracted to learn the edge network. Second, the outputs of edge pipeline are further employed to refine entire model by adding an edge aware regularization, which helps our method to yield very desirable results that are spatially consistent and well boundary located. It also benefits the segmentation of docked ships that are quite challenging for many previous methods. Experimental results on two datasets collected from Google Earth have demonstrated the effectiveness of our approach both in quantitative and qualitative performance compared with state-of-the-art methods.},
author = {Cheng, Dongcai and Meng, Gaofeng and Xiang, Shiming and Pan, Chunhong},
doi = {10.1109/JSTARS.2017.2747599},
file = {::},
issn = {21511535},
journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
keywords = {Edge aware regularization,harbor images,multitask learning,semantic segmentation},
number = {12},
pages = {5769--5783},
title = {{FusionNet: Edge Aware Deep Convolutional Networks for Semantic Segmentation of Remote Sensing Harbor Images}},
volume = {10},
year = {2017}
}
@article{Fierrez2018c,
abstract = {The present paper is Part 2 in this series of two papers. In Part 1 we provided an introduction to Multiple Classifier Systems (MCS) with a focus into the fundamentals: basic nomenclature, key elements, architecture, main methods, and prevalent theory and framework. Part 1 then overviewed the application of MCS to the particular field of multimodal biometric person authentication in the last 25 years, as a prototypical area in which MCS has resulted in important achievements. Here in Part 2 we present in more technical detail recent trends and developments in MCS coming from multimodal biometrics that incorporate context information in an adaptive way. These new MCS architectures exploit input quality measures and pattern-specific particularities that move apart from general population statistics, resulting in robust multimodal biometric systems. Similarly as in Part 1, methods here are described in a general way so they can be applied to other information fusion problems as well. Finally, we also discuss here open challenges in biometrics in which MCS can play a key role.},
author = {Fierrez, Julian and Morales, Aythami and Vera-Rodriguez, Ruben and Camacho, David},
doi = {10.1016/j.inffus.2017.12.005},
file = {::},
issn = {15662535},
journal = {Information Fusion},
keywords = {Adaptive,Biometrics,Classifier,Context,Fusion,Multimodal},
number = {November 2017},
pages = {103--112},
publisher = {Elsevier},
title = {{Multiple classifiers in biometrics. Part 2: Trends and challenges}},
url = {https://doi.org/10.1016/j.inffus.2017.12.005},
volume = {44},
year = {2018}
}
@article{Ortega-Garcia2010,
abstract = {A new multimodal biometric database designed and acquired within the framework of the European BioSecure Network of Excellence is presented. It is comprised of more than 600 individuals acquired simultaneously in three scenarios: 1) over the Internet, 2) in an office environment with desktop PC, and 3) in indoor/outdoor environments with mobile portable hardware. The three scenarios include a common part of audio/video data. Also, signature and fingerprint data have been acquired both with desktop PC and mobile portable hardware. Additionally, hand and iris data were acquired in the second scenario using desktop PC. Acquisition has been conducted by 11 European institutions. Additional features of the BioSecure Multimodal Database (BMDB) are: two acquisition sessions, several sensors in certain modalities, balanced gender and age distributions, multimodal realistic scenarios with simple and quick tasks per modality, cross-European diversity, availability of demographic data, and compatibility with other multimodal databases. The novel acquisition conditions of the BMDB allow us to perform new challenging research and evaluation of either monomodal or multimodal biometric systems, as in the recent BioSecure Multimodal Evaluation campaign. A description of this campaign including baseline results of individual modalities from the new database is also given. The database is expected to be available for research purposes through the BioSecure Association during 2008.},
author = {Ortega-Garcia, Javier and Fierrez, Julian and Alonso-Fernandez, Fernando and Galbally, Javier and Freire, Manuel R. and Gonzalez-Rodriguez, Joaquin and Garcia-Mateo, Carmen and Alba-Castro, Jose Luis and Gonzalez-Agulla, Elisardo and Otero-Muras, Enrique and Garcia-Salicetti, Sonia and Allano, Lorene and Ly-Van, Bao and Dorizzi, Bernadette and Kittler, Josef and Bourlai, Thirimachos and Poh, Norman and Deravi, Farzin and Ng, Ming N.R. and Fairhurst, Michael and Hennebert, Jean and Humm, Andreas and Tistarelli, Massimo and Brodo, Linda and Richiardi, Jonas and Drygajlo, Andrezj and Ganster, Harald and Sukno, Federico M. and Pavani, Sri Kaushik and Frangi, Alejandro and Akarun, Lale and Savran, Arman},
doi = {10.1109/TPAMI.2009.76},
file = {::},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Benchmark,Biometrics,Database,Evaluation,Face,Fingerprint,Hand,Iris.,Multimodal,Performance,Signature,Speaker,Voice},
number = {6},
pages = {1097--1111},
pmid = {20431134},
title = {{The multiscenario multienvironment biosecure multimodal database (BMDB)}},
volume = {32},
year = {2010}
}
@article{Phillips2009,
abstract = {The goal of the Multiple Biometrics Grand Challenge (MBGC) is to improve the performance of face and iris recognition technology from biometric samples acquired under unconstrained conditions. The MBGC is organized into three challenge problems. Each challenge problem re- laxes the acquisition constraints in different directions. In the Portal Challenge Problem, the goal is to recognize people from near-infrared (NIR) and high definition (HD) video as they walk through a portal. Iris recognition can be performed from the NIR video and face recognition from the HD video. The availability of NIR and HD modalities allows for the development of fusion algorithms. The Still Face Challenge Problem has two primary goals. The first is to improve recognition performance from frontal and off angle still face images taken under uncontrolled in- door and outdoor lighting. The second is to improve recognition perfor- mance on still frontal face images that have been resized and compressed, as is required for electronic passports. In the Video Challenge Problem, the goal is to recognize people from video in unconstrained environments. The video is unconstrained in pose, illumination, and camera angle. All three challenge problems include a large data set, experiment descrip- tions, ground truth, and scoring code.},
author = {Phillips, P. Jonathon and Flynn, Patrick J. and Beveridge, J. Ross and Scruggs, W. Todd and O'Toole, Alice J. and Bolme, David and Bowyer, Kevin W. and Draper, Bruce A. and Givens, Geof H. and Lui, Yui Man and Sahibzada, Hassan and Scallan, Joseph A. and Weimer, Samuel},
doi = {10.1007/978-3-642-01793-3_72},
file = {::},
isbn = {3642017924},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {705--714},
title = {{Overview of the multiple biometrics grand challenge}},
volume = {5558 LNCS},
year = {2009}
}
@article{sun2015,
abstract = {The state-of-the-art of face recognition has been significantly advanced by the emergence of deep learning. Very deep neural networks recently achieved great success on general object recognition because of their superb learning capacity. This motivates us to investigate their effectiveness on face recognition. This paper proposes two very deep neural network architectures, referred to as DeepID3, for face recognition. These two architectures are rebuilt from stacked convolution and inception layers proposed in VGG net and GoogLeNet to make them suitable to face recognition. Joint face identification-verification supervisory signals are added to both intermediate and final feature extraction layers during training. An ensemble of the proposed two architectures achieves 99.53{\%} LFW face verification accuracy and 96.0{\%} LFW rank-1 face identification accuracy, respectively. A further discussion of LFW face verification result is given in the end.},
archivePrefix = {arXiv},
arxivId = {1502.00873},
author = {Sun, Yi and Liang, Ding and Wang, Xiaogang and Tang, Xiaoou},
eprint = {1502.00873},
file = {::},
title = {{DeepID3: Face Recognition with Very Deep Neural Networks}},
url = {https://arxiv.org/pdf/1502.00873.pdf http://arxiv.org/abs/1502.00873},
year = {2015}
}
@article{Fierrez-Aguilar2003a,
abstract = {The aim of this paper, regarding multimodal biometric verification, is twofold: on the one hand, some score fusion strategies reported in the literature are reviewed and, on the other hand, we compare experimentally a selection of them using as monomodal baseline experts: i) our face verification system based on a global face appearance representation scheme, ii) our minutiaebased fingerprint verification system, and iii) our on-line signature verification system based on HMM modeling of temporal functions, on the MCYT multimodal database. A new strategy is also proposed and discussed in order to generate a multimodal combined score by means of Support Vector Machine (SVM) classifiers from which user-independent and user-dependent fusion schemes are derived and evaluated.},
author = {Fierrez-Aguilar, J and Ortega-Garcia, J and Garcia-Romero, D and Gonzalez-Rodriguez, J},
file = {::},
isbn = {3-540-40302-7},
issn = {03029743},
journal = {Proc. AVBPA},
pages = {1056},
title = {{A Comparative Evaluation of Fusion Strategies for Multimodal Biometric Verification}},
year = {2003}
}
@inproceedings{Eitel2015,
abstract = {Robust object recognition is a crucial ingredient of many, if not all, real-world robotics applications. This paper leverages recent progress on Convolutional Neural Networks (CNNs) and proposes a novel RGB-D architecture for object recognition. Our architecture is composed of two separate CNN processing streams - one for each modality - which are consecutively combined with a late fusion network. We focus on learning with imperfect sensor data, a typical problem in real-world robotics tasks. For accurate learning, we introduce a multi-stage training methodology and two crucial ingredients for handling depth data with CNNs. The first, an effective encoding of depth information for CNNs that enables learning without the need for large depth datasets. The second, a data augmentation scheme for robust learning with depth images by corrupting them with realistic noise patterns. We present state-of-the-art results on the RGB-D object dataset and show recognition in challenging RGB-D real-world noisy settings.},
archivePrefix = {arXiv},
arxivId = {1507.06821},
author = {Eitel, Andreas and Springenberg, Jost Tobias and Spinello, Luciano and Riedmiller, Martin and Burgard, Wolfram},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2015.7353446},
eprint = {1507.06821},
file = {::},
isbn = {9781479999941},
issn = {21530866},
month = {jul},
pages = {681--687},
title = {{Multimodal deep learning for robust RGB-D object recognition}},
url = {http://arxiv.org/abs/1507.06821},
volume = {2015-Decem},
year = {2015}
}
@inproceedings{Yin2011,
abstract = {In this paper, the acquisition and content of a new homologous multimodal biometric database are presented. The SDUMLA-HMT database consists of face images from 7 view angles, finger vein images of 6 fingers, gait videos from 6 view angles, iris images from an iris sensor, and fingerprint images acquired with 5 different sensors. The database includes real multimodal data from 106 individuals. In addition to database description, we also present possible use of the database. The database is available to research community through http://mla.sdu.edu.cn/sdumla-hmt.html .},
author = {Yin, Yilong and Liu, Lili and Sun, Xiwei},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-25449-9_33},
file = {::},
isbn = {9783642254482},
issn = {03029743},
keywords = {Biometrics,Face,Finger vein,Fingerprint,Gait,Homologous,Iris,Multi-modal},
pages = {260--268},
title = {{SDUMLA-HMT: A multimodal biometric database}},
volume = {7098 LNCS},
year = {2011}
}
@article{Schiel2002,
abstract = {In this contribution we announce and describe in detail the new multimodal corpus evolving from the publicly funded German SmartKom project. The first release of the corpus (BAS SK-P 1.0) has been finished end of 2001 and will be ready for distribution to the scientific community in July 2002. The SmartKom corpus will be the first of a new generation of Language Resources (LR) designed for a more or less complete data gathering of human-machine communication combining acoustic, visual and tactile input and output modalities. Since the funding of about EU 2 Mio for this LR is 100{\%} public, the corpus will be available without royalties via the Bavarian Archive for Speech Signals (BAS) at the University of Munich.},
author = {Schiel, Florian and Steininger, Silke and T{\"{u}}rk, Ulrich},
file = {::},
journal = {Proceedings of the 3rd Language Resources and Evaluation},
number = {34},
pages = {200--2006},
title = {{The SmartKom Multimodal Corpus at BAS}},
year = {2002}
}
@article{Ross2003,
annote = {Nice and clear overview of the four basic modules of the standard biometric system.

Very clear explainations of different aspects, but a bit limited in respect to the separation of methods into categories .},
author = {Ross, Arun and Jain, Anil},
file = {::},
isbn = {1517355931},
keywords = {biometrics,decision tree,face,fingerprints,hand geometry,linear discriminant analysis,multimodal,sum rule,verification},
number = {13},
pages = {2115--2125},
title = {{Information Fusion in Biometrics}},
volume = {24},
year = {2003}
}
@article{Ma2015,
abstract = {This study presents a novel human-machine interface (HMI) based on both electrooculography (EOG) and electroencephalography (EEG). This hybrid interface works in two modes: an EOG mode recognizes eye movements such as blinks, and an EEG mode detects event related potentials (ERPs) like P300. While both eye movements and ERPs have been separately used for implementing assistive interfaces, which help patients with motor disabilities in performing daily tasks, the proposed hybrid interface integrates them together. In this way, both the eye movements and ERPs complement each other. Therefore, it can provide a better efficiency and a wider scope of application. In this study, we design a threshold algorithm that can recognize four kinds of eye movements including blink, wink, gaze, and frown. In addition, an oddball paradigm with stimuli of inverted faces is used to evoke multiple ERP components including P300, N170, and VPP. To verify the effectiveness of the proposed system, two different online experiments are carried out. One is to control a multifunctional humanoid robot, and the other is to control four mobile robots. In both experiments, the subjects can complete tasks effectively by using the proposed interface, whereas the best completion time is relatively short and very close to the one operated by hand.},
author = {Ma, Jiaxin and Zhang, Yu and Cichocki, Andrzej and Matsuno, Fumitoshi},
doi = {10.1109/TBME.2014.2369483},
file = {::},
isbn = {0018-9294},
issn = {15582531},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {Electroencephalogram (EEG),Electrooculogram (EOG),event-related potential (ERP),human-machine interface (HMI),robot control},
number = {3},
pages = {876--889},
pmid = {25398172},
title = {{A novel EOG/EEG hybrid human-machine interface adopting eye movements and ERPs: Application to robot control}},
volume = {62},
year = {2015}
}
@article{Rattani2017,
abstract = {Ocular biometrics encompasses the imaging and use of characteristic features extracted from the eyes for personal recognition. Ocular biometric modalities in visible light have mainly focused on iris, blood vessel structures over the white of the eye (mostly due to conjunctival and episcleral layers), and periocular region around eye. Most of the existing studies on iris recognition use the near infrared spectrum. However, conjunctival vasculature and periocular regions are imaged in the visible spectrum. Iris recognition in the visible spectrum is possible for light color irides or by utilizing special illumination. Ocular recognition in the visible spectrum is an important research area due to factors such as recognition at a distance, suitability for recognition with regular RGB cameras, and adaptability to mobile devices. Further these ocular modalities can be obtained from a single RGB eye image, and then fused together for enhanced performance of the system. Despite these advantages, the state-of-the-art related to ocular biometrics in visible spectrum is not well known. This paper surveys this topic in terms of computational image enhancement, feature extraction, classification schemes and designed hardware-based acquisition set-ups. Future research directions are also enumerated to identify the path forward.},
annote = {A very detalied survey article. They go through a lot of different approaches to iris recognition. They also talk about the other biometrics that can be extracted from a VIS (Visible Spectrum) image like, moles/freckles/nevi. Other patterns can also b extraxted such as conjunctival vaslulature, periocular and retinal biometrics.

UBIRIS is one of the most used publicaly available databses for VIS iris images with noise.},
author = {Rattani, Ajita and Derakhshani, Reza},
doi = {10.1016/j.imavis.2016.11.019},
file = {::},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Biometrics,Conjunctival vasculature,Iris,Mobile biometrics,Ocular biometrics,Periocular biometrics,Visible spectrum},
pages = {1--16},
publisher = {Elsevier B.V.},
title = {{Ocular biometrics in the visible spectrum: A survey}},
url = {http://dx.doi.org/10.1016/j.imavis.2016.11.019},
volume = {59},
year = {2017}
}
@article{Kim2016,
abstract = {The iris recognition on a mobile phone is different from the conventional iris recognition implemented on a dedicated device in that the computational power of a mobile phone and the space for placing NIR (near infrared) LED (light emitting diode) illuminators and iris camera are limited. This paper raises these issues in detail based on real implementation of an iris recognition system in a mobile phone and proposes some solutions to these issues. An experimental study was conducted to search for the relevant power and wavelength of NIR LED illuminators with their positioning on a phone for capturing a good quality iris image. Subsequently, in view of the disparity between the user's gazing point and the center of the iris camera which causes degradation of acquired iris images, an experiment was performed to locate the appropriate gazing point for good iris image capture. A fast eye detection algorithm was proposed for implementation under the mobile platform with low computational facility. The experiments were conducted on a currently released mobile phone and the results showed promising potential for adoption of iris recognition as a reliable authentication means. As a result, two 850 nm LEDs were selected for iris illumination at 1.1 cm away from the iris camera for the size of a 7 cm × 13.7 cm phone. In the performance, the recognition accuracy was 0.1{\%} EER (equal error rate) and the eye detection rate with the speed of 17.64 ms on a mobile phone was 99.4{\%}.},
annote = {Keynotes:
Contributes with a good NIR mobile algorithm that's fast on mobile phones. They also contribute with an mobile iris database of 500 images that they are willing to share for research. Could not find it online.},
author = {Kim, Dongik and Jung, Yujin and Toh, Kar Ann and Son, Byungjun and Kim, Jaihie},
doi = {10.1016/j.eswa.2016.01.050},
file = {::},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Handheld,Iris recognition,Mobile,Portable,Smartphone},
pages = {328--339},
publisher = {Elsevier Ltd},
title = {{An empirical study on iris recognition in a mobile phone}},
url = {http://dx.doi.org/10.1016/j.eswa.2016.01.050},
volume = {54},
year = {2016}
}
@article{Soviany2016,
abstract = {– This paper presents a design approach of a reliable authentication system for mobile applications (such as those within m-Health or m-Banking areas). This means that the biometric data processing should optimize the security performance vs. the computational complexity. The security is given by the combination of fingerprint, iris and voice features that define the multimodal pattern of an individual. The complexity reduction is supported by a reduced feature space, especially for the fingerprint and iris recognition components of the overall system.},
author = {Soviany, Sorin and Săndulescu, Virginia and Puşcoci, Sorin},
file = {::},
isbn = {9781509020478},
journal = {Computers and Artificial Intelligence},
keywords = {-multimodal,biometrics,data fusion,identification},
title = {{A Multimodal Biometric Identification Method for Mobile Applications Security}},
volume = {30},
year = {2016}
}
@article{Chowdhury2016a,
author = {Chowdhury, Anurag and Ghosh, Soumyadeep and Singh, Richa and Vatsa, Mayank},
doi = {10.1109/BTAS.2016.7791199},
file = {::},
isbn = {9781467397339},
journal = {2016 IEEE 8th International Conference on Biometrics Theory, Applications and Systems, BTAS 2016},
title = {{RGB-D face recognition via learning-based reconstruction}},
year = {2016}
}
@article{Uka2017a,
annote = {Keynotes:

CASIA (Most used database) and IIT Delhi Iris Database used.

Hough Transofrm algorithm used to detect boundaries},
author = {Uka, Arban and Ro{\c{c}}i, Albana and Ko{\c{c}}, Oktay},
file = {::},
isbn = {9781509038435},
journal = {IEEE EUROCON 2017 -17th International Conference on Smart Technologies},
keywords = {encoding,equal error rate,segmentation},
number = {July},
pages = {6--8},
title = {{Improved Segmentation Algorithm and Further Optimization for Iris Recognition}},
year = {2017}
}
@article{Hakkinen1993,
abstract = {The primary aim of the study was to determine the best electrode positions for EOG signals in vigilance studies. Two-channel recordings were conducted in analogy to the Rechtschaffen and Kales (1968) system. Twenty electrodes (10 electrode pairs) were compared. Both EOG amplitudes and amplitude asymmetries within an electrode pair were studied. The amplitude of the EOG signal is sensitive to relatively small differences in electrode position. This concerns especially distance from the eye, the direction of eye movement and the effect of the upper eye lid movement. Larger and more symmetrical EOG amplitudes were obtained for different eye movements by placing the electrodes more medially than in the conventionally used system. EOG asymmetry in different electrode positions was dependent on the eye movement direction and even on the starting and end points of a movement with equal angular degrees. Most of the data could be explained by a simple monopolar model when combined with the effects of the upper eye lid movements. The most unexpected finding was that the EOG amplitudes of the horizontal and oblique eye movements were significantly larger when the eye were moving towards an electrode than when they were moving to the opposite direction. {\textcopyright} 1993.},
author = {H{\"{a}}kkinen, V. and Hirvonen, K. and Hasan, J. and Kataja, M. and V{\"{a}}rri, A. and Loula, P. and Eskola, H.},
doi = {10.1016/0013-4694(93)90111-8},
file = {::},
issn = {00134694},
journal = {Electroencephalography and Clinical Neurophysiology},
keywords = {EOG,Electrode positions,Vigilance},
number = {4},
pages = {294--300},
pmid = {7682933},
title = {{The effect of small differences in electrode position on EOG signals: application to vigilance studies}},
volume = {86},
year = {1993}
}
@misc{LiborMasek2003,
annote = {Open Source Matlab Iris Recognition system. Based on Daugmans approach.},
author = {{Libor Masek}, Peter Kovesi},
publisher = {The School of Computer Science and Software Engineering, The University of Western Australia},
title = {{MATLAB Source Code for a Biometric Identification System Based on Iris Patterns}},
url = {http://www.peterkovesi.com/studentprojects/libor/sourcecode.html},
year = {2003}
}
@article{Vivek2012a,
annote = {Score level fusion?},
author = {Vivek, S. Arun and Aravinth, J. and Valarmathy, S.},
doi = {10.1109/ICPRIME.2012.6208377},
file = {::},
isbn = {978-1-4673-1039-0},
journal = {International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME-2012)},
keywords = {density based score level,error,feature extraction,fusion,gmm,likelihood ratio test,multimodal biometrics,rates,template,unimodal biometrics},
pages = {387--392},
title = {{Feature extraction for multimodal biometric and study of fusion using Gaussian mixture model}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6208377},
year = {2012}
}
@article{Krizhevsky2017,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0 {\%} which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2 {\%} achieved by the second-best entry. 1},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
doi = {10.1145/3065386},
eprint = {1102.0183},
isbn = {9781627480031},
issn = {00010782},
journal = {Communications of the ACM},
month = {may},
number = {6},
pages = {84--90},
pmid = {7491034},
publisher = {ACM},
title = {{ImageNet classification with deep convolutional neural networks}},
url = {http://dl.acm.org/citation.cfm?doid=3098997.3065386},
volume = {60},
year = {2017}
}
@article{Kim2013,
author = {Kim, Myoung Ro and Yoon, Gilwon},
file = {::},
journal = {International Journal of Electrical, Computer, Energetic, Electronic and Communication Engineering},
keywords = {ddr game,eog,eye movement},
number = {10},
pages = {1352--1355},
title = {{Control Signal from EOG Analysis and Its Application}},
volume = {7},
year = {2013}
}
@article{Johnson2010a,
abstract = {Identification of individuals using biometric information has found great success in many security and law enforcement applications. Up until the present time, most research in the field has been focused on ideal conditions and most available databases are constructed in these ideal conditions. There has been a growing interest in the perfection of these technologies at a distance and in less than ideal conditions, i.e. low lighting, out-of-focus blur, off angles, etc. This paper presents a dataset consisting of face and iris videos obtained at distances of 5 to 25 feet and in conditions of varying quality. The purpose of this database is to set a standard for quality measurement in face and iris data and to provide a means for analyzing biométrie systems in less than ideal conditions. The structure of the dataset as well as a quantified metric for quality measurement based on a 25 subject subset of the dataset is presented.},
author = {Johnson, P. A. and Lopez-Meyer, P. and Sazonova, N. and Hua, F. and Schuckers, S.},
doi = {10.1109/BTAS.2010.5634513},
file = {::},
isbn = {9781424475803},
journal = {IEEE 4th International Conference on Biometrics: Theory, Applications and Systems, BTAS 2010},
title = {{Quality in face and Iris research ensemble (Q-FIRE)}},
year = {2010}
}
@article{Ribeiro2017a,
author = {Ribeiro, Eduardo and Uhl, Andreas and Alonso-Fernandez, Fernando and Farrugia, Reuben A.},
doi = {10.23919/EUSIPCO.2017.8081595},
file = {::},
isbn = {978-0-9928626-7-1},
journal = {2017 25th European Signal Processing Conference (EUSIPCO)},
pages = {2176--2180},
title = {{Exploring deep learning image super-resolution for iris recognition}},
url = {http://ieeexplore.ieee.org/document/8081595/},
volume = {2},
year = {2017}
}
@article{Al-Waisy2017,
author = {Al-Waisy, Alaa S. and Qahwaji, Rami and Ipson, Stanley and Al-Fahdawi, Shumoos and Nagem, Tarek A.M.},
doi = {10.1007/s10044-017-0656-1},
file = {::},
isbn = {0123456789},
issn = {14337541},
journal = {Pattern Analysis and Applications},
keywords = {AdaGrad method,Convolutional Neural Network,Deep learning,Iris recognition,Multimodal biometric systems,Softmax classifier},
number = {0123456789},
pages = {1--20},
publisher = {Springer London},
title = {{A multi-biometric iris recognition system based on a deep learning approach}},
url = {https://doi.org/10.1007/s10044-017-0656-1},
year = {2017}
}
@article{Luhadiya2017a,
annote = {Really good article with a summery of Iris recognition history and approaches in the introduction part.


Keynotes:

Iris database called CASIA with 756 images of 108 people.

SVM used to classify irises.

Elman Recurrent Neural Netowrk used.},
author = {Luhadiya, Ruchi and Khedkar, Anagha},
doi = {10.1109/ICAECCT.2016.7942619},
file = {::},
isbn = {9781509036622},
journal = {2016 IEEE International Conference on Advances in Electronics, Communication and Computer Technology, ICAECCT 2016},
keywords = {GLCM,Hough circular transform,Iris,Machine learning,Person identification,SVM},
pages = {387--392},
title = {{Iris detection for person identification using multiclass SVM}},
year = {2017}
}
@article{Rifaee2017,
author = {Rifaee, Mustafa and Abdallah, Mohammad and Okosh, Basem},
file = {::},
journal = {international journal of multimedia {\&} its applications},
number = {April},
title = {{A Short Survey for Iris Images Databases}},
url = {https://www.researchgate.net/publication/316093004{\_}A{\_}Short{\_}Survey{\_}for{\_}Iris{\_}Images{\_}Databases},
year = {2017}
}
@article{Zhang2017a,
abstract = {Multimodal classification arises in many computer vi-sion tasks such as object classification and image retrieval. The idea is to utilize multiple sources (modalities) measur-ing the same instance to improve the overall performance compared to using a single source (modality). The varying characteristics exhibited by multiple modalities make it nec-essary to simultaneously learn the corresponding distance metrics. In this paper, we propose a multiple metrics learn-ing algorithm for multimodal data. Metric of each modal-ity is product of two matrices: one matrix is modality spe-cific, the other is enforced to be shared by all the modalities. The learned metrics can improve multimodal classification accuracy and experimental results on four datasets show that the proposed algorithm outperforms existing learning algorithms based on multiple metrics as well as other ap-proaches tested on these datasets. Specifically, we report 95.0{\%} object instance recognition accuracy, 89.2{\%} object category recognition accuracy on the multi-view RGB-D dataset and 52.3{\%} scene category recognition accuracy on SUN RGB-D dataset.},
author = {Zhang, Heng and Patel, Vishal M. and Chellappa, Rama},
doi = {10.1109/CVPR.2017.312},
file = {::},
isbn = {978-1-5386-0457-1},
issn = {1063-6919},
journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {2925--2933},
title = {{Hierarchical Multimodal Metric Learning for Multimodal Classification}},
url = {http://ieeexplore.ieee.org/document/8099795/},
year = {2017}
}
@article{Bazrafkan2017,
abstract = {With the increasing imaging and processing capabilities of today's mobile devices, user authentication using iris biometrics has become feasible. However, as the acquisition conditions become more unconstrained and as image quality is typically lower than dedicated iris acquisition systems, the accurate segmentation of iris regions is crucial for these devices. In this work, an end to end Fully Convolutional Deep Neural Network (FCDNN) design is proposed to perform the iris segmentation task for lower-quality iris images. The network design process is explained in detail, and the resulting network is trained and tuned using several large public iris datasets. A set of methods to generate and augment suitable lower quality iris images from the high-quality public databases are provided. The network is trained on Near InfraRed (NIR) images initially and later tuned on additional datasets derived from visible images. Comprehensive inter-database comparisons are provided together with results from a selection of experiments detailing the effects of different tunings of the network. Finally, the proposed model is compared with SegNet-basic, and a near-optimal tuning of the network is compared to a selection of other state-of-art iris segmentation algorithms. The results show very promising performance from the optimized Deep Neural Networks design when compared with state-of-art techniques applied to the same lower quality datasets.},
archivePrefix = {arXiv},
arxivId = {1712.02877},
author = {Bazrafkan, Shabab and Thavalengal, Shejin and Corcoran, Peter},
eprint = {1712.02877},
file = {::},
month = {dec},
title = {{An End to End Deep Neural Network for Iris Segmentation in Unconstraint Scenarios}},
url = {http://arxiv.org/abs/1712.02877},
year = {2017}
}
@article{Li2017,
abstract = {This paper investigates how to integrate the complementary information from RGB and thermal (RGB-T) sources for object tracking. We propose a novel Convolutional Neural Network (ConvNet) architecture, including a two-stream ConvNet and a FusionNet, to achieve adaptive fusion of different source data for robust RGB-T tracking. Both RGB and thermal streams extract generic semantic information of the target object. In particular, the thermal stream is pre-trained on the ImageNet dataset to encode rich semantic information, and then fine-tuned using thermal images to capture the specific properties of thermal information. For adaptive fusion of different modalities while avoiding redundant noises, the FusionNet is employed to select most discriminative feature maps from the outputs of the two-stream ConvNet, and updated online to adapt to appearance variations of the target object. Finally, the object locations are efficiently predicted by applying the multi-channel correlation filter on the fused feature maps. Extensive experiments on the recently public benchmark GTOT verify the effectiveness of the proposed approach against other state-of-the-art RGB-T trackers.},
author = {Li, Chenglong and Wu, Xiaohao and Zhao, Nan and Cao, Xiaochun and Tang, Jin},
doi = {10.1016/j.neucom.2017.11.068},
file = {::},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Adaptive fusion,Convolutional neural network,Correlation filter,Object tracking,Thermal information},
pages = {78--85},
publisher = {Elsevier B.V.},
title = {{Fusing two-stream convolutional neural networks for RGB-T object tracking}},
url = {https://doi.org/10.1016/j.neucom.2017.11.068},
volume = {281},
year = {2017}
}
@article{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
file = {::},
isbn = {9781450341448},
issn = {09505849},
journal = {International Conference on Learning Representations (ICRL)},
month = {sep},
pages = {1--14},
pmid = {16873662},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2015}
}
@article{Kauba2016,
abstract = {—Authentication based on vein patterns is a very promising biometric technique. The most important step is the accurate extraction of the vein pattern from sometimes low quality input images. A single feature extraction technique may fail to correctly extract the vein pattern, entailing bad recognition performance. One of the solutions that can be used to improve recognition results is biometric fusion. A possible fusion strategy is feature level fusion, that is the fusion of several feature extractors' outputs. In our work, we exploited the feature level fusion to improve the quality of the extracted vein patterns and thus the feature extraction accuracy. An experimental study involving different feature extraction techniques (maximum curvature, repeated line tracking, wide line detector, ...) and different fusion techniques (majority voting, weighted average, STAPLE, ...) is conducted on the UTFVP finger-vein data set. The results show that feature level fusion is able to improve the recognition accuracy in terms of the EER over the single feature extraction techniques.},
author = {Kauba, Christof and Uhl, Andreas and Piciucco, Emanuela and Maiorana, Emanuele and Campisi, Patrizio},
doi = {10.1109/BIOSIG.2016.7736908},
file = {::},
isbn = {9783885796541},
issn = {16175468},
journal = {Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)},
title = {{Advanced variants of feature level fusion for finger vein recognition}},
volume = {P-260},
year = {2016}
}
@article{Ma2015,
abstract = {This study presents a novel human-machine interface (HMI) based on both electrooculography (EOG) and electroencephalography (EEG). This hybrid interface works in two modes: an EOG mode recognizes eye movements such as blinks, and an EEG mode detects event related potentials (ERPs) like P300. While both eye movements and ERPs have been separately used for implementing assistive interfaces, which help patients with motor disabilities in performing daily tasks, the proposed hybrid interface integrates them together. In this way, both the eye movements and ERPs complement each other. Therefore, it can provide a better efficiency and a wider scope of application. In this study, we design a threshold algorithm that can recognize four kinds of eye movements including blink, wink, gaze, and frown. In addition, an oddball paradigm with stimuli of inverted faces is used to evoke multiple ERP components including P300, N170, and VPP. To verify the effectiveness of the proposed system, two different online experiments are carried out. One is to control a multifunctional humanoid robot, and the other is to control four mobile robots. In both experiments, the subjects can complete tasks effectively by using the proposed interface, whereas the best completion time is relatively short and very close to the one operated by hand.},
author = {Ma, Jiaxin and Zhang, Yu and Cichocki, Andrzej and Matsuno, Fumitoshi},
doi = {10.1109/TBME.2014.2369483},
file = {::},
isbn = {0018-9294},
issn = {15582531},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {Electroencephalogram (EEG),Electrooculogram (EOG),event-related potential (ERP),human-machine interface (HMI),robot control},
number = {3},
pages = {876--889},
pmid = {25398172},
title = {{A novel EOG/EEG hybrid human-machine interface adopting eye movements and ERPs: Application to robot control}},
volume = {62},
year = {2015}
}
@article{Karpathy2016a,
abstract = {These notes accompany the Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition.},
author = {Karpathy, Andrej},
journal = {Stanford University},
pages = {1--21},
title = {{Convolutional Neural Networks for Visual Recognition}},
url = {http://cs231n.github.io/classification/},
year = {2016}
}
@article{Wrobel2017a,
abstract = {{\textcopyright} 2017 IEEE. In this paper, a new approach for personal identity verification using finger knuckle images and least-square contour alignment method has been proposed. A special test rig with a digital camera was prepared for acquisition the knuckle images. Next, the obtained images of finger knuckle were subjected to image processing method in order to extract the knuckle furrows from them. The verification of person was performed by comparing the furrows on the verified and the reference knuckle images. To determine the similarity between the furrows we used the least-square contour alignment method. The usability of the proposed approach was tested experimentally. Practical experiments, conducted with our database, confirmed that results obtained are promising.},
author = {Wrobel, K. and Porwik, P. and Doroz, R. and Safaverdi, H.},
doi = {10.1109/ICBAKE.2017.8090616},
file = {::},
isbn = {9781538634004},
journal = {Proceedings of 2017 International Conference on Biometrics and Kansei Engineering, ICBAKE 2017},
keywords = {biometrics,finger knuckle images,least-square contour alignment,verification},
pages = {119--122},
title = {{Person verification based on finger knuckle images and least-squares contour alignment}},
year = {2017}
}
@article{Proenca2017a,
abstract = {The effectiveness of current iris recognition systems de-pends on the accurate segmentation and parameterisation of the iris boundaries, as failures at this point misalign the coefficients of the biometric signatures. This paper de-scribes IRINA, an algorithm for Iris Recognition that is ro-bust against INAccurately segmented samples, which makes it a good candidate to work in poor-quality data. The pro-cess is based in the concept of " corresponding " patch be-tween pairs of images, that is used to estimate the posterior probabilities that patches regard the same biological region, even in case of segmentation errors and non-linear texture deformations. Such information enables to infer a free-form deformation field (2D registration vectors) between images, whose first and second-order statistics provide effective bio-metric discriminating power. Extensive experiments were carried out in four datasets (CASIA-IrisV3-Lamp, CASIA-IrisV4-Lamp, CASIA-IrisV4-Thousand and WVU) and show that IRINA not only achieves state-of-the-art performance in good quality data, but also handles effectively severe seg-mentation errors and large differences in pupillary dilation / constriction.},
author = {Proenca, Hugo and Neves, Joao C.},
doi = {10.1109/CVPR.2017.714},
file = {::},
isbn = {978-1-5386-0457-1},
journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {6747--6756},
title = {{IRINA: Iris Recognition (Even) in Inaccurately Segmented Data}},
url = {http://ieeexplore.ieee.org/document/8100197/},
year = {2017}
}
@article{Kupfer1984,
author = {Kupfer, D J and Ulrich, R F and Coble, P A and Jarnatt, D B and Grochocinski, V and Doman, J and Matthews, G and Borbely, A A},
file = {::},
journal = {Psychiatry Res},
keywords = {ANALYSIS,Depression,Human,REM,Sleep},
pages = {335--343},
title = {{Application of automated REM and slow wave analysis: I normal and depressive subjects}},
volume = {13},
year = {1984}
}
@article{Nielsen2015,
abstract = {Neural Networks and Deep Learning is a free online book. The book will teach you about: Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data Deep learning, a powerful set of techniques for learning in neural networks Neural networks and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing. This book will teach you many of the core concepts behind neural networks and deep learning.},
author = {Nielsen, Michael},
journal = {Determination Press},
title = {{Neural Networks and Deep Learning}},
url = {http://neuralnetworksanddeeplearning.com/index.html},
year = {2015}
}
@article{Arsalan2017,
author = {Arsalan, Muhammad and Hong, Hyung Gil and Naqvi, Rizwan Ali and Lee, Min Beom and Kim, Min Cheol and Kim, Dong Seop and Kim, Chan Sik and Park, Kang Ryoung},
doi = {10.3390/sym9110263},
file = {::},
issn = {20738994},
journal = {Symmetry},
keywords = {Biometrics,Convolutional neural network (CNN),Iris recognition,Iris segmentation},
number = {11},
title = {{Deep learning-based iris segmentation for iris recognition in visible light environment}},
volume = {9},
year = {2017}
}
@article{Berg1991,
author = {Berg, P and Scherg, M},
file = {::},
journal = {Clinical Physiology and Physiological Measures},
keywords = {ERP eye ocular},
pages = {49--54},
title = {{Dipole modelling of eye activity and its application to the removal of eye artifacts from the EEG and MEG}},
volume = {12},
year = {1991}
}
@article{Ho1994,
abstract = {A multiple classifier system is a powerful solution to difficult pattern$\backslash$nrecognition problems involving large class sets and noisy input$\backslash$nbecause it allows simultaneous use of arbitrary feature descriptors$\backslash$nand classification procedures. Decisions by the classifiers can$\backslash$nbe represented as rankings of classifiers and different instances$\backslash$nof a problem. The rankings can be combined by methods that either$\backslash$nreduce or rerank a given set of classes. An intersection method$\backslash$nand union method are proposed for class set reduction. Three methods$\backslash$nbased on the highest rank, the Borda count, and logistic regression$\backslash$nare proposed for class set reranking. These methods have been tested$\backslash$nin applications of degraded machine-printed characters and works$\backslash$nfrom large lexicons, resulting in substantial improvement in overall$\backslash$ncorrectness.},
author = {Ho, Tin Kam and Hull, Jonathan J and Srihari, Sargur N},
doi = {http://dx.doi.org/10.1109/34.273716},
file = {::},
isbn = {0162-8828 VO - 16},
issn = {0162-8828},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
number = {1},
pages = {66--75},
title = {{Decision Combination in Multiple Classifier Systems}},
volume = {16},
year = {1994}
}
@article{Daugman2009a,
abstract = {This chapter explains the iris recognition algorithms and presents results of 9.1 million comparisons among eye images from trials in Britain, the USA, Japan, and Korea. The key to iris recognition is the failure of a test of statistical independence, which involves so many degrees-of-freedom that this test is virtually guaranteed to be passed whenever the phase codes for two different eyes are compared, but to be uniquely failed when any eye's phase code is compared with another version of itself. The test of statistical independence is implemented by the simple Boolean Exclusive-OR operator (XOR) applied to the 2048 bit phase vectors that encode any two iris patterns, masked (AND'ed) by both of their corresponding mask bit vectors to prevent non iris artifacts from influencing iris comparisons. The XOR operator detects disagreement between any corresponding pair of bits, while the AND operator ensures that the compared bits are both deemed to have been uncorrupted by eyelashes, eyelids, specular reflections, or other noise. The norms of the resultant bit vector and of theAND'ed mask vectors are then measured in order to compute a fractional Hamming Distance as the measure of the dissimilarity between any two irises, whose two phase code bit vectors are denoted {\{}. codeA, codeB{\}} and whose mask bit vectors are denoted {\{}. maskA, maskB{\}}. {\textcopyright} 2009 Elsevier Inc. All rights reserved.},
annote = {A chapter about how Iris Recognitions works in general. John Daugman is the creator of IrisCode, a 2D Gabor wavelet-based iris recognition algorithm that is the basis of all publicly deployed automatic iris recognition systems and which has registered more than a billion persons worldwide in government ID programs.

Keywords/phrases:
Near Infra Red (NIR) images can be used for iris capturing. 

Gabor wavelets are used for determining the inter and outer edges of an iris.

Often the iris will not be circular because an eyelid will cover it.

Hamming distance is used when comparing two irises.},
author = {Daugman, John},
doi = {10.1016/B978-0-12-374457-9.00025-1},
file = {::},
isbn = {9780123744579},
issn = {10518215},
journal = {The Essential Guide to Image Processing},
pages = {715--739},
pmid = {20810146},
title = {{How Iris Recognition Works}},
year = {2009}
}
@article{Gulmire2012,
author = {Gulmire, Kshamaraj and Ganorkar, Sanjay},
file = {::},
issn = {2278-0181},
number = {5},
pages = {1--5},
title = {{Iris Recognition Using Gabor Wavelet}},
volume = {1},
year = {2012}
}
@article{Kaur2016b,
abstract = {Face recognition is a type of biometric software application by using which, we can analyzing, identifying or verifying digital image of the person by using the feature of the face of the person that are unique characteristics of each person. These characteristics may be physical or behavioral. The physiological characteristics as like finger print, iris scan, or face etc and behavior characteristics as like hand-writing, voice, key stroke etc. Face recognition is very useful in many areas such as military, airports, universities, ATM, and banks etc, used for the security purposes. There are many techniques or algorithms that are used features extraction in face recognition. This paper make a review of some of those methods which are used for the face recognition that are Principal Component Analysis (PCA), Back Propagation Neural Networks (BPNN), Genetic Algorithm, and LDA, SVM, Independent Component Analysis(ICA). Each method has different -2 functions that are used for the face recognition. Dimensionality is reduced by using the Eigen face approach or PCA, LDA to extract the features from images. Genetic Algorithm is based on feature selection and Back propagation Neural Network (BPNN) is used for the classification of face images.},
author = {Kaur, Gurpreet and Kanwal, Navdeep},
file = {::},
isbn = {9789380544212},
journal = {International Conference on Computing for Sustainable Global Development (INDIACom)},
keywords = {BPNN,Face recognition,Features extraction,LDA,PCA},
pages = {2705--2710},
title = {{A comparative review of various approaches for feature extraction in face recognition}},
url = {http://ieeexplore.ieee.org/document/7724754/},
year = {2016}
}
@article{Kumar2016a,
abstract = {We studied the fusion of three biometric authentication modalities, namely, swiping gestures, typing patterns and the phone movement patterns observed during typing or swiping. A web browser was customized to collect the data generated from the aforementioned modalities over four to seven days in an unconstrained environment. Several features were extracted by using sliding window mechanism for each modality and analyzed by using information gain, correlation, and symmetric uncertainty. Finally, five features from windows of continuous swipes, thirty features from windows of continuously typed letters, and nine features from corresponding phone movement patterns while swiping/typing were used to build the authentication system. We evaluated the performance of each modality and their fusion over a dataset of 28 users. The feature-level fusion of swiping and the corresponding phone movement patterns achieved an authentication accuracy of 93.33{\%}, whereas, the score-level fusion of typing behaviors and the corresponding phone movement patterns achieved an authentication accuracy of 89.31{\%}. 1.},
author = {Kumar, Rajesh and Phoha, Vir V. and Serwadda, Abdul},
doi = {10.1109/BTAS.2016.7791164},
file = {::},
isbn = {9781467397339},
journal = {2016 IEEE 8th International Conference on Biometrics Theory, Applications and Systems, BTAS 2016},
title = {{Continuous authentication of smartphone users by fusing typing, swiping, and phone movement patterns}},
year = {2016}
}
@article{Khan2017a,
annote = {A good article that uses a new database containing smartphone iris images. Theses are taken in visible light. They use Daugmans approach to localize the iris, then they normalize it. They use wavelets on the image to extract the desired featues and then try to classify them using SVM (97{\%}), KNN (95.1{\%}) and LDA (94.28{\%}).

Keynotes:

A different study has used Sparse Reconstruction Classifier with K-means clustering

A different study obtained 99{\%} accuracy using SVM and Hamming Distance

They tried SVM, K-means, Linear Discrimintant in their own study as classifiers.},
author = {Khan, Fahim Faysal and Akif, Ahnaf and Haque, M A},
file = {::},
isbn = {9781538633748},
pages = {26--28},
title = {{Iris Recognition using Machine Learning from Smartphone Captured Images in Visible Light}},
year = {2017}
}
@article{Nguyen2010,
author = {Nguyen, Kien and Fookes, Clinton and Sridharan, Sridha},
doi = {10.1145/1852611.1852635},
file = {::},
isbn = {9781450301053},
journal = {Proceedings of the 2010 Symposium on Information and Communication Technology - SoICT '10},
keywords = {iris recognition,mbgc,signal-level fusion,super-resolution},
number = {November},
pages = {122},
title = {{Robust mean super-resolution for less cooperative NIR iris recognition at a distance and on the move}},
url = {http://portal.acm.org/citation.cfm?doid=1852611.1852635},
year = {2010}
}
@article{Dessimoz2007,
abstract = {The MBioID initiative has been set up to address the following germane question: What and how biometric technologies could be deployed in identity documents in the foreseeable future? This research effort proposes to look at current and future practices and systems of establishing and using biometric identity documents (IDs) and evaluate their effectiveness in large-scale developments. The first objective of the MBioID project is to present a review document establishing the current state-of-the-art related to the use of multimodal biometrics in an IDs application. This research report gives the main definitions, properties and the framework of use related to biometrics, an overview of the main standards developed in the biometric industry and standardisation organisations to ensure interoperability, as well as some of the legal framework and the issues associated to biometrics such as privacy and personal data protection. The state-of-the-art in terms of technological development is also summarised for a range of single biometric modalities (2D and 3D face, fingerprint, iris, on-line signature and speech), chosen according to ICAO recommendations and availabilities, and for various multimodal approaches. This paper gives a summary of the main elements of that report. The second objective of the MBioID project is to propose relevant acquisition and evaluation protocols for a large-scale deployment of biometric IDs. Combined with the protocols, a multimodal database will be acquired in a realistic way, in order to be as close as possible to a real biometric IDs deployment. In this paper, the issues and solutions related to the acquisition setup are briefly presented. {\textcopyright} 2006 Elsevier Ireland Ltd. All rights reserved.},
author = {Dessimoz, Damien and Richiardi, Jonas and Champod, Christophe and Drygajlo, Andrzej},
doi = {10.1016/j.forsciint.2006.06.037},
file = {::},
issn = {03790738},
journal = {Forensic Science International},
keywords = {Acquisition protocol,Biometrics,Electronic passport,Evaluation protocol,Identity documents,Multimodality},
number = {2-3},
pages = {154--159},
pmid = {16890391},
title = {{Multimodal biometrics for identity documents ({\{}A figure is presented{\}})}},
volume = {167},
year = {2007}
}
@article{Daugman1993,
author = {Daugman, J.G.},
doi = {10.1109/34.244676},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {11},
pages = {1148--1161},
title = {{High confidence visual recognition of persons by a test of statistical independence}},
url = {http://ieeexplore.ieee.org/document/244676/},
volume = {15},
year = {1993}
}
@article{Percy,
abstract = {Iris recognition system is a reliable and an accurate biometric system. Localization of the iris borders in an eye image can be considered as a vital step in the iris recognition process. There exist many algorithms to segment the iris. One of the segmentation methods, that is used in many commercial iris biometric systems is an algorithm known as a Daugman's algorithm. The aim of this thesis is to implement this algorithm using MATLAB programming environment. The implemented algorithm was tested on the eye images of different quality, such as the images with partly covered iris or low contrast images. The test results demonstrated that the Daugman's algorithm detects the iris borders in the high quality images with high accuracy. The performance of the algorithm on the lower quality images has been improved by additional preprocessing of these images.},
author = {Percy, Oad and Waqas, Ahmad},
file = {::},
journal = {Blekinge Institute of Technology},
pages = {1--48},
title = {{Iris localization using Daugman ' s algorithm}},
url = {http://www.diva-portal.org/smash/get/diva2:831173/FULLTEXT01.pdf},
year = {2012}
}
@inproceedings{Zhao2015a,
abstract = {This paper proposes a novel and more accurate iris segmentation framework to automatically segment iris region from the face images acquired with relaxed imaging under visible or near-infrared illumination, which provides strong feasibility for applications in surveillance, forensics and the search for missing children, etc. The proposed framework is built on a novel total-variation based formulation which uses l1 norm regularization to robustly suppress noisy texture pixels for the accurate iris localization. A series of novel and robust post processing operations are introduced to more accurately localize the limbic boundaries. Our experimental results on three publicly available databases, i.e., FRGC, UBIRIS.v2 and CASIA.v4-distance, achieve significant performance improvement in terms of iris segmentation accuracy over the state-of-the-art approaches in the literature. Besides, we have shown that using iris masks generated from the proposed approach helps to improve iris recognition performance as well. Unlike prior work, all the implementations in this paper are made publicly available to further advance research and applications in biometrics at-d-distance.},
author = {Zhao, Zijing and Kumar, Ajay},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.436},
file = {::;::},
isbn = {9781467383912},
issn = {15505499},
month = {dec},
pages = {3828--3836},
publisher = {IEEE},
title = {{An accurate iris segmentation framework under relaxed imaging constraints using total variation model}},
url = {http://ieeexplore.ieee.org/document/7410793/},
volume = {2015 Inter},
year = {2015}
}
@article{Jesus2017,
author = {Jesus, Rosales Banderas Jose De and Maximo, Lopez Sanchez and Raul, Pinto Elias and Gabriel, Gonzalez Serna},
doi = {10.1109/CSCI.2016.0167},
file = {::},
isbn = {9781509055104},
journal = {Proceedings - 2016 International Conference on Computational Science and Computational Intelligence, CSCI 2016},
keywords = {color calibration,image processing,image stabilizer,iris detection},
pages = {861--864},
title = {{Methodology for Iris Scanning through Smartphones}},
year = {2017}
}
@article{Suk2014,
abstract = {For the last decade, it has been shown that neuroimaging can be a potential tool for the diagnosis of Alzheimer's Disease (AD) and its prodromal stage, Mild Cognitive Impairment (MCI), and also fusion of different modalities can further provide the complementary information to enhance diagnostic accuracy. Here, we focus on the problems of both feature representation and fusion of multimodal information from Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET). To our best knowledge, the previous methods in the literature mostly used hand-crafted features such as cortical thickness, gray matter densities from MRI, or voxel intensities from PET, and then combined these multimodal features by simply concatenating into a long vector or transforming into a higher-dimensional kernel space. In this paper, we propose a novel method for a high-level latent and shared feature representation from neuroimaging modalities via deep learning. Specifically, we use Deep Boltzmann Machine (DBM).22Although it is clear from the context that the acronym DBM denotes "Deep Boltzmann Machine" in this paper, we would clearly indicate that DBM here is not related to "Deformation Based Morphometry"., a deep network with a restricted Boltzmann machine as a building block, to find a latent hierarchical feature representation from a 3D patch, and then devise a systematic method for a joint feature representation from the paired patches of MRI and PET with a multimodal DBM. To validate the effectiveness of the proposed method, we performed experiments on ADNI dataset and compared with the state-of-the-art methods. In three binary classification problems of AD vs. healthy Normal Control (NC), MCI vs. NC, and MCI converter vs. MCI non-converter, we obtained the maximal accuracies of 95.35{\%}, 85.67{\%}, and 74.58{\%}, respectively, outperforming the competing methods. By visual inspection of the trained model, we observed that the proposed method could hierarchically discover the complex latent patterns inherent in both MRI and PET.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Suk, Heung Il and Lee, Seong Whan and Shen, Dinggang},
doi = {10.1016/j.neuroimage.2014.06.077},
eprint = {NIHMS150003},
file = {::},
isbn = {1053-8119},
issn = {10959572},
journal = {NeuroImage},
keywords = {Alzheimer's Disease,Deep boltzmann machine,Mild cognitive impairment,Multimodal data fusion,Shared feature representation},
pages = {569--582},
pmid = {25042445},
publisher = {Elsevier Inc.},
title = {{Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2014.06.077},
volume = {101},
year = {2014}
}
@article{Baltrusaitis2017a,
abstract = {Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.},
archivePrefix = {arXiv},
arxivId = {1705.09406},
author = {Baltru{\v{s}}aitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
doi = {10.1109/TPAMI.2018.2798607},
eprint = {1705.09406},
file = {::},
issn = {0162-8828},
number = {c},
pages = {1--20},
title = {{Multimodal Machine Learning: A Survey and Taxonomy}},
url = {http://arxiv.org/abs/1705.09406},
volume = {8828},
year = {2017}
}
@article{Deng2010,
abstract = {Several Human-Machine/Computer Interfaces (HMI/HCI) had been developed in recent years. Some designs were specifically made for people with disabilities such as injured-vertebra, apoplexy or poliomyelitis, Amyotrophic Lateral Sclerosis (ALS), and Motor Neuron Disease, (MND). In this paper, we proposed an eye-movement tracking system. Based on Electro-Oculography (EOG) technology we detected the signal with different directions in eye-movements and then analyzed to understand what they represented about (e.g. horizontal direction or vertical direction). We converted the analog signal to digital signal and then used as the control signals for Human-Computer Interface (HCI). In order to make the system "robust", several applications with EOG-based HCI had been designed. Our preliminary results revealed more than 90{\%} accuracy rate for examining the eye-movement that may become a new useful human-machine user interface in the near future. {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
author = {Deng, Lawrence Y. and Hsu, Chun Liang and Lin, Tzu Ching and Tuan, Jui Sen and Chang, Shih Ming},
doi = {10.1016/j.eswa.2009.10.017},
file = {::},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Amyotrophic Lateral Sclerosis,Electro-Oculography (EOG),Eye-movement,Human-Machine/Computer Interface (HMI/HCI),Motor Neuron Disease},
number = {4},
pages = {3337--3343},
publisher = {Elsevier Ltd},
title = {{EOG-based Human-Computer Interface system development}},
url = {http://dx.doi.org/10.1016/j.eswa.2009.10.017},
volume = {37},
year = {2010}
}
@article{Lee2017,
abstract = {In recent years, the iris recognition system has been gaining increasing acceptance for applications such as access control and smartphone security. When the images of the iris are obtained under unconstrained conditions, an issue of undermined quality is caused by optical and motion blur, off-angle view (the user's eyes looking somewhere else, not into the front of the camera), specular reflection (SR) and other factors. Such noisy iris images increase intra-individual variations and, as a result, reduce the accuracy of iris recognition. A typical iris recognition system requires a near-infrared (NIR) illuminator along with an NIR camera, which are larger and more expensive than fingerprint recognition equipment. Hence, many studies have proposed methods of using iris images captured by a visible light camera without the need for an additional illuminator. In this research, we propose a new recognition method for noisy iris and ocular images by using one iris and two periocular regions, based on three convolutional neural networks (CNNs). Experiments were conducted by using the noisy iris challenge evaluation-part II (NICE.II) training dataset (selected from the university of Beira iris (UBIRIS).v2 database), mobile iris challenge evaluation (MICHE) database, and institute of automation of Chinese academy of sciences (CASIA)-Iris-Distance database. As a result, the method proposed by this study outperformed previous methods.},
author = {Lee, Min Beom and Hong, Hyung Gil and Park, Kang Ryoung},
doi = {10.3390/s17122933},
file = {::},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Convolutional neural network,Iris and periocular,Noisy iris and ocular image},
number = {12},
pmid = {29258217},
title = {{Noisy ocular recognition based on three convolutional neural networks}},
volume = {17},
year = {2017}
}
@article{Chen2018,
abstract = {In this work, we address the problem of face verification, namely determining whether a pair of face images belongs to the same or different subjects. Previous works often consider solving the problem of face verification in two steps: feature extraction and face recognition, resulting in a fragmented procedure. We argue that these techniques, although working well, fail to explicitly exploit a full end-to-end framework for face verification, which has received much attention and achieved significant improvements recently. In this paper, we propose a novel Joint Bayesian guided metric learning technique for dealing with the face verification task, which well integrates the above two steps of face verification into an end-to-end convolutional neural network (CNN) architecture. In the training stage, an initial neural network, which has the similar architecture with GoogLeNet CNN model, is firstly pre-trained by optimizing classification-based objective functions on the publicly available CASIA WebFace database. Based on constructed face pairs dataset from CASIA WebFace and LFW datasets, we then fine-tune the whole network parameters under the guide of the learned knowledge, which is obtained from the highly successful Joint Bayesian model. This guided learning procedure, which can also be seen as a metric learning technique, can further update network parameters for discriminating face pairs. In the testing process, the outputs by this unified network are discriminated with a threshold value to produce the ultimate prediction for the face verification task. Comprehensive evaluations over the LFW dataset well demonstrate the encouraging face verification performance of our proposed framework.},
author = {Chen, Di and Xu, Chunyan and Yang, Jian and Qian, Jianjun and Zheng, Yuhui and Shen, Linlin},
doi = {10.1016/J.NEUCOM.2017.09.009},
file = {::},
issn = {0925-2312},
journal = {Neurocomputing},
month = {jan},
pages = {560--567},
publisher = {Elsevier},
title = {{Joint Bayesian guided metric learning for end-to-end face verification}},
url = {https://www-sciencedirect-com.zorac.aub.aau.dk/science/article/pii/S0925231217314807{\#}bib0016},
volume = {275},
year = {2018}
}
@article{Duguleana2010,
abstract = {This paper proposes a new approach to real-time robot controlling by integrating an Electrooculography (EOG) measuring device within human-robot interaction (HRI). Our study focuses on controlling robots using EOG for fulfilling elementary robot activities such as basic motor movements and environment interaction. A new EOG-based HRI paradigm has been developed on the specific defined problem of eye blinking. The resulted model is tested using biometric capturing equipment. We propose a simple algorithm for real-time identification and processing of signals produced by eyes during blinking phases. We present the experimental setup and the results of the experiment. We conclude by listing further research issues.},
author = {Duguleana, Mihai and Mogan, Gheorghe},
doi = {10.1007/978-3-642-11628-5_37},
file = {::},
isbn = {9783642116278},
issn = {18684238},
journal = {IFIP Advances in Information and Communication Technology},
keywords = {Control,Electrooculography,Eye blink,Human-robot interaction,Robot},
number = {29},
pages = {343--350},
title = {{Using eye blinking for EOG-based robot control}},
volume = {314},
year = {2010}
}
@article{Nam2014,
abstract = {We present a novel human-machine interface, called GOM-Face , and its application to humanoid robot control. The GOM-Face bases its interfacing on three electric potentials measured on the face: 1) glossokinetic potential (GKP), which involves the tongue movement; 2) electrooculogram (EOG), which involves the eye movement; 3) electromyogram, which involves the teeth clenching. Each potential has been individually used for assistive interfacing to provide persons with limb motor disabilities or even complete quadriplegia an alternative communication channel. However, to the best of our knowledge, GOM-Face is the first interface that exploits all these potentials together. We resolved the interference between GKP and EOG by extracting discriminative features from two covariance matrices: a tongue-movement-only data matrix and eye-movement-only data matrix. With the feature extraction method, GOM-Face can detect four kinds of horizontal tongue or eye movements with an accuracy of 86.7{\%} within 2.77 s. We demonstrated the applicability of the GOM-Face to humanoid robot control: users were able to communicate with the robot by selecting from a predefined menu using the eye and tongue movements.},
author = {Nam, Yunjun and Koo, Bonkon and Cichocki, Andrzej and Choi, Seungjin},
doi = {10.1109/TBME.2013.2280900},
file = {::},
issn = {00189294},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {Electromyogram (EMG),electrooculogram (EOG),glossokinetic potentials (GKP),human-machine interface,multimodal interface},
number = {2},
pages = {453--462},
pmid = {24021635},
title = {{GOM-face: GKP, EOG, and EMG-based multimodal interface with application to humanoid robot control}},
volume = {61},
year = {2014}
}
@article{Zhao2017b,
abstract = {Person re-identification (ReID) is an important task in video surveillance and has various applications. It is non-trivial due to complex background clutters, varying illu-mination conditions, and uncontrollable camera settings. Moreover, the person body misalignment caused by detec-tors or pose variations is sometimes too severe for feature matching across images. In this study, we propose a novel Convolutional Neural Network (CNN), called Spindle Net, based on human body region guided multi-stage feature de-composition and tree-structured competitive feature fusion. It is the first time human body structure information is con-sidered in a CNN framework to facilitate feature learning. The proposed Spindle Net brings unique advantages: 1) it separately captures semantic features from different body regions thus the macro-and micro-body features can be well aligned across images, 2) the learned region features from different semantic regions are merged with a competitive scheme and discriminative features can be well preserved. State of the art performance can be achieved on multiple datasets by large margins. We further demonstrate the ro-bustness and effectiveness of the proposed Spindle Net on our proposed dataset SenseReID without fine-tuning.},
author = {Zhao, Haiyu and Tian, Maoqing and Sun, Shuyang and Shao, Jing and Yan, Junjie and Yi, Shuai and Wang, Xiaogang and Tang, Xiaoou},
doi = {10.1109/CVPR.2017.103},
file = {::},
isbn = {978-1-5386-0457-1},
journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {907--915},
title = {{Spindle Net: Person Re-identification with Human Body Region Guided Feature Decomposition and Fusion}},
url = {http://ieeexplore.ieee.org/document/8099586/},
volume = {1},
year = {2017}
}
@article{Sung2013,
abstract = {This study uses ZigBee wireless sensor network technology to build a Multi-purpose Electronic Score system based on the electro-oculogram (EOG). As a requirement when learning to play a musical instrument the most commonly seen scores are available in paper copies. The disadvantage of the paper music score is that pages must be turned manually by the performer while playing music. An alternative is to prepare a miniaturized copy of the score, but this makes the score difficult to read clearly. Electronic scores have recently become available and are expected to become mainstream in the future due to the rapid growth in ebook readers and tablet PCs. Tracking the upward, downward, leftward, rightward and clockwise/counterclockwise eyeball movements and eye blinking, an amplified, bandpass filtered electro-oculogram (EOG) signal is converted into digital form. This signal is further transmitted through a ZigBee wireless module on which an electronic score program is installed, including the following operating modes; go to the previous/next page, recording, tuning and tempo modes. Detected eyeball movements allow a score to be viewed conveniently while playing. In the future, the proposed technology can be applied to flexible paper, flexible displays and the like. {\textcopyright} 2012 Elsevier B.V.},
author = {Sung, Wen Tsai and Chen, Jui Ho and Chang, Kuo Yi},
doi = {10.1016/j.sna.2012.11.028},
file = {::},
isbn = {0924-4247},
issn = {09244247},
journal = {Sensors and Actuators, A: Physical},
keywords = {EOG,Multipurpose Electronic Score,Wireless Sensor Network,ZigBee},
pages = {141--152},
publisher = {Elsevier B.V.},
title = {{ZigBee based multi-purpose electronic score design and implementation using EOG}},
url = {http://dx.doi.org/10.1016/j.sna.2012.11.028},
volume = {190},
year = {2013}
}
@article{Abate2017,
abstract = {The increasing popularity of smartphones amongst the population laid the basis for a wide range of applications aimed at security and privacy protection. Very modern mobile devices have recently demonstrated the feasibility of using a camera sensor to access the system without typing any alphanumerical password. In this work, we present a method that implements iris recognition in the visible spectrum through unsupervised learning by means of Self Organizing Maps (SOM). The proposed method uses a SOM network to cluster iris features at pixel level. The discriminative feature map is obtained by using RGB data of the iris combined with the statistical descriptors of kurtosis and skewness. An experimental analysis on MICHE-I and UBIRISv1 datasets demonstrates the strengths and weaknesses of the algorithm, which has been specifically designed to require low processing power in compliance with the limited capability of common mobile devices.},
author = {Abate, Andrea F. and Barra, Silvio and Gallo, Luigi and Narducci, Fabio},
doi = {10.1016/j.patrec.2017.02.002},
file = {::},
isbn = {0000000000},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Iris recognition,Mobile biometric recognition,Statistical descriptors,Unsupervised learning},
pages = {37--43},
pmid = {11341202},
publisher = {Elsevier B.V.},
title = {{Kurtosis and skewness at pixel level as input for SOM networks to iris recognition on mobile devices}},
volume = {91},
year = {2017}
}
@article{Yang2017a,
abstract = {We present an end-to-end, multimodal, fully convolutional$\backslash$r$\backslash$nnetwork for extracting semantic structures from document$\backslash$r$\backslash$nimages. We consider document semantic structure$\backslash$r$\backslash$nextraction as a pixel-wise segmentation task, and propose a$\backslash$r$\backslash$nunified model that classifies pixels based not only on their$\backslash$r$\backslash$nvisual appearance, as in the traditional page segmentation$\backslash$r$\backslash$ntask, but also on the content of underlying text. Moreover,$\backslash$r$\backslash$nwe propose an efficient synthetic document generation process$\backslash$r$\backslash$nthat we use to generate pretraining data for our network.$\backslash$r$\backslash$nOnce the network is trained on a large set of synthetic$\backslash$r$\backslash$ndocuments, we fine-tune the network on unlabeled real documents$\backslash$r$\backslash$nusing a semi-supervised approach. We systematically$\backslash$r$\backslash$nstudy the optimum network architecture and show that$\backslash$r$\backslash$nboth our multimodal approach and the synthetic data pretraining$\backslash$r$\backslash$nsignificantly boost the performance.},
archivePrefix = {arXiv},
arxivId = {1706.02337},
author = {Yang, Xiao and Yumer, Ersin and Asente, Paul and Kraley, Mike and Kifer, Daniel and Giles, C. Lee},
doi = {10.1109/CVPR.2017.462},
eprint = {1706.02337},
file = {::},
isbn = {978-1-5386-0457-1},
journal = {Cvpr2017},
title = {{Learning to Extract Semantic Structure from Documents Using Multimodal Fully Convolutional Neural Networks}},
url = {https://arxiv.org/pdf/1706.02337.pdf},
year = {2017}
}
@inproceedings{Sangeetha2013,
abstract = {-In a Multimodal biometric system, the effective fusion method is necessary for combining information from various single modality systems. Two biometric characteristics are considered in this study: iris and fingerprint. Multimodal biometric system needs an effective fusion scheme to combine biometric characteristics derived from one or more modalities. The score level fusion is used to combine the characteristics from diff erent biometric modalities. Fusion at the score level is a new technique, which has a high potential for e f ficient consolidation of multiple unimodal biometric matcher outputs. Support vector machine and extreme learning techniques are used in this system for recognition of biometric traits. In this, the Fingerprint-Iris system provides better performance and comparison of support vector machine and extreme learning machine based on score-level fusion methods is obtained In score-level fusion, ELM provides better performance as compare to the SVM It reduces the classification time of current system. This work is valuable and makes an e f ficient accuracy in such applications. This system can be utilized for person identification in several applications.},
author = {Sangeetha, S and Radha, N.},
booktitle = {2013 7th International Conference on Intelligent Systems and Control (ISCO)},
doi = {10.1109/ISCO.2013.6481145},
isbn = {978-1-4673-4603-0},
month = {jan},
pages = {183--188},
publisher = {IEEE},
title = {{A New Framework for IRIS and Fingerprint Recognition Using SVM Classification and Extreme Learning Machine Based on Score Level Fusion}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6481145},
year = {2013}
}
@article{Krizhevsky2017b,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%}, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, con-sists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed reg-ularization method called " dropout " that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the sec-ond-best entry. 1. PROLOGUE Four years ago, a paper by Yann LeCun and his collaborators was rejected by the leading computer vision conference on the grounds that it used neural networks and therefore pro-vided no insight into how to design a vision system. At the time, most computer vision researchers believed that a vision system needed to be carefully hand-designed using a detailed understanding of the nature of the task. They assumed that the task of classifying objects in natural images would never be solved by simply presenting examples of images and the names of the objects they contained to a neural network that acquired all of its knowledge from this training data. What many in the vision research community failed to appreciate was that methods that require careful hand-engi-neering by a programmer who understands the domain do not scale as well as methods that replace the programmer with a powerful general-purpose learning procedure. With enough computation and enough data, learning beats pro-gramming for complicated tasks that require the integration of many different, noisy cues. Four years ago, while we were at the University of Toronto, our deep neural network called SuperVision almost halved the error rate for recognizing objects in natural images and triggered an overdue paradigm shift in computer vision. Figure 4 shows some examples of what SuperVision can do. SuperVision evolved from the multilayer neural networks that were widely investigated in the 1980s. These networks used multiple layers of feature detectors that were all learned from the training data. Neuroscientists and psychologists had hypothesized that a hierarchy of such feature detectors would provide a robust way to recognize objects but they had no idea how such a hierarchy could be learned. There was great excite-ment in the 1980s because several different research groups discovered that multiple layers of feature detectors could be trained efficiently using a relatively straight-forward algorithm called backpropagation},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {10.1145/3065386},
file = {::},
journal = {COMMUNICATIONS OF THE ACM},
number = {6},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://delivery.acm.org/10.1145/3070000/3065386/p84-krizhevsky.pdf?ip=130.225.198.196{\&}id=3065386{\&}acc=OA{\&}key=36332CD97FA87885.1DDFD8390336D738.4D4702B0C3E38B35.5945DC2EABF3343C{\&}{\_}{\_}acm{\_}{\_}=1518530104{\_}1e0b97ec1deaadc1937d34c07a1392ac},
volume = {60},
year = {2017}
}
@article{Kuehlkamp2016a,
abstract = {Iris recognition systems are a mature technology that is widely used throughout the world. In identification (as opposed to verification) mode, an iris to be recognized is typically matched against all N enrolled irises. This is the classic "1-to-N search". In order to improve the speed of large-scale identification, a modified "1-to-First" search has been used in some operational systems. A 1-to-First search terminates with the first below-threshold match that is found, whereas a 1-to-N search always finds the best match across all enrollments. We know of no previous studies that evaluate how the accuracy of 1-to-First search differs from that of 1-to-N search. Using a dataset of over 50,000 iris images from 2,800 different irises, we perform experiments to evaluate the relative accuracy of 1-to-First and 1-to-N search. We evaluate how the accuracy difference changes with larger numbers of enrolled irises, and with larger ranges of rotational difference allowed between iris images. We find that False Match error rate for 1-to-First is higher than for 1-to-N, and the the difference grows with larger number of enrolled irises and with larger range of rotation.},
annote = {The way that iris recognition works is that some kind of filter is applied to localize the iris. 2D Gador filter is often cited. Then that iris is checked against the whole database. This is called 1:N. They check the Hamming Distance between of the bits of the data and then choose the lowest ones as a pair. In 1:First search they do the same except there is is a threshold that that is has to be under to be accepted. If it is under that threshold it is accepted and the search is stopped. Two types of error can occour: a False Match (FM) and False Non-Match (FNM). A false match occurs when two samples from different individuals are declared by the system as a match. A false non-match is when two samples from the same individual fail to be considered as a match by the system},
archivePrefix = {arXiv},
arxivId = {1702.01167},
author = {Kuehlkamp, Andrey and Bowyer, Kevin W.},
doi = {10.1109/WACV.2016.7477687},
eprint = {1702.01167},
file = {::},
isbn = {9781509006410},
journal = {2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016},
title = {{An analysis of 1-to-first matching in iris recognition}},
year = {2016}
}
@book{Wechsler2007,
abstract = {One of the grand challenges for computational intelligence and biometrics is to understand how people process and recognize faces and to develop automated and reliable face recognition systems. Biometrics has become the major component in the complex decision making process associated with security applications. The face detection and authentication challenges addressed include cluttered environments, image variability, occlusion and disguise, and temporal changes all within open set recognition. Reliable Face Recognition Methods: System Design, Implementation and Evaluation comprehensively explores the face recognition problem while drawing inspiration from complementary disciplines such as neurosciences, statistics, signal and image processing, computer vision, and machine learning and pattern recognition. This book also examines the evolution of face recognition research and explores promising new avenues for research and development. Reliable Face Recognition Methods: System Design, Implementation and Evaluation benefits graduate-level students, researchers, and practitioners, as well as government and industry decision makers in the security arena. Ruud Bolle (IBM): "Harry Wechsler's monograph provides a thorough, up-to-date, and in-depth overview of the many advances in the area of face recognition. gives an excellent overview of the issues related to security and privacy when it comes to automated biometrics. In summary, this book has the potential to become a classic. Harry Wechsler is to be commended for undertaking the monumental task of writing this book." John Daugman (Cambridge University, UK): "The book looks excellent. The topic is timely, and the perspective-multi-disciplinary, measured, and objective - is much needed and welcome. I believe that Wechsler's latest book will make a valued contribution to this important field and will become a standard." David Zhang (Hong Kong Polytechnic University, China): "From a system view, this book shows an excellent arrangement, from data collection to face recognition, as well as system performance evaluation and error analysis. This book can serve both as an interdisciplinary text and a research reference. Each chapter provides the background and impetus for understanding the problems discussed." Stan Li (Chinese Academy of Sciences, China): "The book treats the subject of face recognition in a fairly systematic way. The book covers most relevant topics in face recognition, in a well organized way. The information is also useful for experts. The manuscript is easy to read." Tom Huang (University of Illinois, USA): "Harry Wechsler is without doubt one of the leading authorities in face recognition and related topics. We have recently seen a number of excellent edited books on Face Recognition. However, Wechsler's book is the first unified treatment of this important subject. In my opinion, Wechsler is one of only a handful of people in the world who could write such a comprehensive, unified, informative, perceptive, and authoritative book on Face Recognition. Harry Wechsler so far, is the only one of this handful who took the time and effort to realize such a project. The book certainly will have a great positive impact on the biometrics research community. But also, because it looks at Face Recognition from different perspectives, it will be welcomed by non-experts." {\textcopyright} 2007 Springer Science+Business Media, LLC. All rights reserved.},
address = {Boston, MA},
author = {Wechsler, Harry},
booktitle = {Reliable Face Recognition Methods: System Design, Impementation and Evaluation},
doi = {10.1007/978-0-387-38464-1},
file = {::},
isbn = {038722372X},
pages = {1--329},
publisher = {Springer US},
title = {{Reliable face recognition methods: System design, implementation and evaluation}},
url = {http://link.springer.com/10.1007/978-0-387-38464-1 https://link-springer-com.zorac.aub.aau.dk/book/10.1007{\%}2F978-0-387-38464-1{\#}about},
year = {2007}
}
@article{Jung2017,
abstract = {Finding the accurate position of an eye is crucial for mobile iris recognition system in order to extract the iris region quickly and correctly. Unfortunately, this is very difficult to accomplish when a person is wearing eyeglasses because of the interference from the eyeglasses. This paper proposes an eye detection method that is robust to eyeglass interference in mobile environment. The proposed method comprises two stages: eye candidate generation and eye validation. In the eye candidate generation stage, multi-scale window masks consisting of 2 × 3 subblocks are used to generate all image blocks possibly containing an eye image. In the ensuing eye validation stage, two methods are employed to determine which blocks actually contain true eye images and locate their precise positions as well: the first method searches for the glint of an NIR illuminator on the pupil region. If this first method fails, the next method computes the intensity difference between the assumed pupil and its surrounding region using multi-scale 3 × 3 window masks. Experimental results show that the proposed method detects the eye position more accurately and quickly than competing methods in the presence of interference from eyeglass frames.},
author = {Jung, Yujin and Kim, Dongik and Son, Byungjun and Kim, Jaihie},
doi = {10.1016/j.eswa.2016.09.036},
file = {::},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Eye detection,Eye validation,Eyeglasses,Iris detection,Iris recognition,Mobile},
pages = {178--188},
publisher = {Elsevier Ltd},
title = {{An eye detection method robust to eyeglasses for mobile iris recognition}},
url = {http://dx.doi.org/10.1016/j.eswa.2016.09.036},
volume = {67},
year = {2017}
}
@article{Zhang2016a,
author = {Zhang, Man and Zhang, Qi and Sun, Zhenan and Zhou, Shujuan and Ahmed, Nasir Uddin},
doi = {10.1109/BTAS.2016.7791191},
file = {::},
isbn = {9781467397339},
journal = {2016 IEEE 8th International Conference on Biometrics Theory, Applications and Systems, BTAS 2016},
title = {{The BTAS∗Competition on Mobile Iris Recognition}},
year = {2016}
}
