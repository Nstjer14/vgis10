Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Zhao2015a,
abstract = {This paper proposes a novel and more accurate iris segmentation framework to automatically segment iris region from the face images acquired with relaxed imaging under visible or near-infrared illumination, which provides strong feasibility for applications in surveillance, forensics and the search for missing children, etc. The proposed framework is built on a novel total-variation based formulation which uses l1 norm regularization to robustly suppress noisy texture pixels for the accurate iris localization. A series of novel and robust post processing operations are introduced to more accurately localize the limbic boundaries. Our experimental results on three publicly available databases, i.e., FRGC, UBIRIS.v2 and CASIA.v4-distance, achieve significant performance improvement in terms of iris segmentation accuracy over the state-of-the-art approaches in the literature. Besides, we have shown that using iris masks generated from the proposed approach helps to improve iris recognition performance as well. Unlike prior work, all the implementations in this paper are made publicly available to further advance research and applications in biometrics at-d-distance.},
author = {Zhao, Zijing and Kumar, Ajay},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.436},
file = {::;::},
isbn = {9781467383912},
issn = {15505499},
month = {dec},
pages = {3828--3836},
publisher = {IEEE},
title = {{An accurate iris segmentation framework under relaxed imaging constraints using total variation model}},
url = {http://ieeexplore.ieee.org/document/7410793/},
volume = {2015 Inter},
year = {2015}
}
@article{Kauba2016,
abstract = {—Authentication based on vein patterns is a very promising biometric technique. The most important step is the accurate extraction of the vein pattern from sometimes low quality input images. A single feature extraction technique may fail to correctly extract the vein pattern, entailing bad recognition performance. One of the solutions that can be used to improve recognition results is biometric fusion. A possible fusion strategy is feature level fusion, that is the fusion of several feature extractors' outputs. In our work, we exploited the feature level fusion to improve the quality of the extracted vein patterns and thus the feature extraction accuracy. An experimental study involving different feature extraction techniques (maximum curvature, repeated line tracking, wide line detector, ...) and different fusion techniques (majority voting, weighted average, STAPLE, ...) is conducted on the UTFVP finger-vein data set. The results show that feature level fusion is able to improve the recognition accuracy in terms of the EER over the single feature extraction techniques.},
author = {Kauba, Christof and Uhl, Andreas and Piciucco, Emanuela and Maiorana, Emanuele and Campisi, Patrizio},
doi = {10.1109/BIOSIG.2016.7736908},
file = {::},
isbn = {9783885796541},
issn = {16175468},
journal = {Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)},
title = {{Advanced variants of feature level fusion for finger vein recognition}},
volume = {P-260},
year = {2016}
}
@article{Yang2017a,
abstract = {We present an end-to-end, multimodal, fully convolutional$\backslash$r$\backslash$nnetwork for extracting semantic structures from document$\backslash$r$\backslash$nimages. We consider document semantic structure$\backslash$r$\backslash$nextraction as a pixel-wise segmentation task, and propose a$\backslash$r$\backslash$nunified model that classifies pixels based not only on their$\backslash$r$\backslash$nvisual appearance, as in the traditional page segmentation$\backslash$r$\backslash$ntask, but also on the content of underlying text. Moreover,$\backslash$r$\backslash$nwe propose an efficient synthetic document generation process$\backslash$r$\backslash$nthat we use to generate pretraining data for our network.$\backslash$r$\backslash$nOnce the network is trained on a large set of synthetic$\backslash$r$\backslash$ndocuments, we fine-tune the network on unlabeled real documents$\backslash$r$\backslash$nusing a semi-supervised approach. We systematically$\backslash$r$\backslash$nstudy the optimum network architecture and show that$\backslash$r$\backslash$nboth our multimodal approach and the synthetic data pretraining$\backslash$r$\backslash$nsignificantly boost the performance.},
archivePrefix = {arXiv},
arxivId = {1706.02337},
author = {Yang, Xiao and Yumer, Ersin and Asente, Paul and Kraley, Mike and Kifer, Daniel and Giles, C. Lee},
doi = {10.1109/CVPR.2017.462},
eprint = {1706.02337},
file = {::},
isbn = {978-1-5386-0457-1},
journal = {Cvpr2017},
title = {{Learning to Extract Semantic Structure from Documents Using Multimodal Fully Convolutional Neural Networks}},
url = {https://arxiv.org/pdf/1706.02337.pdf},
year = {2017}
}
@article{Phillips2009,
abstract = {The goal of the Multiple Biometrics Grand Challenge (MBGC) is to improve the performance of face and iris recognition technology from biometric samples acquired under unconstrained conditions. The MBGC is organized into three challenge problems. Each challenge problem re- laxes the acquisition constraints in different directions. In the Portal Challenge Problem, the goal is to recognize people from near-infrared (NIR) and high definition (HD) video as they walk through a portal. Iris recognition can be performed from the NIR video and face recognition from the HD video. The availability of NIR and HD modalities allows for the development of fusion algorithms. The Still Face Challenge Problem has two primary goals. The first is to improve recognition performance from frontal and off angle still face images taken under uncontrolled in- door and outdoor lighting. The second is to improve recognition perfor- mance on still frontal face images that have been resized and compressed, as is required for electronic passports. In the Video Challenge Problem, the goal is to recognize people from video in unconstrained environments. The video is unconstrained in pose, illumination, and camera angle. All three challenge problems include a large data set, experiment descrip- tions, ground truth, and scoring code.},
author = {Phillips, P. Jonathon and Flynn, Patrick J. and Beveridge, J. Ross and Scruggs, W. Todd and O'Toole, Alice J. and Bolme, David and Bowyer, Kevin W. and Draper, Bruce A. and Givens, Geof H. and Lui, Yui Man and Sahibzada, Hassan and Scallan, Joseph A. and Weimer, Samuel},
doi = {10.1007/978-3-642-01793-3_72},
file = {::},
isbn = {3642017924},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {705--714},
title = {{Overview of the multiple biometrics grand challenge}},
volume = {5558 LNCS},
year = {2009}
}
@article{Galdi2017a,
author = {Galdi, Chiara and Dugelay, Jean Luc},
doi = {10.1109/ICPR.2016.7899626},
file = {::},
isbn = {9781509048472},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
pages = {160--164},
title = {{Fusing iris colour and texture information for fast iris recognition on mobile devices}},
year = {2017}
}
@article{Postelnicu2012,
abstract = {This paper presents an EOG-based (Electrooculography) interface for Human Computer Interface (HCI) purposes. The solution enables the filtering of the recorded signals and identification of characteristic peak amplitudes associated with eye saccades, blinks or winks by using a classifier based on a set of fuzzy logic rules and a deterministic finite automaton. The identified eye saccades were assigned to six low-level commands for navigation purposes. An experiment study was conducted in order to check the accuracy and the performances of the proposed interface compared with three traditional input control interfaces. Experimental results show that the developed interface has good performance and can be used for online communication and control in EOG-based HCI systems or even for first-person navigation metaphors in games industry. {\textcopyright} 2012 Elsevier Ltd. All rights reserved.},
author = {Postelnicu, Cristian Cezar and Girbacia, Florin and Talaba, Doru},
doi = {10.1016/j.eswa.2012.03.007},
file = {::},
isbn = {09574174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Deterministic finite automaton,Electrooculography,Fuzzy logic,Human computer interaction,Navigation,Virtual environment},
number = {12},
pages = {10857--10866},
publisher = {Elsevier Ltd},
title = {{EOG-based visual navigation interface development}},
url = {http://dx.doi.org/10.1016/j.eswa.2012.03.007},
volume = {39},
year = {2012}
}
@article{Zhao2017b,
abstract = {Person re-identification (ReID) is an important task in video surveillance and has various applications. It is non-trivial due to complex background clutters, varying illu-mination conditions, and uncontrollable camera settings. Moreover, the person body misalignment caused by detec-tors or pose variations is sometimes too severe for feature matching across images. In this study, we propose a novel Convolutional Neural Network (CNN), called Spindle Net, based on human body region guided multi-stage feature de-composition and tree-structured competitive feature fusion. It is the first time human body structure information is con-sidered in a CNN framework to facilitate feature learning. The proposed Spindle Net brings unique advantages: 1) it separately captures semantic features from different body regions thus the macro-and micro-body features can be well aligned across images, 2) the learned region features from different semantic regions are merged with a competitive scheme and discriminative features can be well preserved. State of the art performance can be achieved on multiple datasets by large margins. We further demonstrate the ro-bustness and effectiveness of the proposed Spindle Net on our proposed dataset SenseReID without fine-tuning.},
author = {Zhao, Haiyu and Tian, Maoqing and Sun, Shuyang and Shao, Jing and Yan, Junjie and Yi, Shuai and Wang, Xiaogang and Tang, Xiaoou},
doi = {10.1109/CVPR.2017.103},
file = {::},
isbn = {978-1-5386-0457-1},
journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {907--915},
title = {{Spindle Net: Person Re-identification with Human Body Region Guided Feature Decomposition and Fusion}},
url = {http://ieeexplore.ieee.org/document/8099586/},
volume = {1},
year = {2017}
}
@article{Jesus2017,
author = {Jesus, Rosales Banderas Jose De and Maximo, Lopez Sanchez and Raul, Pinto Elias and Gabriel, Gonzalez Serna},
doi = {10.1109/CSCI.2016.0167},
file = {::},
isbn = {9781509055104},
journal = {Proceedings - 2016 International Conference on Computational Science and Computational Intelligence, CSCI 2016},
keywords = {color calibration,image processing,image stabilizer,iris detection},
pages = {861--864},
title = {{Methodology for Iris Scanning through Smartphones}},
year = {2017}
}
@article{Gulmire2012,
author = {Gulmire, Kshamaraj and Ganorkar, Sanjay},
file = {::},
issn = {2278-0181},
number = {5},
pages = {1--5},
title = {{Iris Recognition Using Gabor Wavelet}},
volume = {1},
year = {2012}
}
@article{Aakerberg2017,
abstract = {Object recognition is one of the important tasks in computer vision which has found enormous applications. Depth modality is proven to provide supplementary information to the common RGB modality for object recognition. In this paper, we propose methods to improve the recognition performance of an existing deep learning based RGB-D object recognition model, namely the FusionNet proposed by Eitel et al. First, we show that encoding the depth values as colorized surface normals is beneficial, when the model is initialized with weights learned from training on ImageNet data. Additionally, we show that the RGB stream of the FusionNet model can benefit from using deeper network architectures, namely the 16-layered VGGNet, in exchange for the 8-layered CaffeNet. In combination, these changes improves the recognition performance with 2.2{\%} in comparison to the original FusionNet, when evaluating on the Washington RGB-D Object Dataset.},
author = {Aakerberg, Andreas and Nasrollahi, Kamal and Rasmussen, Christoffer B. and Moeslund, Thomas B.},
doi = {10.5220/0006511501210128},
file = {::},
isbn = {978-989-758-274-5},
journal = {Proceedings of the 9th International Joint Conference on Computational Intelligence},
keywords = {artificial vision,computer vision,convolutional neural networks,deep learning,has found enormous applications,in computer vision which,learning,object recognition is one,of the important tasks,rgb-d,surface normals,transfer},
pages = {121--128},
title = {{Depth Value Pre-Processing for Accurate Transfer Learning based RGB-D Object Recognition}},
url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0006511501210128},
year = {2017}
}
@article{Heide1999,
annote = {General notes:
Review of different eye tracking techniques. From 1999.

EOG:
Used most in clinical research
Pros:
- Non intrusive
- Does not limit field of view
- Can be used while the subject is wearing glasses/lenses
- Can be used with non cooperative subject
- Can be used while eye is closed ( E.G while asleep)

Cons:
- Sensitive to changes in light
- Often contaminated by electrical noise + blinks and eye lids.
- Horisontal range of +-40 deg
- Resolution of about 1-2 deg
- Vertical eye movements are often unreliable and require special configurations
- Torsinal eye movemens (around the line of sight) are not possible

VOG:
Pros:
- Non invasive
- 3D as well.
- Range of +- 40 deg horisontal and +- 30 deg vertical
- spatial resolution of about 0.5 deg

Cons:
Not really any that are relevant anymore},
author = {Heide, W and Koenig, E and Trillenberg, P and K{\"{o}}mpf, D and Zee, D S},
file = {::},
issn = {0424-8155},
journal = {Electroencephalography and clinical neurophysiology. Supplement},
pages = {223--240},
pmid = {10590990},
title = {{Electrooculography: technical standards and applications. The International Federation of Clinical Neurophysiology.}},
volume = {52},
year = {1999}
}
@article{Wijesoma2005,
abstract = {Assistive robots are increasingly being used to improve the quality of the life of disabled or handicapped people. In this paper a complete system is presented that can be used by people with extremely limited peripheral mobility but having the ability for eye motor coordination. The electrooculogram signals (EOG) that results from the eye displacement in the orbit of the subject are processed in real time to interpret intent and hence generate appropriate control signals to the assistive device. The effectiveness of the proposed methodology and the algorithms are demonstrated using a mobile robot for a limited vocabulary},
author = {Wijesoma, W.S. and Wee, Kang Say Wee Kang Say and Wee, Ong Choon Wee Ong Choon and a.P. Balasuriya and San, Koh Tong San Koh Tong and Soon, Low Kay Soon Low Kay},
doi = {10.1109/ROBIO.2005.246316},
file = {::},
isbn = {0-7803-9315-5},
journal = {2005 IEEE International Conference on Robotics and Biomimetics - ROBIO},
keywords = {- electrooculography,disabled people,eoc,eye movements,severely,wheelchair},
pages = {490--494},
title = {{EOG based control of mobile assistive platforms for the severely disabled}},
year = {2005}
}
@article{Kim2016,
abstract = {The iris recognition on a mobile phone is different from the conventional iris recognition implemented on a dedicated device in that the computational power of a mobile phone and the space for placing NIR (near infrared) LED (light emitting diode) illuminators and iris camera are limited. This paper raises these issues in detail based on real implementation of an iris recognition system in a mobile phone and proposes some solutions to these issues. An experimental study was conducted to search for the relevant power and wavelength of NIR LED illuminators with their positioning on a phone for capturing a good quality iris image. Subsequently, in view of the disparity between the user's gazing point and the center of the iris camera which causes degradation of acquired iris images, an experiment was performed to locate the appropriate gazing point for good iris image capture. A fast eye detection algorithm was proposed for implementation under the mobile platform with low computational facility. The experiments were conducted on a currently released mobile phone and the results showed promising potential for adoption of iris recognition as a reliable authentication means. As a result, two 850 nm LEDs were selected for iris illumination at 1.1 cm away from the iris camera for the size of a 7 cm × 13.7 cm phone. In the performance, the recognition accuracy was 0.1{\%} EER (equal error rate) and the eye detection rate with the speed of 17.64 ms on a mobile phone was 99.4{\%}.},
annote = {Keynotes:
Contributes with a good NIR mobile algorithm that's fast on mobile phones. They also contribute with an mobile iris database of 500 images that they are willing to share for research. Could not find it online.},
author = {Kim, Dongik and Jung, Yujin and Toh, Kar Ann and Son, Byungjun and Kim, Jaihie},
doi = {10.1016/j.eswa.2016.01.050},
file = {::},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Handheld,Iris recognition,Mobile,Portable,Smartphone},
pages = {328--339},
publisher = {Elsevier Ltd},
title = {{An empirical study on iris recognition in a mobile phone}},
url = {http://dx.doi.org/10.1016/j.eswa.2016.01.050},
volume = {54},
year = {2016}
}
@article{DeMarsico2018,
abstract = {Mobile biometrics technologies are nowadays the new frontier for secure use of data and services, and are considered particularly important due to the massive use of handheld devices in the entire world. Among the biometric traits with potential to be used in mobile settings, the iris/ocular region is a natural candidate, even considering that further advances in the technology are required to meet the operational requirements of such ambitious environments. Aiming at promoting these advances, we organized the Mobile Iris Challenge Evaluation (MICHE)-I contest. This paper presents a comparison of the performance of the participant methods by various Figures of Merit (FoMs). A particular attention is devoted to the identification of the image covariates that are likely to cause a decrease in the performance levels of the compared algorithms. Among these factors, interoperability among different devices plays an important role. The methods (or parts of them) implemented by the analyzed approaches are classified into segmentation (S), which was the main target of MICHE-I, and recognition (R). The paper reports both the results observed for either S or R, and also for different recombinations (S+R) of such methods. Last but not least, we also present the results obtained by multi-classifier strategies.},
author = {{De Marsico}, Maria and Nappi, Michele and Narducci, Fabio and Proen{\c{c}}a, Hugo},
doi = {10.1016/j.patcog.2017.08.028},
file = {::},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Biometric algorithm fusion,Evaluation,Mobile Iris Recognition},
pages = {286--304},
publisher = {Elsevier Ltd},
title = {{Insights into the results of MICHE I - Mobile Iris CHallenge Evaluation}},
volume = {74},
year = {2018}
}
@article{Chowdhury2016a,
author = {Chowdhury, Anurag and Ghosh, Soumyadeep and Singh, Richa and Vatsa, Mayank},
doi = {10.1109/BTAS.2016.7791199},
file = {::},
isbn = {9781467397339},
journal = {2016 IEEE 8th International Conference on Biometrics Theory, Applications and Systems, BTAS 2016},
title = {{RGB-D face recognition via learning-based reconstruction}},
year = {2016}
}
@article{Abbas2017,
abstract = {Modern biometrics delivers an enhanced level of security by means of a “proof of property”. The design and deployment of a biometric system, however, hide many pitfalls, which, when underestimated, can lead to major security weaknesses and privacy threats. Issues of concern include biometric identity theft and privacy invasion because of the strong connection between a user and his identity. This book showcases a collection of comprehensive references on the advances of biometric security technology. It compiles a total of fourteen articles, all contributed by thirty-two eminent researchers in the field, thus providing concise and accessible coverage of not only general issues, but also state-of-the-art solutions. The book is divided into five parts: (1) Biometric Template Protection, which covers cancellable biometrics and parameter management protocol;(2) Biometric Key and Encryption, focusing on biometric key generation and visual biometric cryptography;(3) Biometric Systems Analysis, dealing with biometric system security, and privacy evaluation and assessment;(4) Privacy-Enhanced Biometric Systems, covering privacy-enhanced biometric system protocol design and implementation; and(5) Other Biometric Security Technologies.The book will be of particular interest to researchers, scholars, graduate students, engineers, practitioners and developers interested in security and privacy-related issues in biometric systems. It will also be attractive to managers of various organizations with strong security needs.},
author = {Abbas, Sherif N},
doi = {10.1007/978-3-319-47301-7},
file = {::},
isbn = {978-3-319-47300-0},
title = {{Biometric Security and Privacy}},
url = {http://link.springer.com/10.1007/978-3-319-47301-7},
year = {2017}
}
@article{Nguyen2010,
author = {Nguyen, Kien and Fookes, Clinton and Sridharan, Sridha},
doi = {10.1145/1852611.1852635},
file = {::},
isbn = {9781450301053},
journal = {Proceedings of the 2010 Symposium on Information and Communication Technology - SoICT '10},
keywords = {iris recognition,mbgc,signal-level fusion,super-resolution},
number = {November},
pages = {122},
title = {{Robust mean super-resolution for less cooperative NIR iris recognition at a distance and on the move}},
url = {http://portal.acm.org/citation.cfm?doid=1852611.1852635},
year = {2010}
}
@article{Wang2009a,
abstract = {Feature-level fusion remains a challenging problem for multimodal biometrics. However, existing fusion schemes such as sum rule and weighted sum rule are inefficient in complicated condition. In this paper, we propose an efficient feature-level fusion algorithm for iris and face in parallel. The algorithm first normalizes the original features of iris and face using z-score model, and then take complex FDA as the classifier of Unitary space. The proposed algorithm is tested using CASIA iris database and two face databases (ORL database and Yale database.). Experimental results show the effectiveness of the proposed algorithm.},
author = {Wang, Zhifang and Han, Qi and Niu, Xiamu and Busch, Christoph},
doi = {10.1007/978-3-642-01513-7_38},
file = {::},
isbn = {3642015123},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Biometrics,CFDA,Feature-level,Parallel fusion,Unitary space},
number = {PART 3},
pages = {356--364},
title = {{Feature-level fusion of Iris and face for personal identification}},
volume = {5553 LNCS},
year = {2009}
}
@article{Vivek2012a,
annote = {Score level fusion?},
author = {Vivek, S. Arun and Aravinth, J. and Valarmathy, S.},
doi = {10.1109/ICPRIME.2012.6208377},
file = {::},
isbn = {978-1-4673-1039-0},
journal = {International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME-2012)},
keywords = {density based score level,error,feature extraction,fusion,gmm,likelihood ratio test,multimodal biometrics,rates,template,unimodal biometrics},
pages = {387--392},
title = {{Feature extraction for multimodal biometric and study of fusion using Gaussian mixture model}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6208377},
year = {2012}
}
@incollection{Bowyer2016b,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
annote = {Read only chapter 17 about iris and face fusion. The article gives a nice and clean overview of different multi-biometric systems as well as levels of data abstraction the data fusion can be applied on.

The work presented performs iris and face fusion using multi-sample, multi instance, and multimodal data. it fuses the multi sample, and multi instance together, by simply finding the best match in the variations. this is done as a score level fusion. and it fuses the two modalities by rank-level fusion using Broda count.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Connaughton, Ryan and Bowyer, Kevin W and Flynn, Patrick J},
booktitle = {Handbook of Iris Recognition},
doi = {10.1007/978-1-4471-6784-6},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {978-1-4471-6782-2},
issn = {16130073},
pages = {397--415},
pmid = {25246403},
title = {{Chapter 17 Fusion of Face and Iris Biometrics}},
url = {http://link.springer.com/10.1007/978-1-4471-6784-6},
year = {2016}
}
@inproceedings{deepID2014,
abstract = {The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset, 99.15{\%} face verification accuracy is achieved. Compared with the best deep learning result on LFW, the error rate has been significantly reduced by 67{\%}.},
archivePrefix = {arXiv},
arxivId = {1406.4773},
author = {Sun, Yi and Wang, Xiaogang and Tang, Xiaoou},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.244},
eprint = {1406.4773},
file = {::},
isbn = {9781479951178},
issn = {10636919},
keywords = {deep learning,face verification},
pages = {1891--1898},
pmid = {21808091},
title = {{Deep learning face representation from predicting 10,000 classes}},
url = {http://mmlab.ie.cuhk.edu.hk/pdf/YiSun{\_}CVPR14.pdf},
year = {2014}
}
@article{Saha2017a,
annote = {Not a very good article. but it has some nice references.

Keypoints

It is possible to aquire iris images in multiple way including Near Infrared (NIRD). a simple lense and monochome CCD camera, Adaboost cascade iris detector.

Iris localization is done by Daugman using a 2D Gabor Filter and Fisher Linear Discriminate method.

To help localize the iris despite of eyelashes a 1D rankfilter and histogram filter can be used.},
author = {Saha, Rishmita and Kundu, Mahasweta and Dutta, Madhuparna and Majumder, Rahul and Mukherjee, Debosmita and Pramanik, Sayak and Thakur, Uttam Narendra and Mukherjee, Chiradeep},
file = {::},
isbn = {9781538633717},
journal = {Information Technology, Electronics and Mobile Communication Conference (IEMCON), 2017 8th IEEE Annual},
pages = {685--688},
title = {{A Brief Study on Evolution of Iris Recognition System}},
url = {http://ieeexplore.ieee.org.ezproxy.psu.edu.sa/stamp/stamp.jsp?arnumber=8117234},
year = {2017}
}
@article{Ross2003,
annote = {Nice and clear overview of the four basic modules of the standard biometric system.

Very clear explainations of different aspects, but a bit limited in respect to the separation of methods into categories .},
author = {Ross, Arun and Jain, Anil},
file = {::},
isbn = {1517355931},
keywords = {biometrics,decision tree,face,fingerprints,hand geometry,linear discriminant analysis,multimodal,sum rule,verification},
number = {13},
pages = {2115--2125},
title = {{Information Fusion in Biometrics}},
volume = {24},
year = {2003}
}
@article{Zhang2017a,
abstract = {Multimodal classification arises in many computer vi-sion tasks such as object classification and image retrieval. The idea is to utilize multiple sources (modalities) measur-ing the same instance to improve the overall performance compared to using a single source (modality). The varying characteristics exhibited by multiple modalities make it nec-essary to simultaneously learn the corresponding distance metrics. In this paper, we propose a multiple metrics learn-ing algorithm for multimodal data. Metric of each modal-ity is product of two matrices: one matrix is modality spe-cific, the other is enforced to be shared by all the modalities. The learned metrics can improve multimodal classification accuracy and experimental results on four datasets show that the proposed algorithm outperforms existing learning algorithms based on multiple metrics as well as other ap-proaches tested on these datasets. Specifically, we report 95.0{\%} object instance recognition accuracy, 89.2{\%} object category recognition accuracy on the multi-view RGB-D dataset and 52.3{\%} scene category recognition accuracy on SUN RGB-D dataset.},
author = {Zhang, Heng and Patel, Vishal M. and Chellappa, Rama},
doi = {10.1109/CVPR.2017.312},
file = {::},
isbn = {978-1-5386-0457-1},
issn = {1063-6919},
journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {2925--2933},
title = {{Hierarchical Multimodal Metric Learning for Multimodal Classification}},
url = {http://ieeexplore.ieee.org/document/8099795/},
year = {2017}
}
@article{Wan2017a,
abstract = {State-of-the-art methods for 3D hand pose estimation from depth images require large amounts of annotated training data. We propose to model the statistical relationships of 3D hand poses and corresponding depth images using two deep generative models with a shared latent space. By design, our architecture allows for learning from unlabeled image data in a semi-supervised manner. Assuming a one-to-one mapping between a pose and a depth map, any given point in the shared latent space can be projected into both a hand pose and a corresponding depth map. Regressing the hand pose can then be done by learning a discriminator to estimate the posterior of the latent pose given some depth map. To improve generalization and to better exploit unlabeled depth maps, we jointly train a generator and a discriminator. At each iteration, the generator is updated with the back-propagated gradient from the discriminator to synthesize realistic depth maps of the articulated hand, while the discriminator benefits from an augmented training set of synthesized and unlabeled samples. The proposed discriminator network architecture is highly efficient and runs at 90 FPS on the CPU with accuracies comparable or better than state-of-art on 3 publicly available benchmarks.},
archivePrefix = {arXiv},
arxivId = {1702.03431},
author = {Wan, Chengde and Probst, Thomas and {Van Gool}, Luc and Yao, Angela},
doi = {10.1109/CVPR.2017.132},
eprint = {1702.03431},
file = {::},
journal = {Cvpr2017},
pages = {10},
title = {{Crossing Nets: Dual Generative Models with a Shared Latent Space for Hand Pose Estimation}},
url = {http://arxiv.org/abs/1702.03431},
year = {2017}
}
@article{Zhao2017a,
annote = {Good article describing a lot of the state of the art deep learning approaches being researched.

Keynotes:

Not much work with deep learning has been done in iris recognition.

THey Use a Fully Convolutional Network (FCN) and talk about others who ahve used a Convolutional Neural Network (CNN). They also mention a Deep Belief Net (DBN) that others have used. DeepIrisNet is also mentioned and tested with

THey have created their own loss function optimized for iris recognition called Extended Triplet Loss (ETL)

Their network is generalizable to other databases meaning that it doesn't require finetuing as many others do.

Used ND-IRIS, CASIA Iris, IITD Iris and WVU Non-Ideal Iris databases to test on.},
author = {Zhao, Zijing and Kumar, Ajay},
doi = {10.1109/ICCV.2017.411},
file = {::},
isbn = {978-1-5386-1032-9},
journal = {2017 IEEE International Conference on Computer Vision (ICCV)},
pages = {3829--3838},
title = {{Towards More Accurate Iris Recognition Using Deeply Learned Spatially Corresponding Features}},
url = {http://ieeexplore.ieee.org/document/8237673/},
year = {2017}
}
@article{Soviany2016,
abstract = {– This paper presents a design approach of a reliable authentication system for mobile applications (such as those within m-Health or m-Banking areas). This means that the biometric data processing should optimize the security performance vs. the computational complexity. The security is given by the combination of fingerprint, iris and voice features that define the multimodal pattern of an individual. The complexity reduction is supported by a reduced feature space, especially for the fingerprint and iris recognition components of the overall system.},
author = {Soviany, Sorin and Săndulescu, Virginia and Puşcoci, Sorin},
file = {::},
isbn = {9781509020478},
journal = {Computers and Artificial Intelligence},
keywords = {-multimodal,biometrics,data fusion,identification},
title = {{A Multimodal Biometric Identification Method for Mobile Applications Security}},
volume = {30},
year = {2016}
}
@article{Daugman1993,
author = {Daugman, J.G.},
doi = {10.1109/34.244676},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {11},
pages = {1148--1161},
title = {{High confidence visual recognition of persons by a test of statistical independence}},
url = {http://ieeexplore.ieee.org/document/244676/},
volume = {15},
year = {1993}
}
@article{BiosecID2008,
abstract = {A new multimodal biometric database, acquired in the framework of the BiosecurID project, is presented together with the description of the acquisition setup and protocol. The database includes eight unimodal biometric traits, namely: speech, iris, face (still images, videos of talking faces), handwritten signature and handwritten text (on-line dynamic signals, off-line scanned images), fingerprints (acquired with two different sensors), hand (palmprint, contour-geometry) and keystroking. The database comprises 400 subjects and presents features such as: realistic acquisition scenario, balanced gender and population distributions, availability of information about particular demographic groups (age, gender, handedness), acquisition of replay attacks for speech and keystroking, skilled forgeries for signatures, and compatibility with other existing databases. All these characteristics make it very useful in research and development of unimodal and multimodal biometric systems. {\textcopyright} Springer-Verlag London Limited 2009.},
author = {Fierrez, J. and Galbally, J. and Ortega-Garcia, J. and Freire, M. R. and Alonso-Fernandez, F. and Ramos, D. and Toledano, D. T. and Gonzalez-Rodriguez, J. and Siguenza, J. A. and Garrido-Salas, J. and Anguiano, E. and Gonzalez-de-Rivera, G. and Ribalda, R. and Faundez-Zanuy, M. and Ortega, J. A. and Carde{\~{n}}oso-Payo, V. and Viloria, A. and Vivaracho, C. E. and Moro, Q. I. and Igarza, J. J. and Sanchez, J. and Hernaez, I. and Orrite-Uru{\~{n}}uela, C. and Martinez-Contreras, F. and Gracia-Roche, J. J.},
doi = {10.1007/s10044-009-0151-4},
file = {::},
issn = {14337541},
journal = {Pattern Analysis and Applications},
keywords = {Biometrics,Database,Face,Fingerprint,Hand geometry,Handwriting,Iris,Keystroking,Multimodal,Palmprint,Signature,Speech},
number = {2},
pages = {235--246},
title = {{BiosecurID: A multimodal biometric database}},
volume = {13},
year = {2010}
}
@article{Crossdata2018,
abstract = {In this paper we study face recognition using convolutional neural network. First, we introduced the basic CNN neural network architecture. Second, we modify the traditional neural network and adapt it to another database by fine tuning its parameters. Third, the network architecture is extended to the cross database problem. The CNN is first trained on a large dataset and then tested on another. Experimental results show that the proposed algorithm is suitable for building various real world applications.},
author = {Guo, Mei and Xiao, Min and Gong, Deliang},
doi = {10.1007/978-3-319-69096-4_54},
file = {:home/nicstar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo, Xiao, Gong - 2017 - Face Recognition Using Deep Convolutional Neural Network in Cross-Database Study.pdf:pdf},
journal = {Advances in Intelligent Systems and Computing},
keywords = {Deep neural network {\'{A}},Face recognition {\'{A}},Image processing},
title = {{Face Recognition Using Deep Convolutional Neural Network in Cross-Database Study}},
url = {https://link-springer-com.zorac.aub.aau.dk/content/pdf/10.1007{\%}2F978-3-319-69096-4{\_}54.pdf},
year = {2017}
}
@inproceedings{Trokielewicz2016,
author = {Trokielewicz, Mateusz},
booktitle = {2016 IEEE International Conference on Identity, Security and Behavior Analysis (ISBA)},
doi = {10.1109/ISBA.2016.7477233},
isbn = {978-1-4673-9727-8},
month = {feb},
pages = {1--6},
publisher = {IEEE},
title = {{Iris recognition with a database of iris images obtained in visible light using smartphone camera}},
url = {http://ieeexplore.ieee.org/document/7477233/},
year = {2016}
}
@article{Karpathy2016,
abstract = {These notes accompany the Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition.},
author = {Karpathy, Andrej},
journal = {Stanford University},
pages = {1--21},
title = {{Convolutional Neural Networks for Visual Recognition}},
url = {http://cs231n.github.io/classification/},
year = {2016}
}
@article{Mellakh2009a,
author = {Mellakh, A and Chaari, A and Guerfi, S and Dhose, J and Colineau, J and Lelandais, S and Petrovska-Delacr{\`{e}}taz, D and Dorizzi, B},
doi = {10.1007/978-3-642-04697-1_3},
file = {::},
isbn = {03029743; 3642046967 (ISBN); 9783642046964 (ISBN)},
issn = {03029743},
journal = {11th International Conference on Advanced Concepts for Intelligent Vision Systems, ACIVS 2009},
keywords = {2D face recognition,Appearance based,Appearance-based algorithms,Computer vision,Database,Database systems,Discriminant analysis,Evaluation campaign,Experimental protocols,Face images,Face recognition,Linear discriminant analysis,Multi-modal,Multimodal database,Principal component analysis,Training sets},
pages = {24--32},
title = {{2D face recognition in the IV2 evaluation campaign}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-70549098348{\&}partnerID=40{\&}md5=0541be27bb29a037c965f0d644b15856},
volume = {5807 LNCS},
year = {2009}
}
@article{Khan2017a,
annote = {A good article that uses a new database containing smartphone iris images. Theses are taken in visible light. They use Daugmans approach to localize the iris, then they normalize it. They use wavelets on the image to extract the desired featues and then try to classify them using SVM (97{\%}), KNN (95.1{\%}) and LDA (94.28{\%}).

Keynotes:

A different study has used Sparse Reconstruction Classifier with K-means clustering

A different study obtained 99{\%} accuracy using SVM and Hamming Distance

They tried SVM, K-means, Linear Discrimintant in their own study as classifiers.},
author = {Khan, Fahim Faysal and Akif, Ahnaf and Haque, M A},
file = {::},
isbn = {9781538633748},
pages = {26--28},
title = {{Iris Recognition using Machine Learning from Smartphone Captured Images in Visible Light}},
year = {2017}
}
@article{Luhadiya2017a,
annote = {Really good article with a summery of Iris recognition history and approaches in the introduction part.


Keynotes:

Iris database called CASIA with 756 images of 108 people.

SVM used to classify irises.

Elman Recurrent Neural Netowrk used.},
author = {Luhadiya, Ruchi and Khedkar, Anagha},
doi = {10.1109/ICAECCT.2016.7942619},
file = {::},
isbn = {9781509036622},
journal = {2016 IEEE International Conference on Advances in Electronics, Communication and Computer Technology, ICAECCT 2016},
keywords = {GLCM,Hough circular transform,Iris,Machine learning,Person identification,SVM},
pages = {387--392},
title = {{Iris detection for person identification using multiclass SVM}},
year = {2017}
}
@article{Schiel2002,
abstract = {In this contribution we announce and describe in detail the new multimodal corpus evolving from the publicly funded German SmartKom project. The first release of the corpus (BAS SK-P 1.0) has been finished end of 2001 and will be ready for distribution to the scientific community in July 2002. The SmartKom corpus will be the first of a new generation of Language Resources (LR) designed for a more or less complete data gathering of human-machine communication combining acoustic, visual and tactile input and output modalities. Since the funding of about EU 2 Mio for this LR is 100{\%} public, the corpus will be available without royalties via the Bavarian Archive for Speech Signals (BAS) at the University of Munich.},
author = {Schiel, Florian and Steininger, Silke and T{\"{u}}rk, Ulrich},
file = {::},
journal = {Proceedings of the 3rd Language Resources and Evaluation},
number = {34},
pages = {200--2006},
title = {{The SmartKom Multimodal Corpus at BAS}},
year = {2002}
}
@article{Kupfer1984,
author = {Kupfer, D J and Ulrich, R F and Coble, P A and Jarnatt, D B and Grochocinski, V and Doman, J and Matthews, G and Borbely, A A},
file = {::},
journal = {Psychiatry Res},
keywords = {ANALYSIS,Depression,Human,REM,Sleep},
pages = {335--343},
title = {{Application of automated REM and slow wave analysis: I normal and depressive subjects}},
volume = {13},
year = {1984}
}
@article{Rifaee2017,
author = {Rifaee, Mustafa and Abdallah, Mohammad and Okosh, Basem},
file = {::},
journal = {international journal of multimedia {\&} its applications},
number = {April},
title = {{A Short Survey for Iris Images Databases}},
url = {https://www.researchgate.net/publication/316093004{\_}A{\_}Short{\_}Survey{\_}for{\_}Iris{\_}Images{\_}Databases},
year = {2017}
}
@article{Kuehlkamp2016a,
abstract = {Iris recognition systems are a mature technology that is widely used throughout the world. In identification (as opposed to verification) mode, an iris to be recognized is typically matched against all N enrolled irises. This is the classic "1-to-N search". In order to improve the speed of large-scale identification, a modified "1-to-First" search has been used in some operational systems. A 1-to-First search terminates with the first below-threshold match that is found, whereas a 1-to-N search always finds the best match across all enrollments. We know of no previous studies that evaluate how the accuracy of 1-to-First search differs from that of 1-to-N search. Using a dataset of over 50,000 iris images from 2,800 different irises, we perform experiments to evaluate the relative accuracy of 1-to-First and 1-to-N search. We evaluate how the accuracy difference changes with larger numbers of enrolled irises, and with larger ranges of rotational difference allowed between iris images. We find that False Match error rate for 1-to-First is higher than for 1-to-N, and the the difference grows with larger number of enrolled irises and with larger range of rotation.},
annote = {The way that iris recognition works is that some kind of filter is applied to localize the iris. 2D Gador filter is often cited. Then that iris is checked against the whole database. This is called 1:N. They check the Hamming Distance between of the bits of the data and then choose the lowest ones as a pair. In 1:First search they do the same except there is is a threshold that that is has to be under to be accepted. If it is under that threshold it is accepted and the search is stopped. Two types of error can occour: a False Match (FM) and False Non-Match (FNM). A false match occurs when two samples from different individuals are declared by the system as a match. A false non-match is when two samples from the same individual fail to be considered as a match by the system},
archivePrefix = {arXiv},
arxivId = {1702.01167},
author = {Kuehlkamp, Andrey and Bowyer, Kevin W.},
doi = {10.1109/WACV.2016.7477687},
eprint = {1702.01167},
file = {::},
isbn = {9781509006410},
journal = {2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016},
title = {{An analysis of 1-to-first matching in iris recognition}},
year = {2016}
}
@article{Nam2014,
abstract = {We present a novel human-machine interface, called GOM-Face , and its application to humanoid robot control. The GOM-Face bases its interfacing on three electric potentials measured on the face: 1) glossokinetic potential (GKP), which involves the tongue movement; 2) electrooculogram (EOG), which involves the eye movement; 3) electromyogram, which involves the teeth clenching. Each potential has been individually used for assistive interfacing to provide persons with limb motor disabilities or even complete quadriplegia an alternative communication channel. However, to the best of our knowledge, GOM-Face is the first interface that exploits all these potentials together. We resolved the interference between GKP and EOG by extracting discriminative features from two covariance matrices: a tongue-movement-only data matrix and eye-movement-only data matrix. With the feature extraction method, GOM-Face can detect four kinds of horizontal tongue or eye movements with an accuracy of 86.7{\%} within 2.77 s. We demonstrated the applicability of the GOM-Face to humanoid robot control: users were able to communicate with the robot by selecting from a predefined menu using the eye and tongue movements.},
author = {Nam, Yunjun and Koo, Bonkon and Cichocki, Andrzej and Choi, Seungjin},
doi = {10.1109/TBME.2013.2280900},
file = {::},
issn = {00189294},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {Electromyogram (EMG),electrooculogram (EOG),glossokinetic potentials (GKP),human-machine interface,multimodal interface},
number = {2},
pages = {453--462},
pmid = {24021635},
title = {{GOM-face: GKP, EOG, and EMG-based multimodal interface with application to humanoid robot control}},
volume = {61},
year = {2014}
}
@inproceedings{Eitel2015,
abstract = {Robust object recognition is a crucial ingredient of many, if not all, real-world robotics applications. This paper leverages recent progress on Convolutional Neural Networks (CNNs) and proposes a novel RGB-D architecture for object recognition. Our architecture is composed of two separate CNN processing streams - one for each modality - which are consecutively combined with a late fusion network. We focus on learning with imperfect sensor data, a typical problem in real-world robotics tasks. For accurate learning, we introduce a multi-stage training methodology and two crucial ingredients for handling depth data with CNNs. The first, an effective encoding of depth information for CNNs that enables learning without the need for large depth datasets. The second, a data augmentation scheme for robust learning with depth images by corrupting them with realistic noise patterns. We present state-of-the-art results on the RGB-D object dataset and show recognition in challenging RGB-D real-world noisy settings.},
archivePrefix = {arXiv},
arxivId = {1507.06821},
author = {Eitel, Andreas and Springenberg, Jost Tobias and Spinello, Luciano and Riedmiller, Martin and Burgard, Wolfram},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2015.7353446},
eprint = {1507.06821},
file = {::},
isbn = {9781479999941},
issn = {21530866},
month = {jul},
pages = {681--687},
title = {{Multimodal deep learning for robust RGB-D object recognition}},
url = {http://arxiv.org/abs/1507.06821},
volume = {2015-Decem},
year = {2015}
}
@article{Fierrez-Aguilar2003a,
abstract = {The aim of this paper, regarding multimodal biometric verification, is twofold: on the one hand, some score fusion strategies reported in the literature are reviewed and, on the other hand, we compare experimentally a selection of them using as monomodal baseline experts: i) our face verification system based on a global face appearance representation scheme, ii) our minutiaebased fingerprint verification system, and iii) our on-line signature verification system based on HMM modeling of temporal functions, on the MCYT multimodal database. A new strategy is also proposed and discussed in order to generate a multimodal combined score by means of Support Vector Machine (SVM) classifiers from which user-independent and user-dependent fusion schemes are derived and evaluated.},
author = {Fierrez-Aguilar, J and Ortega-Garcia, J and Garcia-Romero, D and Gonzalez-Rodriguez, J},
file = {::},
isbn = {3-540-40302-7},
issn = {03029743},
journal = {Proc. AVBPA},
pages = {1056},
title = {{A Comparative Evaluation of Fusion Strategies for Multimodal Biometric Verification}},
year = {2003}
}
@article{sun2015,
abstract = {The state-of-the-art of face recognition has been significantly advanced by the emergence of deep learning. Very deep neural networks recently achieved great success on general object recognition because of their superb learning capacity. This motivates us to investigate their effectiveness on face recognition. This paper proposes two very deep neural network architectures, referred to as DeepID3, for face recognition. These two architectures are rebuilt from stacked convolution and inception layers proposed in VGG net and GoogLeNet to make them suitable to face recognition. Joint face identification-verification supervisory signals are added to both intermediate and final feature extraction layers during training. An ensemble of the proposed two architectures achieves 99.53{\%} LFW face verification accuracy and 96.0{\%} LFW rank-1 face identification accuracy, respectively. A further discussion of LFW face verification result is given in the end.},
archivePrefix = {arXiv},
arxivId = {1502.00873},
author = {Sun, Yi and Liang, Ding and Wang, Xiaogang and Tang, Xiaoou},
eprint = {1502.00873},
file = {::},
title = {{DeepID3: Face Recognition with Very Deep Neural Networks}},
url = {https://arxiv.org/pdf/1502.00873.pdf http://arxiv.org/abs/1502.00873},
year = {2015}
}
@article{Daia,
abstract = {Deep networks have shown impressive performance on many computer vision tasks. Recently, deep convolutional neural networks (CNNs) have been used to learn discrim-inative texture representations. One of the most successful approaches is Bilinear CNN model that explicitly captures the second order statistics within deep features. However, these networks cut off the first order information flow in the deep network and make gradient back-propagation dif-ficult. We propose an effective fusion architecture -FASON that combines second order information flow and first or-der information flow. Our method allows gradients to back-propagate through both flows freely and can be trained ef-fectively. We then build a multi-level deep architecture to exploit the first and second order information within dif-ferent convolutional layers. Experiments show that our method achieves improvements over state-of-the-art meth-ods on several benchmark datasets.},
author = {Dai, Xiyang and Ng, Joe Yue-hei and Davis, Larry S},
file = {::},
journal = {Cvpr2017},
pages = {7352--7360},
title = {{FASON : First and Second Order Information Fusion Network for Texture Recognition}}
}
@article{Ortega-Garcia2010,
abstract = {A new multimodal biometric database designed and acquired within the framework of the European BioSecure Network of Excellence is presented. It is comprised of more than 600 individuals acquired simultaneously in three scenarios: 1) over the Internet, 2) in an office environment with desktop PC, and 3) in indoor/outdoor environments with mobile portable hardware. The three scenarios include a common part of audio/video data. Also, signature and fingerprint data have been acquired both with desktop PC and mobile portable hardware. Additionally, hand and iris data were acquired in the second scenario using desktop PC. Acquisition has been conducted by 11 European institutions. Additional features of the BioSecure Multimodal Database (BMDB) are: two acquisition sessions, several sensors in certain modalities, balanced gender and age distributions, multimodal realistic scenarios with simple and quick tasks per modality, cross-European diversity, availability of demographic data, and compatibility with other multimodal databases. The novel acquisition conditions of the BMDB allow us to perform new challenging research and evaluation of either monomodal or multimodal biometric systems, as in the recent BioSecure Multimodal Evaluation campaign. A description of this campaign including baseline results of individual modalities from the new database is also given. The database is expected to be available for research purposes through the BioSecure Association during 2008.},
author = {Ortega-Garcia, Javier and Fierrez, Julian and Alonso-Fernandez, Fernando and Galbally, Javier and Freire, Manuel R. and Gonzalez-Rodriguez, Joaquin and Garcia-Mateo, Carmen and Alba-Castro, Jose Luis and Gonzalez-Agulla, Elisardo and Otero-Muras, Enrique and Garcia-Salicetti, Sonia and Allano, Lorene and Ly-Van, Bao and Dorizzi, Bernadette and Kittler, Josef and Bourlai, Thirimachos and Poh, Norman and Deravi, Farzin and Ng, Ming N.R. and Fairhurst, Michael and Hennebert, Jean and Humm, Andreas and Tistarelli, Massimo and Brodo, Linda and Richiardi, Jonas and Drygajlo, Andrezj and Ganster, Harald and Sukno, Federico M. and Pavani, Sri Kaushik and Frangi, Alejandro and Akarun, Lale and Savran, Arman},
doi = {10.1109/TPAMI.2009.76},
file = {::},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Benchmark,Biometrics,Database,Evaluation,Face,Fingerprint,Hand,Iris.,Multimodal,Performance,Signature,Speaker,Voice},
number = {6},
pages = {1097--1111},
pmid = {20431134},
title = {{The multiscenario multienvironment biosecure multimodal database (BMDB)}},
volume = {32},
year = {2010}
}
@misc{LiborMasek2003,
annote = {Open Source Matlab Iris Recognition system. Based on Daugmans approach.},
author = {{Libor Masek}, Peter Kovesi},
publisher = {The School of Computer Science and Software Engineering, The University of Western Australia},
title = {{MATLAB Source Code for a Biometric Identification System Based on Iris Patterns}},
url = {http://www.peterkovesi.com/studentprojects/libor/sourcecode.html},
year = {2003}
}
@article{lfw2007,
abstract = {Face recognition has benefitted greatly from the many databases that have been produced to study it. Most of these databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, expression, background, camera quality, occlusion, age, and gender. While there are many applications for face recognition technol- ogy in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database is provided as an aid in studying the latter, unconstrained, face recognition problem. The database represents an initial attempt to provide a set of labeled face photographs spanning the range of conditions typically encountered by people in their everyday lives. The database exhibits natural variability in pose, lighting, focus, resolution, facial expression, age, gender, race, accessories, make-up, occlusions, background, and photographic quality. Despite this variability, the images in the database are presented in a simple and consistent format for maximum ease of use. In addition to describing the details of the database and its acquisition, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible.},
author = {Huang, Gary B and Ramesh, Manu and Berg, Tamara and Learned-Miller, Erik},
doi = {10.1.1.122.8268},
file = {::},
isbn = {9781628414844},
issn = {1996756X},
journal = {University of Massachusetts Amherst Technical Report},
pages = {07--49},
title = {{Labeled faces in the wild: A database for studying face recognition in unconstrained environments}},
url = {http://vis-www.cs.umass.edu/lfw/lfw.pdf},
volume = {1},
year = {2007}
}
@article{Krizhevsky2017,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0 {\%} which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2 {\%} achieved by the second-best entry. 1},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
doi = {10.1145/3065386},
eprint = {1102.0183},
isbn = {9781627480031},
issn = {00010782},
journal = {Communications of the ACM},
month = {may},
number = {6},
pages = {84--90},
pmid = {7491034},
publisher = {ACM},
title = {{ImageNet classification with deep convolutional neural networks}},
url = {http://dl.acm.org/citation.cfm?doid=3098997.3065386},
volume = {60},
year = {2017}
}
@article{Ribeiro2017a,
author = {Ribeiro, Eduardo and Uhl, Andreas and Alonso-Fernandez, Fernando and Farrugia, Reuben A.},
doi = {10.23919/EUSIPCO.2017.8081595},
file = {::},
isbn = {978-0-9928626-7-1},
journal = {2017 25th European Signal Processing Conference (EUSIPCO)},
pages = {2176--2180},
title = {{Exploring deep learning image super-resolution for iris recognition}},
url = {http://ieeexplore.ieee.org/document/8081595/},
volume = {2},
year = {2017}
}
@book{Wechsler2007,
abstract = {One of the grand challenges for computational intelligence and biometrics is to understand how people process and recognize faces and to develop automated and reliable face recognition systems. Biometrics has become the major component in the complex decision making process associated with security applications. The face detection and authentication challenges addressed include cluttered environments, image variability, occlusion and disguise, and temporal changes all within open set recognition. Reliable Face Recognition Methods: System Design, Implementation and Evaluation comprehensively explores the face recognition problem while drawing inspiration from complementary disciplines such as neurosciences, statistics, signal and image processing, computer vision, and machine learning and pattern recognition. This book also examines the evolution of face recognition research and explores promising new avenues for research and development. Reliable Face Recognition Methods: System Design, Implementation and Evaluation benefits graduate-level students, researchers, and practitioners, as well as government and industry decision makers in the security arena. Ruud Bolle (IBM): "Harry Wechsler's monograph provides a thorough, up-to-date, and in-depth overview of the many advances in the area of face recognition. gives an excellent overview of the issues related to security and privacy when it comes to automated biometrics. In summary, this book has the potential to become a classic. Harry Wechsler is to be commended for undertaking the monumental task of writing this book." John Daugman (Cambridge University, UK): "The book looks excellent. The topic is timely, and the perspective-multi-disciplinary, measured, and objective - is much needed and welcome. I believe that Wechsler's latest book will make a valued contribution to this important field and will become a standard." David Zhang (Hong Kong Polytechnic University, China): "From a system view, this book shows an excellent arrangement, from data collection to face recognition, as well as system performance evaluation and error analysis. This book can serve both as an interdisciplinary text and a research reference. Each chapter provides the background and impetus for understanding the problems discussed." Stan Li (Chinese Academy of Sciences, China): "The book treats the subject of face recognition in a fairly systematic way. The book covers most relevant topics in face recognition, in a well organized way. The information is also useful for experts. The manuscript is easy to read." Tom Huang (University of Illinois, USA): "Harry Wechsler is without doubt one of the leading authorities in face recognition and related topics. We have recently seen a number of excellent edited books on Face Recognition. However, Wechsler's book is the first unified treatment of this important subject. In my opinion, Wechsler is one of only a handful of people in the world who could write such a comprehensive, unified, informative, perceptive, and authoritative book on Face Recognition. Harry Wechsler so far, is the only one of this handful who took the time and effort to realize such a project. The book certainly will have a great positive impact on the biometrics research community. But also, because it looks at Face Recognition from different perspectives, it will be welcomed by non-experts." {\textcopyright} 2007 Springer Science+Business Media, LLC. All rights reserved.},
address = {Boston, MA},
author = {Wechsler, Harry},
booktitle = {Reliable Face Recognition Methods: System Design, Impementation and Evaluation},
doi = {10.1007/978-0-387-38464-1},
file = {::},
isbn = {038722372X},
pages = {1--329},
publisher = {Springer US},
title = {{Reliable face recognition methods: System design, implementation and evaluation}},
url = {http://link.springer.com/10.1007/978-0-387-38464-1 https://link-springer-com.zorac.aub.aau.dk/book/10.1007{\%}2F978-0-387-38464-1{\#}about},
year = {2007}
}
@article{Lee2017,
abstract = {In recent years, the iris recognition system has been gaining increasing acceptance for applications such as access control and smartphone security. When the images of the iris are obtained under unconstrained conditions, an issue of undermined quality is caused by optical and motion blur, off-angle view (the user's eyes looking somewhere else, not into the front of the camera), specular reflection (SR) and other factors. Such noisy iris images increase intra-individual variations and, as a result, reduce the accuracy of iris recognition. A typical iris recognition system requires a near-infrared (NIR) illuminator along with an NIR camera, which are larger and more expensive than fingerprint recognition equipment. Hence, many studies have proposed methods of using iris images captured by a visible light camera without the need for an additional illuminator. In this research, we propose a new recognition method for noisy iris and ocular images by using one iris and two periocular regions, based on three convolutional neural networks (CNNs). Experiments were conducted by using the noisy iris challenge evaluation-part II (NICE.II) training dataset (selected from the university of Beira iris (UBIRIS).v2 database), mobile iris challenge evaluation (MICHE) database, and institute of automation of Chinese academy of sciences (CASIA)-Iris-Distance database. As a result, the method proposed by this study outperformed previous methods.},
author = {Lee, Min Beom and Hong, Hyung Gil and Park, Kang Ryoung},
doi = {10.3390/s17122933},
file = {::},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Convolutional neural network,Iris and periocular,Noisy iris and ocular image},
number = {12},
pmid = {29258217},
title = {{Noisy ocular recognition based on three convolutional neural networks}},
volume = {17},
year = {2017}
}
@article{B2017,
author = {B, Qi Wang and Su, Xia and Cai, Zhenlin and Zhang, Xiangde},
doi = {10.1007/978-3-319-69923-3},
file = {::},
isbn = {978-3-319-69922-6},
keywords = {joint bayesian,mobile iris recognition,ordinal measures},
number = {3},
pages = {401--410},
title = {{Biometric Recognition}},
url = {http://link.springer.com/10.1007/978-3-319-69923-3},
volume = {10568},
year = {2017}
}
@article{Li2017,
abstract = {This paper investigates how to integrate the complementary information from RGB and thermal (RGB-T) sources for object tracking. We propose a novel Convolutional Neural Network (ConvNet) architecture, including a two-stream ConvNet and a FusionNet, to achieve adaptive fusion of different source data for robust RGB-T tracking. Both RGB and thermal streams extract generic semantic information of the target object. In particular, the thermal stream is pre-trained on the ImageNet dataset to encode rich semantic information, and then fine-tuned using thermal images to capture the specific properties of thermal information. For adaptive fusion of different modalities while avoiding redundant noises, the FusionNet is employed to select most discriminative feature maps from the outputs of the two-stream ConvNet, and updated online to adapt to appearance variations of the target object. Finally, the object locations are efficiently predicted by applying the multi-channel correlation filter on the fused feature maps. Extensive experiments on the recently public benchmark GTOT verify the effectiveness of the proposed approach against other state-of-the-art RGB-T trackers.},
author = {Li, Chenglong and Wu, Xiaohao and Zhao, Nan and Cao, Xiaochun and Tang, Jin},
doi = {10.1016/j.neucom.2017.11.068},
file = {::},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Adaptive fusion,Convolutional neural network,Correlation filter,Object tracking,Thermal information},
pages = {78--85},
publisher = {Elsevier B.V.},
title = {{Fusing two-stream convolutional neural networks for RGB-T object tracking}},
url = {https://doi.org/10.1016/j.neucom.2017.11.068},
volume = {281},
year = {2017}
}
@article{Neves2017a,
abstract = {{\textcopyright} 2017 IEEE. An error-correcting code (ECC) is a process of adding redundant data to a message, such that it can be recovered by a receiver even if a number of errors are introduced in transmission. Inspired by the principles of ECC, we introduce a method capable of detecting degraded features in biometric signatures by exploiting feature correlation. The main novelty is that, unlike existing biometric cryptosystems, the proposed method works directly on the biometric signature. Our approach performs a redundancy analysis of non-degraded data to build an undirected graphical model (Markov Random Field), whose energy minimization determines the sequence of degraded components of the biometric sample. Experiments carried out in different biometric traits ascertain the improvements attained when disregarding degraded features during the matching phase. Also, we stress that the proposed method is general enough to work in different classification methods, such as CNNs.},
author = {Neves, Joao and Proenca, Hugo},
doi = {10.1109/FG.2017.122},
file = {::},
isbn = {9781509040230},
journal = {Proceedings - 12th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2017 - 1st International Workshop on Adaptive Shot Learning for Gesture Understanding and Production, ASL4GUP 2017, Biometrics in the Wild, Bwild 2017, Heteroge},
pages = {981--986},
title = {{Exploiting Data Redundancy for Error Detection in Degraded Biometric Signatures Resulting from in the Wild Environments}},
year = {2017}
}
@article{Mhaske2013a,
author = {Mhaske, V. D. and Patankar, A. J.},
doi = {10.1109/ICCIC.2013.6724125},
file = {::},
isbn = {9781479915972},
journal = {2013 IEEE International Conference on Computational Intelligence and Computing Research, IEEE ICCIC 2013},
keywords = {Biometrics,Fingerprint,Fusion,MGF,Multimodal,Palm print,ROI,Unimodal},
title = {{Multimodal biometrics by integrating fingerprint and palmprint for security}},
year = {2013}
}
@article{Wrobel2017a,
abstract = {{\textcopyright} 2017 IEEE. In this paper, a new approach for personal identity verification using finger knuckle images and least-square contour alignment method has been proposed. A special test rig with a digital camera was prepared for acquisition the knuckle images. Next, the obtained images of finger knuckle were subjected to image processing method in order to extract the knuckle furrows from them. The verification of person was performed by comparing the furrows on the verified and the reference knuckle images. To determine the similarity between the furrows we used the least-square contour alignment method. The usability of the proposed approach was tested experimentally. Practical experiments, conducted with our database, confirmed that results obtained are promising.},
author = {Wrobel, K. and Porwik, P. and Doroz, R. and Safaverdi, H.},
doi = {10.1109/ICBAKE.2017.8090616},
file = {::},
isbn = {9781538634004},
journal = {Proceedings of 2017 International Conference on Biometrics and Kansei Engineering, ICBAKE 2017},
keywords = {biometrics,finger knuckle images,least-square contour alignment,verification},
pages = {119--122},
title = {{Person verification based on finger knuckle images and least-squares contour alignment}},
year = {2017}
}
@inproceedings{Sun2014,
abstract = {The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset, 99.15{\%} face verification accuracy is achieved. Compared with the best deep learning result on LFW, the error rate has been significantly reduced by 67{\%}.},
archivePrefix = {arXiv},
arxivId = {1406.4773},
author = {Sun, Yi and Wang, Xiaogang and Tang, Xiaoou},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.244},
eprint = {1406.4773},
file = {::},
isbn = {9781479951178},
issn = {10636919},
keywords = {deep learning,face verification},
pages = {1891--1898},
pmid = {21808091},
title = {{Deep Learning Face Representation by Joint Identification-Verification}},
url = {https://arxiv.org/pdf/1406.4773.pdf http://arxiv.org/abs/1406.4773},
year = {2014}
}
@article{Elrefaei2017,
author = {Elrefaei, Lamiaa A. and Hamid, Doaa H. and Bayazed, Afnan A. and Bushnak, Sara S. and Maasher, Shaikhah Y.},
doi = {10.1007/s11042-017-5049-3},
file = {::},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {Biometrics,Deep Sparse Filter,Hough transform,Iris recognition,Visible Light},
pages = {1--25},
publisher = {Multimedia Tools and Applications},
title = {{Developing Iris Recognition System for Smartphone Security}},
year = {2017}
}
@article{Zapata2017a,
author = {Zapata, J C and Duque, C M and Gonzalez, M E},
doi = {10.1007/978-981-10-5427-3},
file = {::},
isbn = {978-981-10-5426-6},
keywords = {biometric {\'{a}} data fusion,signals {\'{a}} signal processing},
number = {i},
pages = {721--733},
title = {{Advances in Computing and Data Sciences}},
url = {http://link.springer.com/10.1007/978-981-10-5427-3},
volume = {721},
year = {2017}
}
@article{Sung2013,
abstract = {This study uses ZigBee wireless sensor network technology to build a Multi-purpose Electronic Score system based on the electro-oculogram (EOG). As a requirement when learning to play a musical instrument the most commonly seen scores are available in paper copies. The disadvantage of the paper music score is that pages must be turned manually by the performer while playing music. An alternative is to prepare a miniaturized copy of the score, but this makes the score difficult to read clearly. Electronic scores have recently become available and are expected to become mainstream in the future due to the rapid growth in ebook readers and tablet PCs. Tracking the upward, downward, leftward, rightward and clockwise/counterclockwise eyeball movements and eye blinking, an amplified, bandpass filtered electro-oculogram (EOG) signal is converted into digital form. This signal is further transmitted through a ZigBee wireless module on which an electronic score program is installed, including the following operating modes; go to the previous/next page, recording, tuning and tempo modes. Detected eyeball movements allow a score to be viewed conveniently while playing. In the future, the proposed technology can be applied to flexible paper, flexible displays and the like. {\textcopyright} 2012 Elsevier B.V.},
author = {Sung, Wen Tsai and Chen, Jui Ho and Chang, Kuo Yi},
doi = {10.1016/j.sna.2012.11.028},
file = {::},
isbn = {0924-4247},
issn = {09244247},
journal = {Sensors and Actuators, A: Physical},
keywords = {EOG,Multipurpose Electronic Score,Wireless Sensor Network,ZigBee},
pages = {141--152},
publisher = {Elsevier B.V.},
title = {{ZigBee based multi-purpose electronic score design and implementation using EOG}},
url = {http://dx.doi.org/10.1016/j.sna.2012.11.028},
volume = {190},
year = {2013}
}
@article{Dessimoz2007,
abstract = {The MBioID initiative has been set up to address the following germane question: What and how biometric technologies could be deployed in identity documents in the foreseeable future? This research effort proposes to look at current and future practices and systems of establishing and using biometric identity documents (IDs) and evaluate their effectiveness in large-scale developments. The first objective of the MBioID project is to present a review document establishing the current state-of-the-art related to the use of multimodal biometrics in an IDs application. This research report gives the main definitions, properties and the framework of use related to biometrics, an overview of the main standards developed in the biometric industry and standardisation organisations to ensure interoperability, as well as some of the legal framework and the issues associated to biometrics such as privacy and personal data protection. The state-of-the-art in terms of technological development is also summarised for a range of single biometric modalities (2D and 3D face, fingerprint, iris, on-line signature and speech), chosen according to ICAO recommendations and availabilities, and for various multimodal approaches. This paper gives a summary of the main elements of that report. The second objective of the MBioID project is to propose relevant acquisition and evaluation protocols for a large-scale deployment of biometric IDs. Combined with the protocols, a multimodal database will be acquired in a realistic way, in order to be as close as possible to a real biometric IDs deployment. In this paper, the issues and solutions related to the acquisition setup are briefly presented. {\textcopyright} 2006 Elsevier Ireland Ltd. All rights reserved.},
author = {Dessimoz, Damien and Richiardi, Jonas and Champod, Christophe and Drygajlo, Andrzej},
doi = {10.1016/j.forsciint.2006.06.037},
file = {::},
issn = {03790738},
journal = {Forensic Science International},
keywords = {Acquisition protocol,Biometrics,Electronic passport,Evaluation protocol,Identity documents,Multimodality},
number = {2-3},
pages = {154--159},
pmid = {16890391},
title = {{Multimodal biometrics for identity documents ({\{}A figure is presented{\}})}},
volume = {167},
year = {2007}
}
@article{Berg1991,
author = {Berg, P and Scherg, M},
file = {::},
journal = {Clinical Physiology and Physiological Measures},
keywords = {ERP eye ocular},
pages = {49--54},
title = {{Dipole modelling of eye activity and its application to the removal of eye artifacts from the EEG and MEG}},
volume = {12},
year = {1991}
}
@article{Kim2013,
author = {Kim, Myoung Ro and Yoon, Gilwon},
file = {::},
journal = {International Journal of Electrical, Computer, Energetic, Electronic and Communication Engineering},
keywords = {ddr game,eog,eye movement},
number = {10},
pages = {1352--1355},
title = {{Control Signal from EOG Analysis and Its Application}},
volume = {7},
year = {2013}
}
@article{Galdi2017,
abstract = {FIRE is a Fast Iris REcognition algorithm especially designed for iris recognition on mobile phones under visible-light. It is based on the combination of three classifiers exploiting the iris colour and texture information. Its limited computational time makes FIRE particularly suitable for fast user verification on mobile devices. The high parallelism of the code allows its use also on large databases. FIRE, in its first version, was submitted to the Mobile Iris CHallenge Evaluation part II held in 2016. In this paper, FIRE is further improved: a number of different techniques has been analyzed and the best performing ones have been selected for fusion at score level. Performance are assessed in terms of Recognition Rate (RR), Area Under Receiver Operating Characteristic Curve (AUC), and Equal Error Rate (EER).},
author = {Galdi, Chiara and Dugelay, Jean Luc},
doi = {10.1016/j.patrec.2017.01.023},
file = {::},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Fast iris recognition,MICHE DB,MICHE II,Multi-classifier,Noisy iris recognition,Visible light},
pages = {44--51},
publisher = {Elsevier B.V.},
title = {{FIRE: Fast Iris REcognition on mobile phones by combining colour and texture features}},
url = {http://dx.doi.org/10.1016/j.patrec.2017.01.023},
volume = {91},
year = {2017}
}
@article{Dhillon2009,
abstract = {This paper discusses a brain-computer interface through electrooculogram (EOG) and electromyogram (EMG) signals. In situations of disease or trauma, there may be inability to communicate with others through means such as speech or typing. Eye movement tends to be one of the last remaining active muscle capabilities for people with neurodegenerative disorders, such as amyotrophic lateral sclerosis (ALS) also known as Lou Gehrig's disease. Thus, there is a need for eye movement based systems to enable communication. To meet this need, we proposed a system to accept eye-gaze controlled navigation of a particular letter and EMG based click to enter the letter. Eye -gaze direction (angle) is obtained from EOG signals and EMG signal is recorded from eyebrow muscle activity. A virtual screen keyboard may be used to examine the usability of the proposed system.},
author = {Dhillon, Hari Singh and Singla, Rajesh and Rekhi, Navleen Singh and Jha, Rameshwar},
doi = {10.1109/ICCSIT.2009.5234951},
file = {::},
isbn = {9781424445196},
journal = {Proceedings - 2009 2nd IEEE International Conference on Computer Science and Information Technology, ICCSIT 2009},
keywords = {Amyotrophic lateral sclerosis (ALS),EMG,EOG,Eye-gaze,Virtual keyboard},
pages = {259--262},
title = {{EOG and EMG based virtual keyboard: A brain-computer interface}},
year = {2009}
}
@article{Gopal2018a,
author = {Gopal and Srivastava, Smriti},
doi = {10.1007/s13369-017-2644-6},
file = {::},
issn = {21914281},
journal = {Arabian Journal for Science and Engineering},
keywords = {Feature-level fusion,Multimodal system,Palm–phalanges,Score-level fusion,Unimodal system},
number = {2},
pages = {543--554},
publisher = {Springer Berlin Heidelberg},
title = {{Accurate Human Recognition by Score-Level and Feature-Level Fusion Using Palm–Phalanges Print}},
volume = {43},
year = {2018}
}
@article{Kumar2016a,
abstract = {We studied the fusion of three biometric authentication modalities, namely, swiping gestures, typing patterns and the phone movement patterns observed during typing or swiping. A web browser was customized to collect the data generated from the aforementioned modalities over four to seven days in an unconstrained environment. Several features were extracted by using sliding window mechanism for each modality and analyzed by using information gain, correlation, and symmetric uncertainty. Finally, five features from windows of continuous swipes, thirty features from windows of continuously typed letters, and nine features from corresponding phone movement patterns while swiping/typing were used to build the authentication system. We evaluated the performance of each modality and their fusion over a dataset of 28 users. The feature-level fusion of swiping and the corresponding phone movement patterns achieved an authentication accuracy of 93.33{\%}, whereas, the score-level fusion of typing behaviors and the corresponding phone movement patterns achieved an authentication accuracy of 89.31{\%}. 1.},
author = {Kumar, Rajesh and Phoha, Vir V. and Serwadda, Abdul},
doi = {10.1109/BTAS.2016.7791164},
file = {::},
isbn = {9781467397339},
journal = {2016 IEEE 8th International Conference on Biometrics Theory, Applications and Systems, BTAS 2016},
title = {{Continuous authentication of smartphone users by fusing typing, swiping, and phone movement patterns}},
year = {2016}
}
@article{Petrovska-Delacretaz2008a,
abstract = {Face recognition finds its place in a large number of applications. They occur in different contexts related to security, entertainment or Internet applications. Reliable face recognition is still a great challenge to computer vision and pattern recognition researchers, and new algorithms need to be evaluated on relevant databases. The publicly available IV2 database allows monomodal and multimodal experiments using face data. Known variabilities, that are critical for the performance of the biometric systems (such as pose, expression, illumination and quality) are present. The face and subface data that are acquired in this database are: 2D audio-video talking-face sequences, 2D stereoscopic data acquired with two pairs of synchronized cameras, 3D facial data acquired with a laser scanner, and iris images acquired with a portable infrared camera. The IV2 database is designed for monomodal and multimodal experiments. The quality of the acquired data is of great importance. Therefore as a first step, and in order to better evaluate the quality of the data, a first internal evaluation was conducted. Only a small amount of the total acquired data was used for this evaluation: 2D still images, 3D scans and iris images. First results show the interest of this database. In parallel to the research algorithms, open-source reference systems were also run for baseline comparisons.},
author = {Petrovska-Delacr{\'{e}}taz, D. and Lelandais, S. and Colineau, J. and Chen, L. and Dorizzi, B. and Ardabilian, M. and Krichen, E. and Mellakh, M. A. and Chaari, A. and Guerfi, S. and D'Hose, J. and Amor, B. Ben},
doi = {10.1109/BTAS.2008.4699323},
file = {::},
isbn = {9781424427307},
journal = {BTAS 2008 - IEEE 2nd International Conference on Biometrics: Theory, Applications and Systems},
pages = {3--9},
title = {{The IV2 multimodal biometric database (including Iris, 2D, 3D, stereoscopic, and talking face data), and the IV2-2007 evaluation campaign}},
volume = {00},
year = {2008}
}
@article{Arslan2017a,
abstract = {Biometric systems may be used to create a remote access model on devices, ensure personal data protection, personalize and facilitate the access security. Biometric systems are generally used to increase the security level in addition to the previous authentication methods and they seen as a good solution. Biometry occupies an important place between the areas of daily life of the machine learning. In this study; the techniques, methods, technologies used in biometric systems are researched, machine learning techniques used biometric aplications are investigated for the security perspective, the advantages and disadvantages that these tecniques provide are given. The studies in the literature between 2010-2016 years, used algorithms, technologies, metrics, usage areas, the machine learning techniques used for different biometric systems such as face, palm prints, iris, voice, fingerprint recognition are researched and the studies made are evaluated. The level of security provided by the use of biometric systems by developed using machine learning and disadvantages that arise in the use of these systems are stated in detail in the study. Also, impact on people of biometric methods in terms of ease of use, security and usages areas are examined.},
author = {Arslan, B. and Yorulmaz, E. and Akca, B. and Sagiroglu, S.},
doi = {10.1109/ICMLA.2016.183},
file = {::},
isbn = {9781509061662},
journal = {Proceedings - 2016 15th IEEE International Conference on Machine Learning and Applications, ICMLA 2016},
keywords = {Biometric,Face,Fingerprint,Iris,Machine learning,Recognition,Security,Teeth,Voice},
pages = {492--497},
title = {{Security perspective of Biometric recognition and machine learning techniques}},
year = {2017}
}
@article{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
file = {::},
isbn = {9781450341448},
issn = {09505849},
journal = {International Conference on Learning Representations (ICRL)},
month = {sep},
pages = {1--14},
pmid = {16873662},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2015}
}
@article{Arsalan2017,
author = {Arsalan, Muhammad and Hong, Hyung Gil and Naqvi, Rizwan Ali and Lee, Min Beom and Kim, Min Cheol and Kim, Dong Seop and Kim, Chan Sik and Park, Kang Ryoung},
doi = {10.3390/sym9110263},
file = {::},
issn = {20738994},
journal = {Symmetry},
keywords = {Biometrics,Convolutional neural network (CNN),Iris recognition,Iris segmentation},
number = {11},
title = {{Deep learning-based iris segmentation for iris recognition in visible light environment}},
volume = {9},
year = {2017}
}
@article{Hqwhua,
author = {Hqwhu, E and Vndudkdq, Qvndudnrf and Jwx, Dnjxo and Wu, H G X},
file = {::},
keywords = {convolutional neural,deep learning,network,pupil center estimation},
title = {{Deep learning based estimation of the eye pupil center by using image patch classification}}
}
@article{Ma2015,
abstract = {This study presents a novel human-machine interface (HMI) based on both electrooculography (EOG) and electroencephalography (EEG). This hybrid interface works in two modes: an EOG mode recognizes eye movements such as blinks, and an EEG mode detects event related potentials (ERPs) like P300. While both eye movements and ERPs have been separately used for implementing assistive interfaces, which help patients with motor disabilities in performing daily tasks, the proposed hybrid interface integrates them together. In this way, both the eye movements and ERPs complement each other. Therefore, it can provide a better efficiency and a wider scope of application. In this study, we design a threshold algorithm that can recognize four kinds of eye movements including blink, wink, gaze, and frown. In addition, an oddball paradigm with stimuli of inverted faces is used to evoke multiple ERP components including P300, N170, and VPP. To verify the effectiveness of the proposed system, two different online experiments are carried out. One is to control a multifunctional humanoid robot, and the other is to control four mobile robots. In both experiments, the subjects can complete tasks effectively by using the proposed interface, whereas the best completion time is relatively short and very close to the one operated by hand.},
author = {Ma, Jiaxin and Zhang, Yu and Cichocki, Andrzej and Matsuno, Fumitoshi},
doi = {10.1109/TBME.2014.2369483},
file = {::},
isbn = {0018-9294},
issn = {15582531},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {Electroencephalogram (EEG),Electrooculogram (EOG),event-related potential (ERP),human-machine interface (HMI),robot control},
number = {3},
pages = {876--889},
pmid = {25398172},
title = {{A novel EOG/EEG hybrid human-machine interface adopting eye movements and ERPs: Application to robot control}},
volume = {62},
year = {2015}
}
@article{Jung2017,
abstract = {Finding the accurate position of an eye is crucial for mobile iris recognition system in order to extract the iris region quickly and correctly. Unfortunately, this is very difficult to accomplish when a person is wearing eyeglasses because of the interference from the eyeglasses. This paper proposes an eye detection method that is robust to eyeglass interference in mobile environment. The proposed method comprises two stages: eye candidate generation and eye validation. In the eye candidate generation stage, multi-scale window masks consisting of 2 × 3 subblocks are used to generate all image blocks possibly containing an eye image. In the ensuing eye validation stage, two methods are employed to determine which blocks actually contain true eye images and locate their precise positions as well: the first method searches for the glint of an NIR illuminator on the pupil region. If this first method fails, the next method computes the intensity difference between the assumed pupil and its surrounding region using multi-scale 3 × 3 window masks. Experimental results show that the proposed method detects the eye position more accurately and quickly than competing methods in the presence of interference from eyeglass frames.},
author = {Jung, Yujin and Kim, Dongik and Son, Byungjun and Kim, Jaihie},
doi = {10.1016/j.eswa.2016.09.036},
file = {::},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Eye detection,Eye validation,Eyeglasses,Iris detection,Iris recognition,Mobile},
pages = {178--188},
publisher = {Elsevier Ltd},
title = {{An eye detection method robust to eyeglasses for mobile iris recognition}},
url = {http://dx.doi.org/10.1016/j.eswa.2016.09.036},
volume = {67},
year = {2017}
}
@inproceedings{Kaur2016,
abstract = {Abstract: Face recognition is a type of biometric software application by using which, we can analyzing, identifying or verifying digital image of the person by using the feature of the face of the person that are unique characteristics of each person. These characteristics may be physical or behavioral. The physiological characteristics as like finger print, iris scan, or face etc and behavior characteristics as like hand-writing, voice, key stroke etc. Face recognition is very useful in many areas such as military, airports, universities, ATM, and banks etc, used for the security purposes. There are many techniques or algorithms that are used features extraction in face recognition. This paper make a review of some of those methods which are used for the face recognition that are Principal Component Analysis (PCA), Back Propagation Neural Networks (BPNN), Genetic Algorithm, and LDA, SVM, Independent Component Analysis(ICA). Each method has different -2 functions that are used for the face recognition. Dimensionality is reduced by using the Eigen face approach or PCA, LDA to extract the features from images. Genetic Algorithm is based on feature selection and Back propagation Neural Network (BPNN) is used for the classification of face images.},
author = {Kaur, Gurpreet and Kanwal, Navdeep},
booktitle = {International Conference on Computing for Sustainable Global Development (INDIACom)},
isbn = {9789380544212},
pages = {2705--2710},
publisher = {IEEE},
title = {{A Comparative Review of Various Approaches for Feature Extraction in Face Recognition}},
url = {http://ieeexplore.ieee.org/document/7724754/},
year = {2016}
}
@article{Hakkinen1993,
abstract = {The primary aim of the study was to determine the best electrode positions for EOG signals in vigilance studies. Two-channel recordings were conducted in analogy to the Rechtschaffen and Kales (1968) system. Twenty electrodes (10 electrode pairs) were compared. Both EOG amplitudes and amplitude asymmetries within an electrode pair were studied. The amplitude of the EOG signal is sensitive to relatively small differences in electrode position. This concerns especially distance from the eye, the direction of eye movement and the effect of the upper eye lid movement. Larger and more symmetrical EOG amplitudes were obtained for different eye movements by placing the electrodes more medially than in the conventionally used system. EOG asymmetry in different electrode positions was dependent on the eye movement direction and even on the starting and end points of a movement with equal angular degrees. Most of the data could be explained by a simple monopolar model when combined with the effects of the upper eye lid movements. The most unexpected finding was that the EOG amplitudes of the horizontal and oblique eye movements were significantly larger when the eye were moving towards an electrode than when they were moving to the opposite direction. {\textcopyright} 1993.},
author = {H{\"{a}}kkinen, V. and Hirvonen, K. and Hasan, J. and Kataja, M. and V{\"{a}}rri, A. and Loula, P. and Eskola, H.},
doi = {10.1016/0013-4694(93)90111-8},
file = {::},
issn = {00134694},
journal = {Electroencephalography and Clinical Neurophysiology},
keywords = {EOG,Electrode positions,Vigilance},
number = {4},
pages = {294--300},
pmid = {7682933},
title = {{The effect of small differences in electrode position on EOG signals: application to vigilance studies}},
volume = {86},
year = {1993}
}
@article{Cheng2017a,
abstract = {This paper focuses on indoor semantic segmentation us-ing RGB-D data. Although the commonly used deconvolu-tion networks (DeconvNet) have achieved impressive results on this task, we find there is still room for improvements in two aspects. One is about the boundary segmentation. DeconvNet aggregates large context to predict the label of each pixel, inherently limiting the segmentation precision of object boundaries. The other is about RGB-D fusion. Re-cent state-of-the-art methods generally fuse RGB and depth networks with equal-weight score fusion, regardless of the varying contributions of the two modalities on delineating different categories in different scenes. To address the two problems, we first propose a locality-sensitive DeconvNet (LS-DeconvNet) to refine the boundary segmentation over each modality. LS-DeconvNet incorporates locally visual and geometric cues from the raw RGB-D data into each DeconvNet, which is able to learn to upsample the coarse convolutional maps with large context whilst recovering sharp object boundaries. Towards RGB-D fusion, we introduce a gated fusion layer to effectively combine the two LS-DeconvNets. This layer can learn to adjust the contributions of RGB and depth over each pixel for high-performance object recognition. Experiments on the large-scale SUN RGB-D dataset and the popular NYU-Depth v2 dataset show that our approach achieves new state-of-the-art results for RGB-D indoor semantic segmentation.},
author = {Cheng, Yanhua and Cai, Rui and Li, Zhiwei and Zhao, Xin and Huang, Kaiqi},
doi = {10.1109/CVPR.2017.161},
file = {::},
isbn = {978-1-5386-0457-1},
journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {1475--1483},
title = {{Locality-Sensitive Deconvolution Networks with Gated Fusion for RGB-D Indoor Semantic Segmentation}},
url = {http://ieeexplore.ieee.org/document/8099644/},
year = {2017}
}
@article{Nam2016a,
abstract = {We propose Dual Attention Networks (DANs) which jointly leverage visual and textual attention mechanisms to capture fine-grained interplay between vision and language. DANs attend to specific regions in images and words in text through multiple steps and gather essential information from both modalities. Based on this framework, we introduce two types of DANs for multimodal reasoning and matching, respectively. The reasoning model allows visual and textual attentions to steer each other during collaborative inference, which is useful for tasks such as Visual Question Answering (VQA). In addition, the matching model exploits the two attention mechanisms to estimate the similarity between images and sentences by focusing on their shared semantics. Our extensive experiments validate the effectiveness of DANs in combining vision and language, achieving the state-of-the-art performance on public benchmarks for VQA and image-text matching.},
archivePrefix = {arXiv},
arxivId = {1611.00471},
author = {Nam, Hyeonseob and Ha, Jung-Woo and Kim, Jeonghee},
doi = {10.1109/CVPR.2017.232},
eprint = {1611.00471},
file = {::},
isbn = {978-1-5386-0457-1},
issn = {1611.00471},
pages = {299--307},
title = {{Dual Attention Networks for Multimodal Reasoning and Matching}},
url = {http://arxiv.org/abs/1611.00471},
year = {2016}
}
@article{Sequeira2014,
abstract = {Biometrics represents a return to a natural way of identification: testing someone by what (s)he is, instead of relying on something (s)he owns or knows seems likely to be the way forward. Biometric systems that include multiple sources of information are known as multimodal. Such systems are generally regarded as an alternative to fight a variety of problems all unimodal systems stumble upon. One of the main challenges found in the development of biometric recognition systems is the shortage of publicly available databases acquired under real unconstrained working conditions. Motivated by such need the MobBIO database was created using an Asus EeePad Transformer tablet, with mobile biometric systems in mind. The proposed database is composed by three modalities: iris, face and voice.},
author = {Sequeira, Ana F. and Monteiro, Joao C. and Rebelo, Ana and Oliveira, Helder P.},
file = {::},
isbn = {9789897580093},
journal = {9th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
number = {c},
pages = {1--14},
title = {{MobBIO: A Multimodal Database Captured with a Portable Handheld Device}},
year = {2014}
}
@article{Cheng2017a,
abstract = {Sea–land segmentation and ship detection are two prevalent research domains for optical remote sensing harbor images and can find many applications in harbor supervision and management. As the spatial resolution of imaging technology improves, traditional methods struggle to perform well due to the complicated appearance and background distributions. In this paper, we unify the above two tasks into a single framework and apply the deep convolutional neural networks to predict pixelwise label for an input. Specifically, an edge aware convolutional network is proposed to parse a remote sensing harbor image into three typical objects, e.g., sea, land, and ship. Two innovations are made on top of the deep structure. First, we design a multitask model by simultaneously training the segmentation and edge detection networks. Hierarchical semantic features from the segmentation network are extracted to learn the edge network. Second, the outputs of edge pipeline are further employed to refine entire model by adding an edge aware regularization, which helps our method to yield very desirable results that are spatially consistent and well boundary located. It also benefits the segmentation of docked ships that are quite challenging for many previous methods. Experimental results on two datasets collected from Google Earth have demonstrated the effectiveness of our approach both in quantitative and qualitative performance compared with state-of-the-art methods.},
author = {Cheng, Dongcai and Meng, Gaofeng and Xiang, Shiming and Pan, Chunhong},
doi = {10.1109/JSTARS.2017.2747599},
file = {::},
issn = {21511535},
journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
keywords = {Edge aware regularization,harbor images,multitask learning,semantic segmentation},
number = {12},
pages = {5769--5783},
title = {{FusionNet: Edge Aware Deep Convolutional Networks for Semantic Segmentation of Remote Sensing Harbor Images}},
volume = {10},
year = {2017}
}
@article{Pathak2016a,
author = {Pathak, Mrunal and Srinivasu, N},
doi = {10.1007/978-981-10-2630-0},
file = {::},
isbn = {978-981-10-2629-4},
keywords = {biometrics},
pages = {137--152},
title = {{Advances in Computing Applications}},
url = {http://link.springer.com/10.1007/978-981-10-2630-0},
year = {2016}
}
@article{Fierrez2018c,
abstract = {The present paper is Part 2 in this series of two papers. In Part 1 we provided an introduction to Multiple Classifier Systems (MCS) with a focus into the fundamentals: basic nomenclature, key elements, architecture, main methods, and prevalent theory and framework. Part 1 then overviewed the application of MCS to the particular field of multimodal biometric person authentication in the last 25 years, as a prototypical area in which MCS has resulted in important achievements. Here in Part 2 we present in more technical detail recent trends and developments in MCS coming from multimodal biometrics that incorporate context information in an adaptive way. These new MCS architectures exploit input quality measures and pattern-specific particularities that move apart from general population statistics, resulting in robust multimodal biometric systems. Similarly as in Part 1, methods here are described in a general way so they can be applied to other information fusion problems as well. Finally, we also discuss here open challenges in biometrics in which MCS can play a key role.},
author = {Fierrez, Julian and Morales, Aythami and Vera-Rodriguez, Ruben and Camacho, David},
doi = {10.1016/j.inffus.2017.12.005},
file = {::},
issn = {15662535},
journal = {Information Fusion},
keywords = {Adaptive,Biometrics,Classifier,Context,Fusion,Multimodal},
number = {November 2017},
pages = {103--112},
publisher = {Elsevier},
title = {{Multiple classifiers in biometrics. Part 2: Trends and challenges}},
url = {https://doi.org/10.1016/j.inffus.2017.12.005},
volume = {44},
year = {2018}
}
@article{Lv2015,
abstract = {There is an increasing interest in creating pervasive games based on emerging interaction technologies. In order to develop touch-less, interactive and augmented reality games on vision-based wearable device, a touch-less motion interaction technology is designed and evaluated in this work. Users interact with the augmented reality games with dynamic hands/feet gestures in front of the camera, which triggers the interaction event to interact with the virtual object in the scene. Three primitive augmented reality games with eleven dynamic gestures are developed based on the proposed touch-less interaction technology as proof. At last, a comparing evaluation is proposed to demonstrate the social acceptability and usability of the touch-less approach, running on a hybrid wearable framework or with Google Glass, as well as workload assessment, user's emotions and satisfaction.},
author = {Lv, Zhihan and Halawani, Alaa and Feng, Shengzhong and {Ur R{\'{e}}hman}, Shafiq and Li, Haibo},
doi = {10.1007/s00779-015-0844-1},
file = {::},
isbn = {16174909 (ISSN)},
issn = {16174909},
journal = {Personal and Ubiquitous Computing},
keywords = {Augmented reality game,Hand free,Pervasive game,Smartphone game,Touch-less,Wearable device},
number = {3},
pages = {551--567},
title = {{Touch-less interactive augmented reality game on vision-based wearable device}},
volume = {19},
year = {2015}
}
@article{Duguleana2010,
abstract = {This paper proposes a new approach to real-time robot controlling by integrating an Electrooculography (EOG) measuring device within human-robot interaction (HRI). Our study focuses on controlling robots using EOG for fulfilling elementary robot activities such as basic motor movements and environment interaction. A new EOG-based HRI paradigm has been developed on the specific defined problem of eye blinking. The resulted model is tested using biometric capturing equipment. We propose a simple algorithm for real-time identification and processing of signals produced by eyes during blinking phases. We present the experimental setup and the results of the experiment. We conclude by listing further research issues.},
author = {Duguleana, Mihai and Mogan, Gheorghe},
doi = {10.1007/978-3-642-11628-5_37},
file = {::},
isbn = {9783642116278},
issn = {18684238},
journal = {IFIP Advances in Information and Communication Technology},
keywords = {Control,Electrooculography,Eye blink,Human-robot interaction,Robot},
number = {29},
pages = {343--350},
title = {{Using eye blinking for EOG-based robot control}},
volume = {314},
year = {2010}
}
@article{Percy,
abstract = {Iris recognition system is a reliable and an accurate biometric system. Localization of the iris borders in an eye image can be considered as a vital step in the iris recognition process. There exist many algorithms to segment the iris. One of the segmentation methods, that is used in many commercial iris biometric systems is an algorithm known as a Daugman's algorithm. The aim of this thesis is to implement this algorithm using MATLAB programming environment. The implemented algorithm was tested on the eye images of different quality, such as the images with partly covered iris or low contrast images. The test results demonstrated that the Daugman's algorithm detects the iris borders in the high quality images with high accuracy. The performance of the algorithm on the lower quality images has been improved by additional preprocessing of these images.},
author = {Percy, Oad and Waqas, Ahmad},
file = {::},
journal = {Blekinge Institute of Technology},
pages = {1--48},
title = {{Iris localization using Daugman ' s algorithm}},
url = {http://www.diva-portal.org/smash/get/diva2:831173/FULLTEXT01.pdf},
year = {2012}
}
@article{Johnson2010a,
abstract = {Identification of individuals using biometric information has found great success in many security and law enforcement applications. Up until the present time, most research in the field has been focused on ideal conditions and most available databases are constructed in these ideal conditions. There has been a growing interest in the perfection of these technologies at a distance and in less than ideal conditions, i.e. low lighting, out-of-focus blur, off angles, etc. This paper presents a dataset consisting of face and iris videos obtained at distances of 5 to 25 feet and in conditions of varying quality. The purpose of this database is to set a standard for quality measurement in face and iris data and to provide a means for analyzing biométrie systems in less than ideal conditions. The structure of the dataset as well as a quantified metric for quality measurement based on a 25 subject subset of the dataset is presented.},
author = {Johnson, P. A. and Lopez-Meyer, P. and Sazonova, N. and Hua, F. and Schuckers, S.},
doi = {10.1109/BTAS.2010.5634513},
file = {::},
isbn = {9781424475803},
journal = {IEEE 4th International Conference on Biometrics: Theory, Applications and Systems, BTAS 2010},
title = {{Quality in face and Iris research ensemble (Q-FIRE)}},
year = {2010}
}
@article{Bazrafkan2017,
abstract = {With the increasing imaging and processing capabilities of today's mobile devices, user authentication using iris biometrics has become feasible. However, as the acquisition conditions become more unconstrained and as image quality is typically lower than dedicated iris acquisition systems, the accurate segmentation of iris regions is crucial for these devices. In this work, an end to end Fully Convolutional Deep Neural Network (FCDNN) design is proposed to perform the iris segmentation task for lower-quality iris images. The network design process is explained in detail, and the resulting network is trained and tuned using several large public iris datasets. A set of methods to generate and augment suitable lower quality iris images from the high-quality public databases are provided. The network is trained on Near InfraRed (NIR) images initially and later tuned on additional datasets derived from visible images. Comprehensive inter-database comparisons are provided together with results from a selection of experiments detailing the effects of different tunings of the network. Finally, the proposed model is compared with SegNet-basic, and a near-optimal tuning of the network is compared to a selection of other state-of-art iris segmentation algorithms. The results show very promising performance from the optimized Deep Neural Networks design when compared with state-of-art techniques applied to the same lower quality datasets.},
archivePrefix = {arXiv},
arxivId = {1712.02877},
author = {Bazrafkan, Shabab and Thavalengal, Shejin and Corcoran, Peter},
eprint = {1712.02877},
file = {::},
month = {dec},
title = {{An End to End Deep Neural Network for Iris Segmentation in Unconstraint Scenarios}},
url = {http://arxiv.org/abs/1712.02877},
year = {2017}
}
@article{Biosec2007,
abstract = {The baseline corpus of a new multimodal database, acquired in the framework of the FP6 EU BioSec Integrated Project, is presented. The corpus consists of fingerprint images acquired with three different sensors, frontal face images from a webcam, iris images from an iris sensor, and voice utterances acquired both with a close-talk headset and a distant webcam microphone. The BioSec baseline corpus includes real multimodal data from 200 individuals in two acquisition sessions. In this contribution, the acquisition setup and protocol are outlined, and the contents of the corpus-including data and population statistics-are described. The database will be publicly available for research purposes by mid-2006. {\textcopyright} 2006 Pattern Recognition Society.},
author = {Fierrez, Julian and Ortega-Garcia, Javier and {Torre Toledano}, Doroteo and Gonzalez-Rodriguez, Joaquin},
doi = {10.1016/j.patcog.2006.10.014},
file = {::},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Authentication,Biometrics,Database,Face,Fingerprint,Iris,Multimodal,Performance,Verification,Voice},
number = {4},
pages = {1389--1392},
title = {{Biosec baseline corpus: A multimodal biometric database}},
volume = {40},
year = {2007}
}
@inproceedings{Ghazi2016a,
abstract = {Deep learning based approaches have been dominating the face recognition field due to the significant performance improvement they have provided on the challenging wild datasets. These approaches have been extensively tested on such unconstrained datasets, on the Labeled Faces in the Wild and YouTube Faces, to name a few. However, their capability to handle individual appearance variations caused by factors such as head pose, illumination, occlusion, and misalignment has not been thoroughly assessed till now. In this paper, we present a comprehensive study to evaluate the performance of deep learning based face representation under several conditions including the varying head pose angles, upper and lower face occlusion, changing illumination of different strengths, and misalignment due to erroneous facial feature localization. Two successful and publicly available deep learning models, namely VGG-Face and Lightened CNN have been utilized to extract face representations. The obtained results show that although deep learning provides a powerful representation for face recognition, it can still benefit from preprocessing, for example, for pose and illumination normalization in order to achieve better performance under various conditions. Particularly, if these variations are not included in the dataset used to train the deep learning model, the role of preprocessing becomes more crucial. Experimental results also show that deep learning based representation is robust to misalignment and can tolerate facial feature localization errors up to 10{\%} of the interocular distance.},
author = {Ghazi, Mostafa Mehdipour and Ekenel, Hazim Kemal},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
doi = {10.1109/CVPRW.2016.20},
file = {::},
isbn = {978-1-5090-1437-8},
month = {jun},
pages = {102--109},
publisher = {IEEE},
title = {{A Comprehensive Analysis of Deep Learning Based Representation for Face Recognition}},
url = {http://ieeexplore.ieee.org/document/7789510/},
year = {2016}
}
@article{Nielsen2015,
abstract = {Neural Networks and Deep Learning is a free online book. The book will teach you about: Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data Deep learning, a powerful set of techniques for learning in neural networks Neural networks and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing. This book will teach you many of the core concepts behind neural networks and deep learning.},
author = {Nielsen, Michael},
journal = {Determination Press},
title = {{Neural Networks and Deep Learning}},
url = {http://neuralnetworksanddeeplearning.com/index.html},
year = {2015}
}
@inproceedings{Yin2011,
abstract = {In this paper, the acquisition and content of a new homologous multimodal biometric database are presented. The SDUMLA-HMT database consists of face images from 7 view angles, finger vein images of 6 fingers, gait videos from 6 view angles, iris images from an iris sensor, and fingerprint images acquired with 5 different sensors. The database includes real multimodal data from 106 individuals. In addition to database description, we also present possible use of the database. The database is available to research community through http://mla.sdu.edu.cn/sdumla-hmt.html .},
author = {Yin, Yilong and Liu, Lili and Sun, Xiwei},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-25449-9_33},
file = {::},
isbn = {9783642254482},
issn = {03029743},
keywords = {Biometrics,Face,Finger vein,Fingerprint,Gait,Homologous,Iris,Multi-modal},
pages = {260--268},
title = {{SDUMLA-HMT: A multimodal biometric database}},
volume = {7098 LNCS},
year = {2011}
}
@article{Chen2018,
abstract = {In this work, we address the problem of face verification, namely determining whether a pair of face images belongs to the same or different subjects. Previous works often consider solving the problem of face verification in two steps: feature extraction and face recognition, resulting in a fragmented procedure. We argue that these techniques, although working well, fail to explicitly exploit a full end-to-end framework for face verification, which has received much attention and achieved significant improvements recently. In this paper, we propose a novel Joint Bayesian guided metric learning technique for dealing with the face verification task, which well integrates the above two steps of face verification into an end-to-end convolutional neural network (CNN) architecture. In the training stage, an initial neural network, which has the similar architecture with GoogLeNet CNN model, is firstly pre-trained by optimizing classification-based objective functions on the publicly available CASIA WebFace database. Based on constructed face pairs dataset from CASIA WebFace and LFW datasets, we then fine-tune the whole network parameters under the guide of the learned knowledge, which is obtained from the highly successful Joint Bayesian model. This guided learning procedure, which can also be seen as a metric learning technique, can further update network parameters for discriminating face pairs. In the testing process, the outputs by this unified network are discriminated with a threshold value to produce the ultimate prediction for the face verification task. Comprehensive evaluations over the LFW dataset well demonstrate the encouraging face verification performance of our proposed framework.},
author = {Chen, Di and Xu, Chunyan and Yang, Jian and Qian, Jianjun and Zheng, Yuhui and Shen, Linlin},
doi = {10.1016/J.NEUCOM.2017.09.009},
file = {::},
issn = {0925-2312},
journal = {Neurocomputing},
month = {jan},
pages = {560--567},
publisher = {Elsevier},
title = {{Joint Bayesian guided metric learning for end-to-end face verification}},
url = {https://www-sciencedirect-com.zorac.aub.aau.dk/science/article/pii/S0925231217314807{\#}bib0016},
volume = {275},
year = {2018}
}
@article{Liu2013a,
author = {Liu, Jing and Sun, Zhenan and Tan, Tieniu},
doi = {10.1109/BTAS.2013.6712692},
file = {::},
isbn = {9781479905270},
journal = {IEEE 6th International Conference on Biometrics: Theory, Applications and Systems, BTAS 2013},
pages = {1--6},
title = {{Code-level information fusion of low-resolution iris image sequences for personal identification at a distance}},
year = {2013}
}
@article{Karpathy2016a,
abstract = {These notes accompany the Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition.},
author = {Karpathy, Andrej},
journal = {Stanford University},
pages = {1--21},
title = {{Convolutional Neural Networks for Visual Recognition}},
url = {http://cs231n.github.io/classification/},
year = {2016}
}
@article{Proenca2017a,
abstract = {The effectiveness of current iris recognition systems de-pends on the accurate segmentation and parameterisation of the iris boundaries, as failures at this point misalign the coefficients of the biometric signatures. This paper de-scribes IRINA, an algorithm for Iris Recognition that is ro-bust against INAccurately segmented samples, which makes it a good candidate to work in poor-quality data. The pro-cess is based in the concept of " corresponding " patch be-tween pairs of images, that is used to estimate the posterior probabilities that patches regard the same biological region, even in case of segmentation errors and non-linear texture deformations. Such information enables to infer a free-form deformation field (2D registration vectors) between images, whose first and second-order statistics provide effective bio-metric discriminating power. Extensive experiments were carried out in four datasets (CASIA-IrisV3-Lamp, CASIA-IrisV4-Lamp, CASIA-IrisV4-Thousand and WVU) and show that IRINA not only achieves state-of-the-art performance in good quality data, but also handles effectively severe seg-mentation errors and large differences in pupillary dilation / constriction.},
author = {Proenca, Hugo and Neves, Joao C.},
doi = {10.1109/CVPR.2017.714},
file = {::},
isbn = {978-1-5386-0457-1},
journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {6747--6756},
title = {{IRINA: Iris Recognition (Even) in Inaccurately Segmented Data}},
url = {http://ieeexplore.ieee.org/document/8100197/},
year = {2017}
}
@article{Khiari-Hili2017a,
author = {Khiari-Hili, Nefissa and Montagne, Christophe and Lelandais, Sylvie and Hamrouni, Kamel},
doi = {10.1109/IPTA.2016.7820954},
file = {::},
isbn = {9781467389105},
journal = {2016 6th International Conference on Image Processing Theory, Tools and Applications, IPTA 2016},
keywords = {Multimodal biometrics,authentication,dynamic weighted sum,face,iris,quality,score fusion},
pages = {1--6},
title = {{Quality dependent multimodal fusion of face and iris biometrics}},
year = {2017}
}
@article{Zhang2016a,
author = {Zhang, Man and Zhang, Qi and Sun, Zhenan and Zhou, Shujuan and Ahmed, Nasir Uddin},
doi = {10.1109/BTAS.2016.7791191},
file = {::},
isbn = {9781467397339},
journal = {2016 IEEE 8th International Conference on Biometrics Theory, Applications and Systems, BTAS 2016},
title = {{The BTAS∗Competition on Mobile Iris Recognition}},
year = {2016}
}
@article{Al-Waisy2017,
author = {Al-Waisy, Alaa S. and Qahwaji, Rami and Ipson, Stanley and Al-Fahdawi, Shumoos and Nagem, Tarek A.M.},
doi = {10.1007/s10044-017-0656-1},
file = {::},
isbn = {0123456789},
issn = {14337541},
journal = {Pattern Analysis and Applications},
keywords = {AdaGrad method,Convolutional Neural Network,Deep learning,Iris recognition,Multimodal biometric systems,Softmax classifier},
number = {0123456789},
pages = {1--20},
publisher = {Springer London},
title = {{A multi-biometric iris recognition system based on a deep learning approach}},
url = {https://doi.org/10.1007/s10044-017-0656-1},
year = {2017}
}
@article{Ho1994,
abstract = {A multiple classifier system is a powerful solution to difficult pattern$\backslash$nrecognition problems involving large class sets and noisy input$\backslash$nbecause it allows simultaneous use of arbitrary feature descriptors$\backslash$nand classification procedures. Decisions by the classifiers can$\backslash$nbe represented as rankings of classifiers and different instances$\backslash$nof a problem. The rankings can be combined by methods that either$\backslash$nreduce or rerank a given set of classes. An intersection method$\backslash$nand union method are proposed for class set reduction. Three methods$\backslash$nbased on the highest rank, the Borda count, and logistic regression$\backslash$nare proposed for class set reranking. These methods have been tested$\backslash$nin applications of degraded machine-printed characters and works$\backslash$nfrom large lexicons, resulting in substantial improvement in overall$\backslash$ncorrectness.},
author = {Ho, Tin Kam and Hull, Jonathan J and Srihari, Sargur N},
doi = {http://dx.doi.org/10.1109/34.273716},
file = {::},
isbn = {0162-8828 VO - 16},
issn = {0162-8828},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
number = {1},
pages = {66--75},
title = {{Decision Combination in Multiple Classifier Systems}},
volume = {16},
year = {1994}
}
@article{Ma2015,
abstract = {This study presents a novel human-machine interface (HMI) based on both electrooculography (EOG) and electroencephalography (EEG). This hybrid interface works in two modes: an EOG mode recognizes eye movements such as blinks, and an EEG mode detects event related potentials (ERPs) like P300. While both eye movements and ERPs have been separately used for implementing assistive interfaces, which help patients with motor disabilities in performing daily tasks, the proposed hybrid interface integrates them together. In this way, both the eye movements and ERPs complement each other. Therefore, it can provide a better efficiency and a wider scope of application. In this study, we design a threshold algorithm that can recognize four kinds of eye movements including blink, wink, gaze, and frown. In addition, an oddball paradigm with stimuli of inverted faces is used to evoke multiple ERP components including P300, N170, and VPP. To verify the effectiveness of the proposed system, two different online experiments are carried out. One is to control a multifunctional humanoid robot, and the other is to control four mobile robots. In both experiments, the subjects can complete tasks effectively by using the proposed interface, whereas the best completion time is relatively short and very close to the one operated by hand.},
author = {Ma, Jiaxin and Zhang, Yu and Cichocki, Andrzej and Matsuno, Fumitoshi},
doi = {10.1109/TBME.2014.2369483},
file = {::},
isbn = {0018-9294},
issn = {15582531},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {Electroencephalogram (EEG),Electrooculogram (EOG),event-related potential (ERP),human-machine interface (HMI),robot control},
number = {3},
pages = {876--889},
pmid = {25398172},
title = {{A novel EOG/EEG hybrid human-machine interface adopting eye movements and ERPs: Application to robot control}},
volume = {62},
year = {2015}
}
@inproceedings{Sangeetha2013,
abstract = {-In a Multimodal biometric system, the effective fusion method is necessary for combining information from various single modality systems. Two biometric characteristics are considered in this study: iris and fingerprint. Multimodal biometric system needs an effective fusion scheme to combine biometric characteristics derived from one or more modalities. The score level fusion is used to combine the characteristics from diff erent biometric modalities. Fusion at the score level is a new technique, which has a high potential for e f ficient consolidation of multiple unimodal biometric matcher outputs. Support vector machine and extreme learning techniques are used in this system for recognition of biometric traits. In this, the Fingerprint-Iris system provides better performance and comparison of support vector machine and extreme learning machine based on score-level fusion methods is obtained In score-level fusion, ELM provides better performance as compare to the SVM It reduces the classification time of current system. This work is valuable and makes an e f ficient accuracy in such applications. This system can be utilized for person identification in several applications.},
author = {Sangeetha, S and Radha, N.},
booktitle = {2013 7th International Conference on Intelligent Systems and Control (ISCO)},
doi = {10.1109/ISCO.2013.6481145},
isbn = {978-1-4673-4603-0},
month = {jan},
pages = {183--188},
publisher = {IEEE},
title = {{A New Framework for IRIS and Fingerprint Recognition Using SVM Classification and Extreme Learning Machine Based on Score Level Fusion}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6481145},
year = {2013}
}
@article{Daugman2009a,
abstract = {This chapter explains the iris recognition algorithms and presents results of 9.1 million comparisons among eye images from trials in Britain, the USA, Japan, and Korea. The key to iris recognition is the failure of a test of statistical independence, which involves so many degrees-of-freedom that this test is virtually guaranteed to be passed whenever the phase codes for two different eyes are compared, but to be uniquely failed when any eye's phase code is compared with another version of itself. The test of statistical independence is implemented by the simple Boolean Exclusive-OR operator (XOR) applied to the 2048 bit phase vectors that encode any two iris patterns, masked (AND'ed) by both of their corresponding mask bit vectors to prevent non iris artifacts from influencing iris comparisons. The XOR operator detects disagreement between any corresponding pair of bits, while the AND operator ensures that the compared bits are both deemed to have been uncorrupted by eyelashes, eyelids, specular reflections, or other noise. The norms of the resultant bit vector and of theAND'ed mask vectors are then measured in order to compute a fractional Hamming Distance as the measure of the dissimilarity between any two irises, whose two phase code bit vectors are denoted {\{}. codeA, codeB{\}} and whose mask bit vectors are denoted {\{}. maskA, maskB{\}}. {\textcopyright} 2009 Elsevier Inc. All rights reserved.},
annote = {A chapter about how Iris Recognitions works in general. John Daugman is the creator of IrisCode, a 2D Gabor wavelet-based iris recognition algorithm that is the basis of all publicly deployed automatic iris recognition systems and which has registered more than a billion persons worldwide in government ID programs.

Keywords/phrases:
Near Infra Red (NIR) images can be used for iris capturing.

Gabor wavelets are used for determining the inter and outer edges of an iris.

Often the iris will not be circular because an eyelid will cover it.

Hamming distance is used when comparing two irises.},
author = {Daugman, John},
doi = {10.1016/B978-0-12-374457-9.00025-1},
file = {::},
isbn = {9780123744579},
issn = {10518215},
journal = {The Essential Guide to Image Processing},
pages = {715--739},
pmid = {20810146},
title = {{How Iris Recognition Works}},
year = {2009}
}
@article{LeCun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
file = {::},
isbn = {9780521835688},
issn = {14764687},
journal = {Nature},
keywords = {Computer science,Mathematics and computing},
month = {may},
number = {7553},
pages = {436--444},
pmid = {10463930},
publisher = {Nature Publishing Group},
title = {{Deep learning}},
url = {http://www.nature.com/articles/nature14539},
volume = {521},
year = {2015}
}
@article{Deng2010,
abstract = {Several Human-Machine/Computer Interfaces (HMI/HCI) had been developed in recent years. Some designs were specifically made for people with disabilities such as injured-vertebra, apoplexy or poliomyelitis, Amyotrophic Lateral Sclerosis (ALS), and Motor Neuron Disease, (MND). In this paper, we proposed an eye-movement tracking system. Based on Electro-Oculography (EOG) technology we detected the signal with different directions in eye-movements and then analyzed to understand what they represented about (e.g. horizontal direction or vertical direction). We converted the analog signal to digital signal and then used as the control signals for Human-Computer Interface (HCI). In order to make the system "robust", several applications with EOG-based HCI had been designed. Our preliminary results revealed more than 90{\%} accuracy rate for examining the eye-movement that may become a new useful human-machine user interface in the near future. {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
author = {Deng, Lawrence Y. and Hsu, Chun Liang and Lin, Tzu Ching and Tuan, Jui Sen and Chang, Shih Ming},
doi = {10.1016/j.eswa.2009.10.017},
file = {::},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Amyotrophic Lateral Sclerosis,Electro-Oculography (EOG),Eye-movement,Human-Machine/Computer Interface (HMI/HCI),Motor Neuron Disease},
number = {4},
pages = {3337--3343},
publisher = {Elsevier Ltd},
title = {{EOG-based Human-Computer Interface system development}},
url = {http://dx.doi.org/10.1016/j.eswa.2009.10.017},
volume = {37},
year = {2010}
}
@incollection{Bowyer2016,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bowyer, Kevin W and Hollingsworth, Karen P and Flynn, Patrick J},
booktitle = {Handbook of Iris Recognition},
doi = {10.1007/978-1-4471-6784-6},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {978-1-4471-6782-2},
issn = {16130073},
pages = {23--61},
pmid = {25246403},
title = {{Chapter 2 A Survey of Iris Biometrics Research: 2008–2010}},
url = {http://link.springer.com/10.1007/978-1-4471-6784-6},
year = {2016}
}
@article{Suk2014,
abstract = {For the last decade, it has been shown that neuroimaging can be a potential tool for the diagnosis of Alzheimer's Disease (AD) and its prodromal stage, Mild Cognitive Impairment (MCI), and also fusion of different modalities can further provide the complementary information to enhance diagnostic accuracy. Here, we focus on the problems of both feature representation and fusion of multimodal information from Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET). To our best knowledge, the previous methods in the literature mostly used hand-crafted features such as cortical thickness, gray matter densities from MRI, or voxel intensities from PET, and then combined these multimodal features by simply concatenating into a long vector or transforming into a higher-dimensional kernel space. In this paper, we propose a novel method for a high-level latent and shared feature representation from neuroimaging modalities via deep learning. Specifically, we use Deep Boltzmann Machine (DBM).22Although it is clear from the context that the acronym DBM denotes "Deep Boltzmann Machine" in this paper, we would clearly indicate that DBM here is not related to "Deformation Based Morphometry"., a deep network with a restricted Boltzmann machine as a building block, to find a latent hierarchical feature representation from a 3D patch, and then devise a systematic method for a joint feature representation from the paired patches of MRI and PET with a multimodal DBM. To validate the effectiveness of the proposed method, we performed experiments on ADNI dataset and compared with the state-of-the-art methods. In three binary classification problems of AD vs. healthy Normal Control (NC), MCI vs. NC, and MCI converter vs. MCI non-converter, we obtained the maximal accuracies of 95.35{\%}, 85.67{\%}, and 74.58{\%}, respectively, outperforming the competing methods. By visual inspection of the trained model, we observed that the proposed method could hierarchically discover the complex latent patterns inherent in both MRI and PET.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Suk, Heung Il and Lee, Seong Whan and Shen, Dinggang},
doi = {10.1016/j.neuroimage.2014.06.077},
eprint = {NIHMS150003},
file = {::},
isbn = {1053-8119},
issn = {10959572},
journal = {NeuroImage},
keywords = {Alzheimer's Disease,Deep boltzmann machine,Mild cognitive impairment,Multimodal data fusion,Shared feature representation},
pages = {569--582},
pmid = {25042445},
publisher = {Elsevier Inc.},
title = {{Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2014.06.077},
volume = {101},
year = {2014}
}
@article{Fierrez2018b,
abstract = {We provide an introduction to Multiple Classifier Systems (MCS) including basic nomenclature and describing key elements: classifier dependencies, type of classifier outputs, aggregation procedures, architecture, and types of methods. This introduction complements other existing overviews of MCS, as here we also review the most prevalent theoretical framework for MCS and discuss theoretical developments related to MCS. The introduction to MCS is then followed by a review of the application of MCS to the particular field of multimodal biometric person authentication in the last 25 years, as a prototypical area in which MCS has resulted in important achievements. This review includes general descriptions of successful MCS methods and architectures in order to facilitate the export of them to other information fusion problems. Based on the theory and framework introduced here, in the companion paper we then develop in more technical detail recent trends and developments in MCS from multimodal biometrics that incorporate context information in an adaptive way. These new MCS architectures exploit input quality measures and pattern-specific particularities that move apart from general population statistics, resulting in robust multimodal biometric systems. Similarly as in the present paper, methods in the companion paper are introduced in a general way so they can be applied to other information fusion problems as well. Finally, also in the companion paper, we discuss open challenges in biometrics and the role of MCS to advance them.},
author = {Fierrez, Julian and Morales, Aythami and Vera-Rodriguez, Ruben and Camacho, David},
doi = {10.1016/j.inffus.2017.12.003},
file = {::},
issn = {15662535},
journal = {Information Fusion},
keywords = {Adaptive,Biometrics,Classifier,Context,Fusion,Multimodal},
number = {November 2017},
pages = {57--64},
publisher = {Elsevier},
title = {{Multiple classifiers in biometrics. part 1: Fundamentals and review}},
url = {https://doi.org/10.1016/j.inffus.2017.12.003},
volume = {44},
year = {2018}
}
@article{Al-Waisy2017a,
abstract = {Multimodal biometrie systems seek to alleviate some of the limitations of unimodal biometrie systems by combining multiple pieces of evidence of the same person in the deeision-making process. In this paper, a novel multimodal biometric identification system is proposed based on fusing the results obtained from both the face and the left and right irises using deep learning approaches. Firstly, the facial features are extracted using a Deep Belief Network (DBN) architecture consisting of 3-layers. The first two RBMs are used as features detectors, while the last one is used as a discriminative model associated with softmax units for the multi-class classification purpose. Secondly, an efficient deep learning system is employed for iris recognition, whose architecture is based on a combination of Convolutional Neural Network (CNN) and Softmax classifier to extract discriminative features from an iris image. Extensive experiments on large-scale challenging databases, including FERET, CASIA V1.0 and MMU1, and SDUMLA-HMT have demonstrated the superiority of the proposed approaches by achieving new state-of-the-art Rank-1 identification rates on all the employed databases.},
author = {Al-Waisy, A S and Qahwaji, R and Ipson, S and Al-Fahdawi, S},
doi = {10.1109/EST.2017.8090417},
file = {::},
isbn = {9781538640173},
journal = {2017 Seventh International Conference on Emerging Security Technologies (EST)},
keywords = {Databases;Face;Feature extraction;Iris recognition},
pages = {163--168},
title = {{A multimodal biometrie system for personal identification based on deep learning approaches}},
year = {2017}
}
@article{Furman1950,
abstract = {Axonal sprouting of excitatory neurons is frequently observed in$\backslash$ntemporal lobe epilepsy, but the extent to which inhibitory interneurons$\backslash$nundergo similar axonal reorganization remains unclear. The goal of this$\backslash$nstudy was to determine whether somatostatin (SOM)-expressing neurons in$\backslash$nstratum (s.) oriens of the hippocampus exhibit axonal sprouting beyond$\backslash$ntheir normal territory and innervate granule cells of the dentate gyrus$\backslash$nin a pilocarpine model of epilepsy. To obtain selective labeling of$\backslash$nSOM-expressing neurons in s. oriens, a Cre recombinase-dependent$\backslash$nconstruct for channelrhodopsin2 fused to enhanced yellow fluorescent$\backslash$nprotein (ChR2-eYFP) was virally delivered to this region in SOM-Cre$\backslash$nmice. In control mice, labeled axons were restricted primarily to s.$\backslash$nlacunosum-moleculare. However, in pilocarpine-treated animals, a rich$\backslash$nplexus of ChR2-eYFP-labeled fibers and boutons extended into the dentate$\backslash$nmolecular layer. Electron microscopy with immunogold labeling$\backslash$ndemonstrated labeled axon terminals that formed symmetric synapses on$\backslash$ndendritic profiles in this region, consistent with innervation of$\backslash$ngranule cells. Patterned illumination of ChR2-labeled fibers in s.$\backslash$nlacunosum-moleculare of CA1 and the dentate molecular layer elicited$\backslash$nGABAergic inhibitory responses in dentate granule cells in$\backslash$npilocarpine-treated mice but not in controls. Similar optical$\backslash$nstimulation in the dentate hilus evoked no significant responses in$\backslash$ngranule cells of either group of mice. These findings indicate that$\backslash$nunder pathological conditions, SOM/GABAergic neurons can undergo$\backslash$nsubstantial axonal reorganization beyond their normal territory and$\backslash$nestablish aberrant synaptic connections. Such reorganized circuitry$\backslash$ncould contribute to functional deficits in inhibition in epilepsy,$\backslash$ndespite the presence of numerous GABAergic terminals in the region.},
author = {Furman, Joseph M. and Wuyts, Floris L. and Siddiqui, Uzma and Shaikh, A N and Lord, Mary P. and Wright, W. D. and Colegatet, Robert L and Hoffman, James E},
doi = {10.1016/B978-1-4557-0308-1.00032-7},
edition = {6},
file = {::;::;::},
isbn = {978-1-4665-5543-3},
issn = {2278-1021},
journal = {Reports on Progress in Physics},
keywords = {adc,analogdigitalconverter,eeg,electroencefalogram,electromyalgy,electrooculography,emg,eog,rapid eye movement,rem,sem,slow eye movement},
number = {1},
pages = {4328--4330},
publisher = {Elsevier Inc.},
title = {{The investigation of eye movements}},
url = {www.ijarcce.com http://dx.doi.org/10.1016/B978-1-4557-0308-1.00032-7},
volume = {2},
year = {1950}
}
@article{Baltrusaitis2017a,
abstract = {Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.},
archivePrefix = {arXiv},
arxivId = {1705.09406},
author = {Baltru{\v{s}}aitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
doi = {10.1109/TPAMI.2018.2798607},
eprint = {1705.09406},
file = {::},
issn = {0162-8828},
number = {c},
pages = {1--20},
title = {{Multimodal Machine Learning: A Survey and Taxonomy}},
url = {http://arxiv.org/abs/1705.09406},
volume = {8828},
year = {2017}
}
@article{Kaur2016b,
abstract = {Face recognition is a type of biometric software application by using which, we can analyzing, identifying or verifying digital image of the person by using the feature of the face of the person that are unique characteristics of each person. These characteristics may be physical or behavioral. The physiological characteristics as like finger print, iris scan, or face etc and behavior characteristics as like hand-writing, voice, key stroke etc. Face recognition is very useful in many areas such as military, airports, universities, ATM, and banks etc, used for the security purposes. There are many techniques or algorithms that are used features extraction in face recognition. This paper make a review of some of those methods which are used for the face recognition that are Principal Component Analysis (PCA), Back Propagation Neural Networks (BPNN), Genetic Algorithm, and LDA, SVM, Independent Component Analysis(ICA). Each method has different -2 functions that are used for the face recognition. Dimensionality is reduced by using the Eigen face approach or PCA, LDA to extract the features from images. Genetic Algorithm is based on feature selection and Back propagation Neural Network (BPNN) is used for the classification of face images.},
author = {Kaur, Gurpreet and Kanwal, Navdeep},
file = {::},
isbn = {9789380544212},
journal = {International Conference on Computing for Sustainable Global Development (INDIACom)},
keywords = {BPNN,Face recognition,Features extraction,LDA,PCA},
pages = {2705--2710},
title = {{A comparative review of various approaches for feature extraction in face recognition}},
url = {http://ieeexplore.ieee.org/document/7724754/},
year = {2016}
}
@article{Krizhevsky2017b,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%}, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, con-sists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed reg-ularization method called " dropout " that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the sec-ond-best entry. 1. PROLOGUE Four years ago, a paper by Yann LeCun and his collaborators was rejected by the leading computer vision conference on the grounds that it used neural networks and therefore pro-vided no insight into how to design a vision system. At the time, most computer vision researchers believed that a vision system needed to be carefully hand-designed using a detailed understanding of the nature of the task. They assumed that the task of classifying objects in natural images would never be solved by simply presenting examples of images and the names of the objects they contained to a neural network that acquired all of its knowledge from this training data. What many in the vision research community failed to appreciate was that methods that require careful hand-engi-neering by a programmer who understands the domain do not scale as well as methods that replace the programmer with a powerful general-purpose learning procedure. With enough computation and enough data, learning beats pro-gramming for complicated tasks that require the integration of many different, noisy cues. Four years ago, while we were at the University of Toronto, our deep neural network called SuperVision almost halved the error rate for recognizing objects in natural images and triggered an overdue paradigm shift in computer vision. Figure 4 shows some examples of what SuperVision can do. SuperVision evolved from the multilayer neural networks that were widely investigated in the 1980s. These networks used multiple layers of feature detectors that were all learned from the training data. Neuroscientists and psychologists had hypothesized that a hierarchy of such feature detectors would provide a robust way to recognize objects but they had no idea how such a hierarchy could be learned. There was great excite-ment in the 1980s because several different research groups discovered that multiple layers of feature detectors could be trained efficiently using a relatively straight-forward algorithm called backpropagation},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {10.1145/3065386},
file = {::},
journal = {COMMUNICATIONS OF THE ACM},
number = {6},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://delivery.acm.org/10.1145/3070000/3065386/p84-krizhevsky.pdf?ip=130.225.198.196{\&}id=3065386{\&}acc=OA{\&}key=36332CD97FA87885.1DDFD8390336D738.4D4702B0C3E38B35.5945DC2EABF3343C{\&}{\_}{\_}acm{\_}{\_}=1518530104{\_}1e0b97ec1deaadc1937d34c07a1392ac},
volume = {60},
year = {2017}
}
@article{Abate2017,
abstract = {The increasing popularity of smartphones amongst the population laid the basis for a wide range of applications aimed at security and privacy protection. Very modern mobile devices have recently demonstrated the feasibility of using a camera sensor to access the system without typing any alphanumerical password. In this work, we present a method that implements iris recognition in the visible spectrum through unsupervised learning by means of Self Organizing Maps (SOM). The proposed method uses a SOM network to cluster iris features at pixel level. The discriminative feature map is obtained by using RGB data of the iris combined with the statistical descriptors of kurtosis and skewness. An experimental analysis on MICHE-I and UBIRISv1 datasets demonstrates the strengths and weaknesses of the algorithm, which has been specifically designed to require low processing power in compliance with the limited capability of common mobile devices.},
author = {Abate, Andrea F. and Barra, Silvio and Gallo, Luigi and Narducci, Fabio},
doi = {10.1016/j.patrec.2017.02.002},
file = {::},
isbn = {0000000000},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Iris recognition,Mobile biometric recognition,Statistical descriptors,Unsupervised learning},
pages = {37--43},
pmid = {11341202},
publisher = {Elsevier B.V.},
title = {{Kurtosis and skewness at pixel level as input for SOM networks to iris recognition on mobile devices}},
volume = {91},
year = {2017}
}
@article{Hu2015,
abstract = {Deep learning, in particular Convolutional Neural Network (CNN), has achieved promising results in face recognition recently. However, it remains an open question: why CNNs work well and how to design a 'good' architecture. The existing works tend to focus on reporting CNN architectures that work well for face recognition rather than investigate the reason. In this work, we conduct an extensive evaluation of CNN-based face recognition systems (CNN-FRS) on a common ground to make our work easily reproducible. Specifically, we use public database LFW (Labeled Faces in the Wild) to train CNNs, unlike most existing CNNs trained on private databases. We propose three CNN architectures which are the first reported architectures trained using LFW data. This paper quantitatively compares the architectures of CNNs and evaluate the effect of different implementation choices. We identify several useful properties of CNN-FRS. For instance, the dimensionality of the learned features can be significantly reduced without adverse effect on face recognition accuracy. In addition, traditional metric learning method exploiting CNN-learned features is evaluated. Experiments show two crucial factors to good CNN-FRS performance are the fusion of multiple CNNs and metric learning. To make our work reproducible, source code and models will be made publicly available.},
archivePrefix = {arXiv},
arxivId = {1504.02351},
author = {Hu, Guosheng and Yang, Yongxin and Yi, Dong and Kittler, Josef and Christmas, William and Li, Stan Z. and Hospedales, Timothy},
doi = {10.1109/ICCVW.2015.58},
eprint = {1504.02351},
file = {::},
isbn = {9780769557205},
issn = {15505499},
journal = {2015 IEEE International Conference on Computer Vision Workshop (ICCVW)},
month = {dec},
pages = {384--392},
publisher = {IEEE},
title = {{When Face Recognition Meets with Deep Learning: an Evaluation of Convolutional Neural Networks for Face Recognition}},
url = {http://ieeexplore.ieee.org/document/7406407/ http://arxiv.org/abs/1504.02351},
year = {2015}
}
@article{Uka2017a,
annote = {Keynotes:

CASIA (Most used database) and IIT Delhi Iris Database used.

Hough Transofrm algorithm used to detect boundaries},
author = {Uka, Arban and Ro{\c{c}}i, Albana and Ko{\c{c}}, Oktay},
file = {::},
isbn = {9781509038435},
journal = {IEEE EUROCON 2017 -17th International Conference on Smart Technologies},
keywords = {encoding,equal error rate,segmentation},
number = {July},
pages = {6--8},
title = {{Improved Segmentation Algorithm and Further Optimization for Iris Recognition}},
year = {2017}
}
@article{Chen2005a,
abstract = {The recognition accuracy of a single biometric authentication system is often much reduced due to the environment, user mode and physiological defects. In this paper, we combine face and iris features for developing a multimode biometric approach, which is able to diminish the drawback of single biometric approach as well as to improve the performance of authentication system. We combine a face database ORL and iris database CASIA to construct a multimodal biometric experimental database with which we validate the proposed approach and evaluate the multimodal biometrics performance. The experimental results reveal the multimodal biometrics verification is much more reliable and precise than single biometric approach.},
annote = {Face and iris for authentication
uses synthetic multimodal biometric dataset

Shows how to evaluate system.

proves multimodal performs better.},
author = {Chen, Ching-Han and {Te Chu}, Chia},
doi = {10.1007/11608288_76},
file = {::},
isbn = {978-3-540-31621-3},
issn = {03029743},
keywords = {face,iris,multimodal biometrics,wavelet probabilistic neural},
pages = {571--580},
title = {{Fusion of Face and Iris Features for Multimodal Biometrics}},
url = {http://link.springer.com/10.1007/11608288{\_}76},
year = {2005}
}
@article{Rattani2017,
abstract = {Ocular biometrics encompasses the imaging and use of characteristic features extracted from the eyes for personal recognition. Ocular biometric modalities in visible light have mainly focused on iris, blood vessel structures over the white of the eye (mostly due to conjunctival and episcleral layers), and periocular region around eye. Most of the existing studies on iris recognition use the near infrared spectrum. However, conjunctival vasculature and periocular regions are imaged in the visible spectrum. Iris recognition in the visible spectrum is possible for light color irides or by utilizing special illumination. Ocular recognition in the visible spectrum is an important research area due to factors such as recognition at a distance, suitability for recognition with regular RGB cameras, and adaptability to mobile devices. Further these ocular modalities can be obtained from a single RGB eye image, and then fused together for enhanced performance of the system. Despite these advantages, the state-of-the-art related to ocular biometrics in visible spectrum is not well known. This paper surveys this topic in terms of computational image enhancement, feature extraction, classification schemes and designed hardware-based acquisition set-ups. Future research directions are also enumerated to identify the path forward.},
annote = {A very detalied survey article. They go through a lot of different approaches to iris recognition. They also talk about the other biometrics that can be extracted from a VIS (Visible Spectrum) image like, moles/freckles/nevi. Other patterns can also b extraxted such as conjunctival vaslulature, periocular and retinal biometrics.

UBIRIS is one of the most used publicaly available databses for VIS iris images with noise.},
author = {Rattani, Ajita and Derakhshani, Reza},
doi = {10.1016/j.imavis.2016.11.019},
file = {::},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Biometrics,Conjunctival vasculature,Iris,Mobile biometrics,Ocular biometrics,Periocular biometrics,Visible spectrum},
pages = {1--16},
publisher = {Elsevier B.V.},
title = {{Ocular biometrics in the visible spectrum: A survey}},
url = {http://dx.doi.org/10.1016/j.imavis.2016.11.019},
volume = {59},
year = {2017}
}
@inproceedings{Misztal2012,
abstract = {A new method for finding the rotation angle in iris images for biometric identification is presented in this paper. The proposed approach is based on Fourier descriptors analysis and algebraic properties of vector rotation in complex space. {\textcopyright} 2012 IFIP International Federation for Information Processing.},
author = {Misztal, Krzysztof and Tabor, Jacek and Saeed, Khalid},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-33260-9-11},
file = {::},
isbn = {9783642332593},
issn = {03029743},
keywords = {Fourier descriptors,Iris pattern recognition,rotation estimation,rotation recovery},
pages = {135--145},
title = {{A new algorithm for rotation detection in iris pattern recognition}},
volume = {7564 LNCS},
year = {2012}
}
@article{Ma2015,
abstract = {This study presents a novel human-machine interface (HMI) based on both electrooculography (EOG) and electroencephalography (EEG). This hybrid interface works in two modes: an EOG mode recognizes eye movements such as blinks, and an EEG mode detects event related potentials (ERPs) like P300. While both eye movements and ERPs have been separately used for implementing assistive interfaces, which help patients with motor disabilities in performing daily tasks, the proposed hybrid interface integrates them together. In this way, both the eye movements and ERPs complement each other. Therefore, it can provide a better efficiency and a wider scope of application. In this study, we design a threshold algorithm that can recognize four kinds of eye movements including blink, wink, gaze, and frown. In addition, an oddball paradigm with stimuli of inverted faces is used to evoke multiple ERP components including P300, N170, and VPP. To verify the effectiveness of the proposed system, two different online experiments are carried out. One is to control a multifunctional humanoid robot, and the other is to control four mobile robots. In both experiments, the subjects can complete tasks effectively by using the proposed interface, whereas the best completion time is relatively short and very close to the one operated by hand.},
author = {Ma, Jiaxin and Zhang, Yu and Cichocki, Andrzej and Matsuno, Fumitoshi},
doi = {10.1109/TBME.2014.2369483},
isbn = {0018-9294},
issn = {15582531},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {Electroencephalogram (EEG),Electrooculogram (EOG),event-related potential (ERP),human-machine interface (HMI),robot control},
number = {3},
pages = {876--889},
pmid = {25398172},
title = {{A novel EOG/EEG hybrid human-machine interface adopting eye movements and ERPs: Application to robot control}},
volume = {62},
year = {2015}
}
@article{Leal-Taixe2017,
archivePrefix = {arXiv},
arxivId = {1704.02781},
author = {Leal-Taix{\'{e}}, Laura and Milan, Anton and Schindler, Konrad and Cremers, Daniel and Reid, Ian and Roth, Stefan},
eprint = {1704.02781},
file = {:home/nicstar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Leal-Taix{\'{e}} et al. - 2017 - Tracking the Trackers An Analysis of the State of the Art in Multiple Object Tracking.pdf:pdf},
month = {apr},
title = {{Tracking the Trackers: An Analysis of the State of the Art in Multiple Object Tracking}},
url = {https://arxiv.org/abs/1704.02781},
year = {2017}
}
@inproceedings{Sun2014,
abstract = {The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset, 99.15{\%} face verification accuracy is achieved. Compared with the best deep learning result on LFW, the error rate has been significantly reduced by 67{\%}.},
archivePrefix = {arXiv},
arxivId = {1406.4773},
author = {Sun, Yi and Wang, Xiaogang and Tang, Xiaoou},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.244},
eprint = {1406.4773},
isbn = {9781479951178},
issn = {10636919},
keywords = {deep learning,face verification},
pages = {1891--1898},
pmid = {21808091},
title = {{Deep Learning Face Representation by Joint Identification-Verification}},
url = {https://arxiv.org/pdf/1406.4773.pdf http://arxiv.org/abs/1406.4773},
year = {2014}
}
@article{Wrobel2017a,
abstract = {{\textcopyright} 2017 IEEE. In this paper, a new approach for personal identity verification using finger knuckle images and least-square contour alignment method has been proposed. A special test rig with a digital camera was prepared for acquisition the knuckle images. Next, the obtained images of finger knuckle were subjected to image processing method in order to extract the knuckle furrows from them. The verification of person was performed by comparing the furrows on the verified and the reference knuckle images. To determine the similarity between the furrows we used the least-square contour alignment method. The usability of the proposed approach was tested experimentally. Practical experiments, conducted with our database, confirmed that results obtained are promising.},
author = {Wrobel, K. and Porwik, P. and Doroz, R. and Safaverdi, H.},
doi = {10.1109/ICBAKE.2017.8090616},
isbn = {9781538634004},
journal = {Proceedings of 2017 International Conference on Biometrics and Kansei Engineering, ICBAKE 2017},
keywords = {biometrics,finger knuckle images,least-square contour alignment,verification},
pages = {119--122},
title = {{Person verification based on finger knuckle images and least-squares contour alignment}},
year = {2017}
}
@inproceedings{Eitel2015,
abstract = {Robust object recognition is a crucial ingredient of many, if not all, real-world robotics applications. This paper leverages recent progress on Convolutional Neural Networks (CNNs) and proposes a novel RGB-D architecture for object recognition. Our architecture is composed of two separate CNN processing streams - one for each modality - which are consecutively combined with a late fusion network. We focus on learning with imperfect sensor data, a typical problem in real-world robotics tasks. For accurate learning, we introduce a multi-stage training methodology and two crucial ingredients for handling depth data with CNNs. The first, an effective encoding of depth information for CNNs that enables learning without the need for large depth datasets. The second, a data augmentation scheme for robust learning with depth images by corrupting them with realistic noise patterns. We present state-of-the-art results on the RGB-D object dataset and show recognition in challenging RGB-D real-world noisy settings.},
archivePrefix = {arXiv},
arxivId = {1507.06821},
author = {Eitel, Andreas and Springenberg, Jost Tobias and Spinello, Luciano and Riedmiller, Martin and Burgard, Wolfram},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2015.7353446},
eprint = {1507.06821},
isbn = {9781479999941},
issn = {21530866},
month = {jul},
pages = {681--687},
title = {{Multimodal deep learning for robust RGB-D object recognition}},
url = {http://arxiv.org/abs/1507.06821},
volume = {2015-Decem},
year = {2015}
}
@article{Ho1994,
abstract = {A multiple classifier system is a powerful solution to difficult pattern$\backslash$nrecognition problems involving large class sets and noisy input$\backslash$nbecause it allows simultaneous use of arbitrary feature descriptors$\backslash$nand classification procedures. Decisions by the classifiers can$\backslash$nbe represented as rankings of classifiers and different instances$\backslash$nof a problem. The rankings can be combined by methods that either$\backslash$nreduce or rerank a given set of classes. An intersection method$\backslash$nand union method are proposed for class set reduction. Three methods$\backslash$nbased on the highest rank, the Borda count, and logistic regression$\backslash$nare proposed for class set reranking. These methods have been tested$\backslash$nin applications of degraded machine-printed characters and works$\backslash$nfrom large lexicons, resulting in substantial improvement in overall$\backslash$ncorrectness.},
author = {Ho, Tin Kam and Hull, Jonathan J and Srihari, Sargur N},
doi = {http://dx.doi.org/10.1109/34.273716},
isbn = {0162-8828 VO - 16},
issn = {0162-8828},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
number = {1},
pages = {66--75},
title = {{Decision Combination in Multiple Classifier Systems}},
volume = {16},
year = {1994}
}
@article{Ghazi2016,
abstract = {Deep learning based approaches have been dominating the face recognition field due to the significant performance improvement they have provided on the challenging wild datasets. These approaches have been extensively tested on such unconstrained datasets, on the Labeled Faces in the Wild and YouTube Faces, to name a few. However, their capability to handle individual appearance variations caused by factors such as head pose, illumination, occlusion, and misalignment has not been thoroughly assessed till now. In this paper, we present a comprehensive study to evaluate the performance of deep learning based face representation under several conditions including the varying head pose angles, upper and lower face occlusion, changing illumination of different strengths, and misalignment due to erroneous facial feature localization. Two successful and publicly available deep learning models, namely VGG-Face and Lightened CNN have been utilized to extract face representations. The obtained results show that although deep learning provides a powerful representation for face recognition, it can still benefit from preprocessing, for example, for pose and illumination normalization in order to achieve better performance under various conditions. Particularly, if these variations are not included in the dataset used to train the deep learning model, the role of preprocessing becomes more crucial. Experimental results also show that deep learning based representation is robust to misalignment and can tolerate facial feature localization errors up to 10{\%} of the interocular distance.},
archivePrefix = {arXiv},
arxivId = {1606.02894},
author = {Ghazi, Mostafa Mehdipour and Ekenel, Hazim Kemal},
doi = {10.1109/CVPRW.2016.20},
eprint = {1606.02894},
isbn = {9781509014378},
issn = {21607516},
journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
month = {jun},
pages = {102--109},
publisher = {IEEE},
title = {{A Comprehensive Analysis of Deep Learning Based Representation for Face Recognition}},
url = {http://ieeexplore.ieee.org/document/7789510/ http://arxiv.org/abs/1606.02894},
year = {2016}
}
@article{Crossdata2018,
abstract = {In this paper we study face recognition using convolutional neural network. First, we introduced the basic CNN neural network architecture. Second, we modify the traditional neural network and adapt it to another database by fine tuning its parameters. Third, the network architecture is extended to the cross database problem. The CNN is first trained on a large dataset and then tested on another. Experimental results show that the proposed algorithm is suitable for building various real world applications.},
author = {Guo, Mei and Xiao, Min and Gong, Deliang},
doi = {10.1007/978-3-319-69096-4_54},
file = {:home/nicstar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo, Xiao, Gong - 2017 - Face Recognition Using Deep Convolutional Neural Network in Cross-Database Study.pdf:pdf},
journal = {Advances in Intelligent Systems and Computing},
keywords = {Deep neural network {\'{A}},Face recognition {\'{A}},Image processing},
title = {{Face Recognition Using Deep Convolutional Neural Network in Cross-Database Study}},
url = {https://link-springer-com.zorac.aub.aau.dk/content/pdf/10.1007{\%}2F978-3-319-69096-4{\_}54.pdf},
year = {2017}
}
@article{lfw2007,
abstract = {Face recognition has benefitted greatly from the many databases that have been produced to study it. Most of these databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, expression, background, camera quality, occlusion, age, and gender. While there are many applications for face recognition technol- ogy in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database is provided as an aid in studying the latter, unconstrained, face recognition problem. The database represents an initial attempt to provide a set of labeled face photographs spanning the range of conditions typically encountered by people in their everyday lives. The database exhibits natural variability in pose, lighting, focus, resolution, facial expression, age, gender, race, accessories, make-up, occlusions, background, and photographic quality. Despite this variability, the images in the database are presented in a simple and consistent format for maximum ease of use. In addition to describing the details of the database and its acquisition, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible.},
author = {Huang, Gary B and Ramesh, Manu and Berg, Tamara and Learned-Miller, Erik},
doi = {10.1.1.122.8268},
isbn = {9781628414844},
issn = {1996756X},
journal = {University of Massachusetts Amherst Technical Report},
pages = {7--49},
title = {{Labeled faces in the wild: A database for studying face recognition in unconstrained environments}},
url = {http://vis-www.cs.umass.edu/lfw/lfw.pdf},
volume = {1},
year = {2007}
}
@article{Zhao2017b,
abstract = {Person re-identification (ReID) is an important task in video surveillance and has various applications. It is non-trivial due to complex background clutters, varying illu-mination conditions, and uncontrollable camera settings. Moreover, the person body misalignment caused by detec-tors or pose variations is sometimes too severe for feature matching across images. In this study, we propose a novel Convolutional Neural Network (CNN), called Spindle Net, based on human body region guided multi-stage feature de-composition and tree-structured competitive feature fusion. It is the first time human body structure information is con-sidered in a CNN framework to facilitate feature learning. The proposed Spindle Net brings unique advantages: 1) it separately captures semantic features from different body regions thus the macro-and micro-body features can be well aligned across images, 2) the learned region features from different semantic regions are merged with a competitive scheme and discriminative features can be well preserved. State of the art performance can be achieved on multiple datasets by large margins. We further demonstrate the ro-bustness and effectiveness of the proposed Spindle Net on our proposed dataset SenseReID without fine-tuning.},
author = {Zhao, Haiyu and Tian, Maoqing and Sun, Shuyang and Shao, Jing and Yan, Junjie and Yi, Shuai and Wang, Xiaogang and Tang, Xiaoou},
doi = {10.1109/CVPR.2017.103},
isbn = {978-1-5386-0457-1},
journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {907--915},
title = {{Spindle Net: Person Re-identification with Human Body Region Guided Feature Decomposition and Fusion}},
url = {http://ieeexplore.ieee.org/document/8099586/},
volume = {1},
year = {2017}
}
@article{BiosecID2008,
abstract = {A new multimodal biometric database, acquired in the framework of the BiosecurID project, is presented together with the description of the acquisition setup and protocol. The database includes eight unimodal biometric traits, namely: speech, iris, face (still images, videos of talking faces), handwritten signature and handwritten text (on-line dynamic signals, off-line scanned images), fingerprints (acquired with two different sensors), hand (palmprint, contour-geometry) and keystroking. The database comprises 400 subjects and presents features such as: realistic acquisition scenario, balanced gender and population distributions, availability of information about particular demographic groups (age, gender, handedness), acquisition of replay attacks for speech and keystroking, skilled forgeries for signatures, and compatibility with other existing databases. All these characteristics make it very useful in research and development of unimodal and multimodal biometric systems. {\textcopyright} Springer-Verlag London Limited 2009.},
author = {Fierrez, J. and Galbally, J. and Ortega-Garcia, J. and Freire, M. R. and Alonso-Fernandez, F. and Ramos, D. and Toledano, D. T. and Gonzalez-Rodriguez, J. and Siguenza, J. A. and Garrido-Salas, J. and Anguiano, E. and Gonzalez-de-Rivera, G. and Ribalda, R. and Faundez-Zanuy, M. and Ortega, J. A. and Carde{\~{n}}oso-Payo, V. and Viloria, A. and Vivaracho, C. E. and Moro, Q. I. and Igarza, J. J. and Sanchez, J. and Hernaez, I. and Orrite-Uru{\~{n}}uela, C. and Martinez-Contreras, F. and Gracia-Roche, J. J.},
doi = {10.1007/s10044-009-0151-4},
issn = {14337541},
journal = {Pattern Analysis and Applications},
keywords = {Biometrics,Database,Face,Fingerprint,Hand geometry,Handwriting,Iris,Keystroking,Multimodal,Palmprint,Signature,Speech},
number = {2},
pages = {235--246},
title = {{BiosecurID: A multimodal biometric database}},
volume = {13},
year = {2010}
}
@article{Abbas2017,
abstract = {Modern biometrics delivers an enhanced level of security by means of a “proof of property”. The design and deployment of a biometric system, however, hide many pitfalls, which, when underestimated, can lead to major security weaknesses and privacy threats. Issues of concern include biometric identity theft and privacy invasion because of the strong connection between a user and his identity. This book showcases a collection of comprehensive references on the advances of biometric security technology. It compiles a total of fourteen articles, all contributed by thirty-two eminent researchers in the field, thus providing concise and accessible coverage of not only general issues, but also state-of-the-art solutions. The book is divided into five parts: (1) Biometric Template Protection, which covers cancellable biometrics and parameter management protocol;(2) Biometric Key and Encryption, focusing on biometric key generation and visual biometric cryptography;(3) Biometric Systems Analysis, dealing with biometric system security, and privacy evaluation and assessment;(4) Privacy-Enhanced Biometric Systems, covering privacy-enhanced biometric system protocol design and implementation; and(5) Other Biometric Security Technologies.The book will be of particular interest to researchers, scholars, graduate students, engineers, practitioners and developers interested in security and privacy-related issues in biometric systems. It will also be attractive to managers of various organizations with strong security needs.},
author = {Abbas, Sherif N},
doi = {10.1007/978-3-319-47301-7},
isbn = {978-3-319-47300-0},
title = {{Biometric Security and Privacy}},
url = {http://link.springer.com/10.1007/978-3-319-47301-7},
year = {2017}
}
@article{Mhaske2013a,
author = {Mhaske, V. D. and Patankar, A. J.},
doi = {10.1109/ICCIC.2013.6724125},
isbn = {9781479915972},
journal = {2013 IEEE International Conference on Computational Intelligence and Computing Research, IEEE ICCIC 2013},
keywords = {Biometrics,Fingerprint,Fusion,MGF,Multimodal,Palm print,ROI,Unimodal},
title = {{Multimodal biometrics by integrating fingerprint and palmprint for security}},
year = {2013}
}
@article{Dessimoz2007,
abstract = {The MBioID initiative has been set up to address the following germane question: What and how biometric technologies could be deployed in identity documents in the foreseeable future? This research effort proposes to look at current and future practices and systems of establishing and using biometric identity documents (IDs) and evaluate their effectiveness in large-scale developments. The first objective of the MBioID project is to present a review document establishing the current state-of-the-art related to the use of multimodal biometrics in an IDs application. This research report gives the main definitions, properties and the framework of use related to biometrics, an overview of the main standards developed in the biometric industry and standardisation organisations to ensure interoperability, as well as some of the legal framework and the issues associated to biometrics such as privacy and personal data protection. The state-of-the-art in terms of technological development is also summarised for a range of single biometric modalities (2D and 3D face, fingerprint, iris, on-line signature and speech), chosen according to ICAO recommendations and availabilities, and for various multimodal approaches. This paper gives a summary of the main elements of that report. The second objective of the MBioID project is to propose relevant acquisition and evaluation protocols for a large-scale deployment of biometric IDs. Combined with the protocols, a multimodal database will be acquired in a realistic way, in order to be as close as possible to a real biometric IDs deployment. In this paper, the issues and solutions related to the acquisition setup are briefly presented. {\textcopyright} 2006 Elsevier Ireland Ltd. All rights reserved.},
author = {Dessimoz, Damien and Richiardi, Jonas and Champod, Christophe and Drygajlo, Andrzej},
doi = {10.1016/j.forsciint.2006.06.037},
issn = {03790738},
journal = {Forensic Science International},
keywords = {Acquisition protocol,Biometrics,Electronic passport,Evaluation protocol,Identity documents,Multimodality},
number = {2-3},
pages = {154--159},
pmid = {16890391},
title = {{Multimodal biometrics for identity documents ({\{}A figure is presented{\}})}},
volume = {167},
year = {2007}
}
@article{Proenca2017,
abstract = {The effectiveness of current iris recognition systems de-pends on the accurate segmentation and parameterisation of the iris boundaries, as failures at this point misalign the coefficients of the biometric signatures. This paper de-scribes IRINA, an algorithm for Iris Recognition that is ro-bust against INAccurately segmented samples, which makes it a good candidate to work in poor-quality data. The pro-cess is based in the concept of " corresponding " patch be-tween pairs of images, that is used to estimate the posterior probabilities that patches regard the same biological region, even in case of segmentation errors and non-linear texture deformations. Such information enables to infer a free-form deformation field (2D registration vectors) between images, whose first and second-order statistics provide effective bio-metric discriminating power. Extensive experiments were carried out in four datasets (CASIA-IrisV3-Lamp, CASIA-IrisV4-Lamp, CASIA-IrisV4-Thousand and WVU) and show that IRINA not only achieves state-of-the-art performance in good quality data, but also handles effectively severe seg-mentation errors and large differences in pupillary dilation / constriction.},
author = {Proenca, Hugo and Neves, Joao C},
doi = {10.1109/CVPR.2017.714},
isbn = {978-1-5386-0457-1},
journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {6747--6756},
title = {{IRINA: Iris Recognition (Even) in Inaccurately Segmented Data}},
url = {http://ieeexplore.ieee.org/document/8100197/},
year = {2017}
}
@inproceedings{Sangeetha2013,
abstract = {-In a Multimodal biometric system, the effective fusion method is necessary for combining information from various single modality systems. Two biometric characteristics are considered in this study: iris and fingerprint. Multimodal biometric system needs an effective fusion scheme to combine biometric characteristics derived from one or more modalities. The score level fusion is used to combine the characteristics from diff erent biometric modalities. Fusion at the score level is a new technique, which has a high potential for e f ficient consolidation of multiple unimodal biometric matcher outputs. Support vector machine and extreme learning techniques are used in this system for recognition of biometric traits. In this, the Fingerprint-Iris system provides better performance and comparison of support vector machine and extreme learning machine based on score-level fusion methods is obtained In score-level fusion, ELM provides better performance as compare to the SVM It reduces the classification time of current system. This work is valuable and makes an e f ficient accuracy in such applications. This system can be utilized for person identification in several applications.},
author = {Sangeetha, S and Radha, N.},
booktitle = {2013 7th International Conference on Intelligent Systems and Control (ISCO)},
doi = {10.1109/ISCO.2013.6481145},
isbn = {978-1-4673-4603-0},
month = {jan},
pages = {183--188},
publisher = {IEEE},
title = {{A New Framework for IRIS and Fingerprint Recognition Using SVM Classification and Extreme Learning Machine Based on Score Level Fusion}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6481145},
year = {2013}
}
@article{Elrefaei2017,
author = {Elrefaei, Lamiaa A and Hamid, Doaa H and Bayazed, Afnan A and Bushnak, Sara S and Maasher, Shaikhah Y},
doi = {10.1007/s11042-017-5049-3},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {Biometrics,Deep Sparse Filter,Hough transform,Iris recognition,Visible Light},
pages = {1--25},
publisher = {Multimedia Tools and Applications},
title = {{Developing Iris Recognition System for Smartphone Security}},
year = {2017}
}
@article{Zapata2017a,
author = {Zapata, J C and Duque, C M and Gonzalez, M E},
doi = {10.1007/978-981-10-5427-3},
isbn = {978-981-10-5426-6},
keywords = {biometric {\'{a}} data fusion,signals {\'{a}} signal processing},
number = {i},
pages = {721--733},
title = {{Advances in Computing and Data Sciences}},
url = {http://link.springer.com/10.1007/978-981-10-5427-3},
volume = {721},
year = {2017}
}
@article{Cheng2017a,
abstract = {This paper focuses on indoor semantic segmentation us-ing RGB-D data. Although the commonly used deconvolu-tion networks (DeconvNet) have achieved impressive results on this task, we find there is still room for improvements in two aspects. One is about the boundary segmentation. DeconvNet aggregates large context to predict the label of each pixel, inherently limiting the segmentation precision of object boundaries. The other is about RGB-D fusion. Re-cent state-of-the-art methods generally fuse RGB and depth networks with equal-weight score fusion, regardless of the varying contributions of the two modalities on delineating different categories in different scenes. To address the two problems, we first propose a locality-sensitive DeconvNet (LS-DeconvNet) to refine the boundary segmentation over each modality. LS-DeconvNet incorporates locally visual and geometric cues from the raw RGB-D data into each DeconvNet, which is able to learn to upsample the coarse convolutional maps with large context whilst recovering sharp object boundaries. Towards RGB-D fusion, we introduce a gated fusion layer to effectively combine the two LS-DeconvNets. This layer can learn to adjust the contributions of RGB and depth over each pixel for high-performance object recognition. Experiments on the large-scale SUN RGB-D dataset and the popular NYU-Depth v2 dataset show that our approach achieves new state-of-the-art results for RGB-D indoor semantic segmentation.},
author = {Cheng, Yanhua and Cai, Rui and Li, Zhiwei and Zhao, Xin and Huang, Kaiqi},
doi = {10.1109/CVPR.2017.161},
isbn = {978-1-5386-0457-1},
journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {1475--1483},
title = {{Locality-Sensitive Deconvolution Networks with Gated Fusion for RGB-D Indoor Semantic Segmentation}},
url = {http://ieeexplore.ieee.org/document/8099644/},
year = {2017}
}
@inproceedings{Kaur2016a,
abstract = {Abstract: Face recognition is a type of biometric software application by using which, we can analyzing, identifying or verifying digital image of the person by using the feature of the face of the person that are unique characteristics of each person. These characteristics may be physical or behavioral. The physiological characteristics as like finger print, iris scan, or face etc and behavior characteristics as like hand-writing, voice, key stroke etc. Face recognition is very useful in many areas such as military, airports, universities, ATM, and banks etc, used for the security purposes. There are many techniques or algorithms that are used features extraction in face recognition. This paper make a review of some of those methods which are used for the face recognition that are Principal Component Analysis (PCA), Back Propagation Neural Networks (BPNN), Genetic Algorithm, and LDA, SVM, Independent Component Analysis(ICA). Each method has different -2 functions that are used for the face recognition. Dimensionality is reduced by using the Eigen face approach or PCA, LDA to extract the features from images. Genetic Algorithm is based on feature selection and Back propagation Neural Network (BPNN) is used for the classification of face images.},
author = {Kaur, Gurpreet and Kanwal, Navdeep},
booktitle = {International Conference on Computing for Sustainable Global Development (INDIACom)},
isbn = {9789380544212},
pages = {2705--2710},
publisher = {IEEE},
title = {{A Comparative Review of Various Approaches for Feature Extraction in Face Recognition}},
year = {2016}
}
@article{Zhang2017a,
abstract = {Multimodal classification arises in many computer vi-sion tasks such as object classification and image retrieval. The idea is to utilize multiple sources (modalities) measur-ing the same instance to improve the overall performance compared to using a single source (modality). The varying characteristics exhibited by multiple modalities make it nec-essary to simultaneously learn the corresponding distance metrics. In this paper, we propose a multiple metrics learn-ing algorithm for multimodal data. Metric of each modal-ity is product of two matrices: one matrix is modality spe-cific, the other is enforced to be shared by all the modalities. The learned metrics can improve multimodal classification accuracy and experimental results on four datasets show that the proposed algorithm outperforms existing learning algorithms based on multiple metrics as well as other ap-proaches tested on these datasets. Specifically, we report 95.0{\%} object instance recognition accuracy, 89.2{\%} object category recognition accuracy on the multi-view RGB-D dataset and 52.3{\%} scene category recognition accuracy on SUN RGB-D dataset.},
author = {Zhang, Heng and Patel, Vishal M. and Chellappa, Rama},
doi = {10.1109/CVPR.2017.312},
isbn = {978-1-5386-0457-1},
issn = {1063-6919},
journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {2925--2933},
title = {{Hierarchical Multimodal Metric Learning for Multimodal Classification}},
url = {http://ieeexplore.ieee.org/document/8099795/},
year = {2017}
}
@article{Pathak2016a,
author = {Pathak, Mrunal and Srinivasu, N},
doi = {10.1007/978-981-10-2630-0},
isbn = {978-981-10-2629-4},
keywords = {biometrics},
pages = {137--152},
title = {{Advances in Computing Applications}},
url = {http://link.springer.com/10.1007/978-981-10-2630-0},
year = {2016}
}
@article{Yang2017a,
abstract = {We present an end-to-end, multimodal, fully convolutional$\backslash$r$\backslash$nnetwork for extracting semantic structures from document$\backslash$r$\backslash$nimages. We consider document semantic structure$\backslash$r$\backslash$nextraction as a pixel-wise segmentation task, and propose a$\backslash$r$\backslash$nunified model that classifies pixels based not only on their$\backslash$r$\backslash$nvisual appearance, as in the traditional page segmentation$\backslash$r$\backslash$ntask, but also on the content of underlying text. Moreover,$\backslash$r$\backslash$nwe propose an efficient synthetic document generation process$\backslash$r$\backslash$nthat we use to generate pretraining data for our network.$\backslash$r$\backslash$nOnce the network is trained on a large set of synthetic$\backslash$r$\backslash$ndocuments, we fine-tune the network on unlabeled real documents$\backslash$r$\backslash$nusing a semi-supervised approach. We systematically$\backslash$r$\backslash$nstudy the optimum network architecture and show that$\backslash$r$\backslash$nboth our multimodal approach and the synthetic data pretraining$\backslash$r$\backslash$nsignificantly boost the performance.},
archivePrefix = {arXiv},
arxivId = {1706.02337},
author = {Yang, Xiao and Yumer, Ersin and Asente, Paul and Kraley, Mike and Kifer, Daniel and Giles, C. Lee},
doi = {10.1109/CVPR.2017.462},
eprint = {1706.02337},
isbn = {978-1-5386-0457-1},
journal = {Cvpr2017},
title = {{Learning to Extract Semantic Structure from Documents Using Multimodal Fully Convolutional Neural Networks}},
url = {https://arxiv.org/pdf/1706.02337.pdf},
year = {2017}
}
@article{Dhillon2009,
abstract = {This paper discusses a brain-computer interface through electrooculogram (EOG) and electromyogram (EMG) signals. In situations of disease or trauma, there may be inability to communicate with others through means such as speech or typing. Eye movement tends to be one of the last remaining active muscle capabilities for people with neurodegenerative disorders, such as amyotrophic lateral sclerosis (ALS) also known as Lou Gehrig's disease. Thus, there is a need for eye movement based systems to enable communication. To meet this need, we proposed a system to accept eye-gaze controlled navigation of a particular letter and EMG based click to enter the letter. Eye -gaze direction (angle) is obtained from EOG signals and EMG signal is recorded from eyebrow muscle activity. A virtual screen keyboard may be used to examine the usability of the proposed system.},
author = {Dhillon, Hari Singh and Singla, Rajesh and Rekhi, Navleen Singh and Jha, Rameshwar},
doi = {10.1109/ICCSIT.2009.5234951},
isbn = {9781424445196},
journal = {Proceedings - 2009 2nd IEEE International Conference on Computer Science and Information Technology, ICCSIT 2009},
keywords = {Amyotrophic lateral sclerosis (ALS),EMG,EOG,Eye-gaze,Virtual keyboard},
pages = {259--262},
title = {{EOG and EMG based virtual keyboard: A brain-computer interface}},
year = {2009}
}
@article{Karpathy2016,
abstract = {These notes accompany the Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition.},
author = {Karpathy, Andrej},
journal = {Stanford University},
pages = {1--21},
title = {{Convolutional Neural Networks for Visual Recognition}},
url = {http://cs231n.github.io/classification/},
year = {2016}
}
@article{Gulmire2012,
author = {Gulmire, Kshamaraj and Ganorkar, Sanjay},
issn = {2278-0181},
number = {5},
pages = {1--5},
title = {{Iris Recognition Using Gabor Wavelet}},
volume = {1},
year = {2012}
}
@article{Jung2017,
abstract = {Finding the accurate position of an eye is crucial for mobile iris recognition system in order to extract the iris region quickly and correctly. Unfortunately, this is very difficult to accomplish when a person is wearing eyeglasses because of the interference from the eyeglasses. This paper proposes an eye detection method that is robust to eyeglass interference in mobile environment. The proposed method comprises two stages: eye candidate generation and eye validation. In the eye candidate generation stage, multi-scale window masks consisting of 2 × 3 subblocks are used to generate all image blocks possibly containing an eye image. In the ensuing eye validation stage, two methods are employed to determine which blocks actually contain true eye images and locate their precise positions as well: the first method searches for the glint of an NIR illuminator on the pupil region. If this first method fails, the next method computes the intensity difference between the assumed pupil and its surrounding region using multi-scale 3 × 3 window masks. Experimental results show that the proposed method detects the eye position more accurately and quickly than competing methods in the presence of interference from eyeglass frames.},
author = {Jung, Yujin and Kim, Dongik and Son, Byungjun and Kim, Jaihie},
doi = {10.1016/j.eswa.2016.09.036},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Eye detection,Eye validation,Eyeglasses,Iris detection,Iris recognition,Mobile},
pages = {178--188},
publisher = {Elsevier Ltd},
title = {{An eye detection method robust to eyeglasses for mobile iris recognition}},
url = {http://dx.doi.org/10.1016/j.eswa.2016.09.036},
volume = {67},
year = {2017}
}
@article{Lee2017,
abstract = {In recent years, the iris recognition system has been gaining increasing acceptance for applications such as access control and smartphone security. When the images of the iris are obtained under unconstrained conditions, an issue of undermined quality is caused by optical and motion blur, off-angle view (the user's eyes looking somewhere else, not into the front of the camera), specular reflection (SR) and other factors. Such noisy iris images increase intra-individual variations and, as a result, reduce the accuracy of iris recognition. A typical iris recognition system requires a near-infrared (NIR) illuminator along with an NIR camera, which are larger and more expensive than fingerprint recognition equipment. Hence, many studies have proposed methods of using iris images captured by a visible light camera without the need for an additional illuminator. In this research, we propose a new recognition method for noisy iris and ocular images by using one iris and two periocular regions, based on three convolutional neural networks (CNNs). Experiments were conducted by using the noisy iris challenge evaluation-part II (NICE.II) training dataset (selected from the university of Beira iris (UBIRIS).v2 database), mobile iris challenge evaluation (MICHE) database, and institute of automation of Chinese academy of sciences (CASIA)-Iris-Distance database. As a result, the method proposed by this study outperformed previous methods.},
author = {Lee, Min Beom and Hong, Hyung Gil and Park, Kang Ryoung},
doi = {10.3390/s17122933},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Convolutional neural network,Iris and periocular,Noisy iris and ocular image},
number = {12},
pmid = {29258217},
title = {{Noisy ocular recognition based on three convolutional neural networks}},
volume = {17},
year = {2017}
}
@article{Ribeiro2017,
author = {Ribeiro, Eduardo and Uhl, Andreas and Alonso-Fernandez, Fernando and Farrugia, Reuben A},
doi = {10.23919/EUSIPCO.2017.8081595},
isbn = {978-0-9928626-7-1},
journal = {2017 25th European Signal Processing Conference (EUSIPCO)},
pages = {2176--2180},
title = {{Exploring deep learning image super-resolution for iris recognition}},
url = {http://ieeexplore.ieee.org/document/8081595/},
volume = {2},
year = {2017}
}
@article{Yilmaz2006,
abstract = {The goal of this article is to review the state-of-the-art tracking methods, classify them into different categories , and identify new trends. Object tracking, in general, is a challenging problem. Difficulties in tracking objects can arise due to abrupt object motion, changing appearance patterns of both the object and the scene, nonrigid object structures, object-to-object and object-to-scene occlusions, and camera motion. Tracking is usually performed in the context of higher-level applications that require the location and/or shape of the object in every frame. Typically, assumptions are made to constrain the tracking problem in the context of a particular application. In this survey, we categorize the tracking methods on the basis of the object and motion representations used, provide detailed descriptions of representative methods in each category, and examine their pros and cons. Moreover, we discuss the important issues related to tracking including the use of appropriate image features, selection of motion models, and detection of objects.},
author = {Yilmaz, A. and Javed, O. and Shah, M.},
doi = {10.1145/1177352.1177355},
file = {:home/nicstar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Acm Reference Format Yilmaz, Javed, Shah - 2006 - Object tracking A survey.pdf:pdf},
journal = {ACM Comput. Surv},
keywords = {I48 [Image Processing and Computer Vision]: Scene,contour evolution,feature selection,object detection,object representation,point tracking,shape tracking},
pages = {45},
title = {{Object tracking: A survey}},
url = {http://doi.acm.org/10.1145/1177352.1177355},
volume = {38},
year = {2006}
}
@article{Furman1950,
abstract = {Axonal sprouting of excitatory neurons is frequently observed in{\$}\backslash{\$}ntemporal lobe epilepsy, but the extent to which inhibitory interneurons{\$}\backslash{\$}nundergo similar axonal reorganization remains unclear. The goal of this{\$}\backslash{\$}nstudy was to determine whether somatostatin (SOM)-expressing neurons in{\$}\backslash{\$}nstratum (s.) oriens of the hippocampus exhibit axonal sprouting beyond{\$}\backslash{\$}ntheir normal territory and innervate granule cells of the dentate gyrus{\$}\backslash{\$}nin a pilocarpine model of epilepsy. To obtain selective labeling of{\$}\backslash{\$}nSOM-expressing neurons in s. oriens, a Cre recombinase-dependent{\$}\backslash{\$}nconstruct for channelrhodopsin2 fused to enhanced yellow fluorescent{\$}\backslash{\$}nprotein (ChR2-eYFP) was virally delivered to this region in SOM-Cre{\$}\backslash{\$}nmice. In control mice, labeled axons were restricted primarily to s.{\$}\backslash{\$}nlacunosum-moleculare. However, in pilocarpine-treated animals, a rich{\$}\backslash{\$}nplexus of ChR2-eYFP-labeled fibers and boutons extended into the dentate{\$}\backslash{\$}nmolecular layer. Electron microscopy with immunogold labeling{\$}\backslash{\$}ndemonstrated labeled axon terminals that formed symmetric synapses on{\$}\backslash{\$}ndendritic profiles in this region, consistent with innervation of{\$}\backslash{\$}ngranule cells. Patterned illumination of ChR2-labeled fibers in s.{\$}\backslash{\$}nlacunosum-moleculare of CA1 and the dentate molecular layer elicited{\$}\backslash{\$}nGABAergic inhibitory responses in dentate granule cells in{\$}\backslash{\$}npilocarpine-treated mice but not in controls. Similar optical{\$}\backslash{\$}nstimulation in the dentate hilus evoked no significant responses in{\$}\backslash{\$}ngranule cells of either group of mice. These findings indicate that{\$}\backslash{\$}nunder pathological conditions, SOM/GABAergic neurons can undergo{\$}\backslash{\$}nsubstantial axonal reorganization beyond their normal territory and{\$}\backslash{\$}nestablish aberrant synaptic connections. Such reorganized circuitry{\$}\backslash{\$}ncould contribute to functional deficits in inhibition in epilepsy,{\$}\backslash{\$}ndespite the presence of numerous GABAergic terminals in the region.},
author = {Furman, Joseph M and Wuyts, Floris L and Siddiqui, Uzma and Shaikh, A N and Lord, Mary P and Wright, W D and Colegatet, Robert L and Hoffman, James E},
doi = {10.1016/B978-1-4557-0308-1.00032-7},
edition = {6},
isbn = {978-1-4665-5543-3},
issn = {2278-1021},
journal = {Reports on Progress in Physics},
keywords = {adc,analogdigitalconverter,eeg,electroencefalogram,electromyalgy,electrooculography,emg,eog,rapid eye movement,rem,sem,slow eye movement},
number = {1},
pages = {4328--4330},
publisher = {Elsevier Inc.},
title = {{The investigation of eye movements}},
url = {www.ijarcce.com http://dx.doi.org/10.1016/B978-1-4557-0308-1.00032-7},
volume = {2},
year = {1950}
}
@article{Saha2017,
annote = {Not a very good article. but it has some nice references.

Keypoints

It is possible to aquire iris images in multiple way including Near Infrared (NIRD). a simple lense and monochome CCD camera, Adaboost cascade iris detector.

Iris localization is done by Daugman using a 2D Gabor Filter and Fisher Linear Discriminate method.

To help localize the iris despite of eyelashes a 1D rankfilter and histogram filter can be used.},
author = {Saha, Rishmita and Kundu, Mahasweta and Dutta, Madhuparna and Majumder, Rahul and Mukherjee, Debosmita and Pramanik, Sayak and Thakur, Uttam Narendra and Mukherjee, Chiradeep},
isbn = {9781538633717},
journal = {Information Technology, Electronics and Mobile Communication Conference (IEMCON), 2017 8th IEEE Annual},
pages = {685--688},
title = {{A Brief Study on Evolution of Iris Recognition System}},
url = {http://ieeexplore.ieee.org.ezproxy.psu.edu.sa/stamp/stamp.jsp?arnumber=8117234},
year = {2017}
}
@article{Redmon2015,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
archivePrefix = {arXiv},
arxivId = {1506.02640},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
eprint = {1506.02640},
file = {:home/nicstar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon et al. - 2015 - You Only Look Once Unified, Real-Time Object Detection.pdf:pdf},
month = {jun},
title = {{You Only Look Once: Unified, Real-Time Object Detection}},
url = {http://arxiv.org/abs/1506.02640},
year = {2015}
}
@article{B2017,
author = {B, Qi Wang and Su, Xia and Cai, Zhenlin and Zhang, Xiangde},
doi = {10.1007/978-3-319-69923-3},
isbn = {978-3-319-69922-6},
keywords = {joint bayesian,mobile iris recognition,ordinal measures},
number = {3},
pages = {401--410},
title = {{Biometric Recognition}},
url = {http://link.springer.com/10.1007/978-3-319-69923-3},
volume = {10568},
year = {2017}
}
@article{Rattani2017,
abstract = {Ocular biometrics encompasses the imaging and use of characteristic features extracted from the eyes for personal recognition. Ocular biometric modalities in visible light have mainly focused on iris, blood vessel structures over the white of the eye (mostly due to conjunctival and episcleral layers), and periocular region around eye. Most of the existing studies on iris recognition use the near infrared spectrum. However, conjunctival vasculature and periocular regions are imaged in the visible spectrum. Iris recognition in the visible spectrum is possible for light color irides or by utilizing special illumination. Ocular recognition in the visible spectrum is an important research area due to factors such as recognition at a distance, suitability for recognition with regular RGB cameras, and adaptability to mobile devices. Further these ocular modalities can be obtained from a single RGB eye image, and then fused together for enhanced performance of the system. Despite these advantages, the state-of-the-art related to ocular biometrics in visible spectrum is not well known. This paper surveys this topic in terms of computational image enhancement, feature extraction, classification schemes and designed hardware-based acquisition set-ups. Future research directions are also enumerated to identify the path forward.},
annote = {A very detalied survey article. They go through a lot of different approaches to iris recognition. They also talk about the other biometrics that can be extracted from a VIS (Visible Spectrum) image like, moles/freckles/nevi. Other patterns can also b extraxted such as conjunctival vaslulature, periocular and retinal biometrics.

UBIRIS is one of the most used publicaly available databses for VIS iris images with noise.},
author = {Rattani, Ajita and Derakhshani, Reza},
doi = {10.1016/j.imavis.2016.11.019},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Biometrics,Conjunctival vasculature,Iris,Mobile biometrics,Ocular biometrics,Periocular biometrics,Visible spectrum},
pages = {1--16},
publisher = {Elsevier B.V.},
title = {{Ocular biometrics in the visible spectrum: A survey}},
url = {http://dx.doi.org/10.1016/j.imavis.2016.11.019},
volume = {59},
year = {2017}
}
@article{Abate2017,
abstract = {The increasing popularity of smartphones amongst the population laid the basis for a wide range of applications aimed at security and privacy protection. Very modern mobile devices have recently demonstrated the feasibility of using a camera sensor to access the system without typing any alphanumerical password. In this work, we present a method that implements iris recognition in the visible spectrum through unsupervised learning by means of Self Organizing Maps (SOM). The proposed method uses a SOM network to cluster iris features at pixel level. The discriminative feature map is obtained by using RGB data of the iris combined with the statistical descriptors of kurtosis and skewness. An experimental analysis on MICHE-I and UBIRISv1 datasets demonstrates the strengths and weaknesses of the algorithm, which has been specifically designed to require low processing power in compliance with the limited capability of common mobile devices.},
author = {Abate, Andrea F and Barra, Silvio and Gallo, Luigi and Narducci, Fabio},
doi = {10.1016/j.patrec.2017.02.002},
isbn = {0000000000},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Iris recognition,Mobile biometric recognition,Statistical descriptors,Unsupervised learning},
pages = {37--43},
pmid = {11341202},
publisher = {Elsevier B.V.},
title = {{Kurtosis and skewness at pixel level as input for SOM networks to iris recognition on mobile devices}},
volume = {91},
year = {2017}
}
@article{Zhang2016,
author = {Zhang, Man and Zhang, Qi and Sun, Zhenan and Zhou, Shujuan and Ahmed, Nasir Uddin},
doi = {10.1109/BTAS.2016.7791191},
isbn = {9781467397339},
journal = {2016 IEEE 8th International Conference on Biometrics Theory, Applications and Systems, BTAS 2016},
title = {{The BTAS∗Competition on Mobile Iris Recognition}},
year = {2016}
}
@article{Daia,
abstract = {Deep networks have shown impressive performance on many computer vision tasks. Recently, deep convolutional neural networks (CNNs) have been used to learn discrim-inative texture representations. One of the most successful approaches is Bilinear CNN model that explicitly captures the second order statistics within deep features. However, these networks cut off the first order information flow in the deep network and make gradient back-propagation dif-ficult. We propose an effective fusion architecture -FASON that combines second order information flow and first or-der information flow. Our method allows gradients to back-propagate through both flows freely and can be trained ef-fectively. We then build a multi-level deep architecture to exploit the first and second order information within dif-ferent convolutional layers. Experiments show that our method achieves improvements over state-of-the-art meth-ods on several benchmark datasets.},
author = {Dai, Xiyang and Ng, Joe Yue-hei and Davis, Larry S},
journal = {Cvpr2017},
pages = {7352--7360},
title = {{FASON : First and Second Order Information Fusion Network for Texture Recognition}}
}
@article{Sung2013,
abstract = {This study uses ZigBee wireless sensor network technology to build a Multi-purpose Electronic Score system based on the electro-oculogram (EOG). As a requirement when learning to play a musical instrument the most commonly seen scores are available in paper copies. The disadvantage of the paper music score is that pages must be turned manually by the performer while playing music. An alternative is to prepare a miniaturized copy of the score, but this makes the score difficult to read clearly. Electronic scores have recently become available and are expected to become mainstream in the future due to the rapid growth in ebook readers and tablet PCs. Tracking the upward, downward, leftward, rightward and clockwise/counterclockwise eyeball movements and eye blinking, an amplified, bandpass filtered electro-oculogram (EOG) signal is converted into digital form. This signal is further transmitted through a ZigBee wireless module on which an electronic score program is installed, including the following operating modes; go to the previous/next page, recording, tuning and tempo modes. Detected eyeball movements allow a score to be viewed conveniently while playing. In the future, the proposed technology can be applied to flexible paper, flexible displays and the like. {\textcopyright}2012 Elsevier B.V.},
author = {Sung, Wen Tsai and Chen, Jui Ho and Chang, Kuo Yi},
doi = {10.1016/j.sna.2012.11.028},
isbn = {0924-4247},
issn = {09244247},
journal = {Sensors and Actuators, A: Physical},
keywords = {EOG,Multipurpose Electronic Score,Wireless Sensor Network,ZigBee},
pages = {141--152},
publisher = {Elsevier B.V.},
title = {{ZigBee based multi-purpose electronic score design and implementation using EOG}},
url = {http://dx.doi.org/10.1016/j.sna.2012.11.028},
volume = {190},
year = {2013}
}
@article{Berg1991,
author = {Berg, P and Scherg, M},
journal = {Clinical Physiology and Physiological Measures},
keywords = {ERP eye ocular},
pages = {49--54},
title = {{Dipole modelling of eye activity and its application to the removal of eye artifacts from the EEG and MEG}},
volume = {12},
year = {1991}
}
@article{Karpathy2016,
abstract = {These notes accompany the Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition.},
author = {Karpathy, Andrej},
journal = {Stanford University},
pages = {1--21},
title = {{Convolutional Neural Networks for Visual Recognition}},
url = {http://cs231n.github.io/classification/},
year = {2016}
}
@article{Nielsen2015,
abstract = {Neural Networks and Deep Learning is a free online book. The book will teach you about: Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data Deep learning, a powerful set of techniques for learning in neural networks Neural networks and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing. This book will teach you many of the core concepts behind neural networks and deep learning.},
author = {Nielsen, Michael},
journal = {Determination Press},
title = {{Neural Networks and Deep Learning}},
url = {http://neuralnetworksanddeeplearning.com/index.html},
year = {2015}
}
@article{Rifaee2017,
author = {Rifaee, Mustafa and Abdallah, Mohammad and Okosh, Basem},
journal = {international journal of multimedia {\&} its applications},
number = {April},
title = {{A Short Survey for Iris Images Databases}},
url = {https://www.researchgate.net/publication/316093004{\_}A{\_}Short{\_}Survey{\_}for{\_}Iris{\_}Images{\_}Databases},
year = {2017}
}
@inproceedings{deepID2014,
abstract = {The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset, 99.15{\%} face verification accuracy is achieved. Compared with the best deep learning result on LFW, the error rate has been significantly reduced by 67{\%}.},
archivePrefix = {arXiv},
arxivId = {1406.4773},
author = {Sun, Yi and Wang, Xiaogang and Tang, Xiaoou},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.244},
eprint = {1406.4773},
isbn = {9781479951178},
issn = {10636919},
keywords = {deep learning,face verification},
pages = {1891--1898},
pmid = {21808091},
title = {{Deep learning face representation from predicting 10,000 classes}},
url = {http://mmlab.ie.cuhk.edu.hk/pdf/YiSun{\_}CVPR14.pdf},
year = {2014}
}
@article{Li2017,
abstract = {This paper investigates how to integrate the complementary information from RGB and thermal (RGB-T) sources for object tracking. We propose a novel Convolutional Neural Network (ConvNet) architecture, including a two-stream ConvNet and a FusionNet, to achieve adaptive fusion of different source data for robust RGB-T tracking. Both RGB and thermal streams extract generic semantic information of the target object. In particular, the thermal stream is pre-trained on the ImageNet dataset to encode rich semantic information, and then fine-tuned using thermal images to capture the specific properties of thermal information. For adaptive fusion of different modalities while avoiding redundant noises, the FusionNet is employed to select most discriminative feature maps from the outputs of the two-stream ConvNet, and updated online to adapt to appearance variations of the target object. Finally, the object locations are efficiently predicted by applying the multi-channel correlation filter on the fused feature maps. Extensive experiments on the recently public benchmark GTOT verify the effectiveness of the proposed approach against other state-of-the-art RGB-T trackers.},
author = {Li, Chenglong and Wu, Xiaohao and Zhao, Nan and Cao, Xiaochun and Tang, Jin},
doi = {10.1016/j.neucom.2017.11.068},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Adaptive fusion,Convolutional neural network,Correlation filter,Object tracking,Thermal information},
pages = {78--85},
publisher = {Elsevier B.V.},
title = {{Fusing two-stream convolutional neural networks for RGB-T object tracking}},
url = {https://doi.org/10.1016/j.neucom.2017.11.068},
volume = {281},
year = {2017}
}
@article{Ortega-Garcia2010,
abstract = {A new multimodal biometric database designed and acquired within the framework of the European BioSecure Network of Excellence is presented. It is comprised of more than 600 individuals acquired simultaneously in three scenarios: 1) over the Internet, 2) in an office environment with desktop PC, and 3) in indoor/outdoor environments with mobile portable hardware. The three scenarios include a common part of audio/video data. Also, signature and fingerprint data have been acquired both with desktop PC and mobile portable hardware. Additionally, hand and iris data were acquired in the second scenario using desktop PC. Acquisition has been conducted by 11 European institutions. Additional features of the BioSecure Multimodal Database (BMDB) are: two acquisition sessions, several sensors in certain modalities, balanced gender and age distributions, multimodal realistic scenarios with simple and quick tasks per modality, cross-European diversity, availability of demographic data, and compatibility with other multimodal databases. The novel acquisition conditions of the BMDB allow us to perform new challenging research and evaluation of either monomodal or multimodal biometric systems, as in the recent BioSecure Multimodal Evaluation campaign. A description of this campaign including baseline results of individual modalities from the new database is also given. The database is expected to be available for research purposes through the BioSecure Association during 2008.},
author = {Ortega-Garcia, Javier and Fierrez, Julian and Alonso-Fernandez, Fernando and Galbally, Javier and Freire, Manuel R. and Gonzalez-Rodriguez, Joaquin and Garcia-Mateo, Carmen and Alba-Castro, Jose Luis and Gonzalez-Agulla, Elisardo and Otero-Muras, Enrique and Garcia-Salicetti, Sonia and Allano, Lorene and Ly-Van, Bao and Dorizzi, Bernadette and Kittler, Josef and Bourlai, Thirimachos and Poh, Norman and Deravi, Farzin and Ng, Ming N.R. and Fairhurst, Michael and Hennebert, Jean and Humm, Andreas and Tistarelli, Massimo and Brodo, Linda and Richiardi, Jonas and Drygajlo, Andrezj and Ganster, Harald and Sukno, Federico M. and Pavani, Sri Kaushik and Frangi, Alejandro and Akarun, Lale and Savran, Arman},
doi = {10.1109/TPAMI.2009.76},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Benchmark,Biometrics,Database,Evaluation,Face,Fingerprint,Hand,Iris.,Multimodal,Performance,Signature,Speaker,Voice},
number = {6},
pages = {1097--1111},
pmid = {20431134},
title = {{The multiscenario multienvironment biosecure multimodal database (BMDB)}},
volume = {32},
year = {2010}
}
@article{DeMarsico2018,
abstract = {Mobile biometrics technologies are nowadays the new frontier for secure use of data and services, and are considered particularly important due to the massive use of handheld devices in the entire world. Among the biometric traits with potential to be used in mobile settings, the iris/ocular region is a natural candidate, even considering that further advances in the technology are required to meet the operational requirements of such ambitious environments. Aiming at promoting these advances, we organized the Mobile Iris Challenge Evaluation (MICHE)-I contest. This paper presents a comparison of the performance of the participant methods by various Figures of Merit (FoMs). A particular attention is devoted to the identification of the image covariates that are likely to cause a decrease in the performance levels of the compared algorithms. Among these factors, interoperability among different devices plays an important role. The methods (or parts of them) implemented by the analyzed approaches are classified into segmentation (S), which was the main target of MICHE-I, and recognition (R). The paper reports both the results observed for either S or R, and also for different recombinations (S+R) of such methods. Last but not least, we also present the results obtained by multi-classifier strategies.},
author = {{De Marsico}, Maria and Nappi, Michele and Narducci, Fabio and Proen{\c{c}}a, Hugo},
doi = {10.1016/j.patcog.2017.08.028},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Biometric algorithm fusion,Evaluation,Mobile Iris Recognition},
pages = {286--304},
publisher = {Elsevier Ltd},
title = {{Insights into the results of MICHE I - Mobile Iris CHallenge Evaluation}},
volume = {74},
year = {2018}
}
@article{Liu2013a,
author = {Liu, Jing and Sun, Zhenan and Tan, Tieniu},
doi = {10.1109/BTAS.2013.6712692},
isbn = {9781479905270},
journal = {IEEE 6th International Conference on Biometrics: Theory, Applications and Systems, BTAS 2013},
pages = {1--6},
title = {{Code-level information fusion of low-resolution iris image sequences for personal identification at a distance}},
year = {2013}
}
@article{Uka2017,
annote = {Keynotes:

CASIA (Most used database) and IIT Delhi Iris Database used.

Hough Transofrm algorithm used to detect boundaries},
author = {Uka, Arban and Ro{\c{c}}i, Albana and Ko{\c{c}}, Oktay},
isbn = {9781509038435},
journal = {IEEE EUROCON 2017 -17th International Conference on Smart Technologies},
keywords = {encoding,equal error rate,segmentation},
number = {July},
pages = {6--8},
title = {{Improved Segmentation Algorithm and Further Optimization for Iris Recognition}},
year = {2017}
}
@article{Biosec2007,
abstract = {The baseline corpus of a new multimodal database, acquired in the framework of the FP6 EU BioSec Integrated Project, is presented. The corpus consists of fingerprint images acquired with three different sensors, frontal face images from a webcam, iris images from an iris sensor, and voice utterances acquired both with a close-talk headset and a distant webcam microphone. The BioSec baseline corpus includes real multimodal data from 200 individuals in two acquisition sessions. In this contribution, the acquisition setup and protocol are outlined, and the contents of the corpus-including data and population statistics-are described. The database will be publicly available for research purposes by mid-2006. {\textcopyright} 2006 Pattern Recognition Society.},
author = {Fierrez, Julian and Ortega-Garcia, Javier and {Torre Toledano}, Doroteo and Gonzalez-Rodriguez, Joaquin},
doi = {10.1016/j.patcog.2006.10.014},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Authentication,Biometrics,Database,Face,Fingerprint,Iris,Multimodal,Performance,Verification,Voice},
number = {4},
pages = {1389--1392},
title = {{Biosec baseline corpus: A multimodal biometric database}},
volume = {40},
year = {2007}
}
@techreport{Zhao,
abstract = {Due to object detection's close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles which combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy and optimization function, etc. In this paper, we provide a review on deep learning based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely Convolutional Neural Network (CNN). Then we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network based learning systems.},
archivePrefix = {arXiv},
arxivId = {1807.05511v1},
author = {Zhao, Zhong-Qiu and Zheng, Peng and Xu, Shou-Tao and Wu, Xindong},
eprint = {1807.05511v1},
file = {:home/nicstar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - Unknown - Object Detection with Deep Learning A Review.pdf:pdf},
keywords = {Index Terms-deep learning,neural network,object detection},
title = {{Object Detection with Deep Learning: A Review}},
url = {https://arxiv.org/pdf/1807.05511.pdf},
year = {2018}
}
@article{Nam2014,
abstract = {We present a novel human-machine interface, called GOM-Face , and its application to humanoid robot control. The GOM-Face bases its interfacing on three electric potentials measured on the face: 1) glossokinetic potential (GKP), which involves the tongue movement; 2) electrooculogram (EOG), which involves the eye movement; 3) electromyogram, which involves the teeth clenching. Each potential has been individually used for assistive interfacing to provide persons with limb motor disabilities or even complete quadriplegia an alternative communication channel. However, to the best of our knowledge, GOM-Face is the first interface that exploits all these potentials together. We resolved the interference between GKP and EOG by extracting discriminative features from two covariance matrices: a tongue-movement-only data matrix and eye-movement-only data matrix. With the feature extraction method, GOM-Face can detect four kinds of horizontal tongue or eye movements with an accuracy of 86.7{\%} within 2.77 s. We demonstrated the applicability of the GOM-Face to humanoid robot control: users were able to communicate with the robot by selecting from a predefined menu using the eye and tongue movements.},
author = {Nam, Yunjun and Koo, Bonkon and Cichocki, Andrzej and Choi, Seungjin},
doi = {10.1109/TBME.2013.2280900},
issn = {00189294},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {Electromyogram (EMG),electrooculogram (EOG),glossokinetic potentials (GKP),human-machine interface,multimodal interface},
number = {2},
pages = {453--462},
pmid = {24021635},
title = {{GOM-face: GKP, EOG, and EMG-based multimodal interface with application to humanoid robot control}},
volume = {61},
year = {2014}
}
@article{Sequeira2014,
abstract = {Biometrics represents a return to a natural way of identification: testing someone by what (s)he is, instead of relying on something (s)he owns or knows seems likely to be the way forward. Biometric systems that include multiple sources of information are known as multimodal. Such systems are generally regarded as an alternative to fight a variety of problems all unimodal systems stumble upon. One of the main challenges found in the development of biometric recognition systems is the shortage of publicly available databases acquired under real unconstrained working conditions. Motivated by such need the MobBIO database was created using an Asus EeePad Transformer tablet, with mobile biometric systems in mind. The proposed database is composed by three modalities: iris, face and voice.},
author = {Sequeira, Ana F. and Monteiro, Joao C. and Rebelo, Ana and Oliveira, Helder P.},
isbn = {9789897580093},
journal = {9th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
number = {c},
pages = {1--14},
title = {{MobBIO: A Multimodal Database Captured with a Portable Handheld Device}},
year = {2014}
}
@article{Wang2009a,
abstract = {Feature-level fusion remains a challenging problem for multimodal biometrics. However, existing fusion schemes such as sum rule and weighted sum rule are inefficient in complicated condition. In this paper, we propose an efficient feature-level fusion algorithm for iris and face in parallel. The algorithm first normalizes the original features of iris and face using z-score model, and then take complex FDA as the classifier of Unitary space. The proposed algorithm is tested using CASIA iris database and two face databases (ORL database and Yale database.). Experimental results show the effectiveness of the proposed algorithm.},
author = {Wang, Zhifang and Han, Qi and Niu, Xiamu and Busch, Christoph},
doi = {10.1007/978-3-642-01513-7_38},
isbn = {3642015123},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Biometrics,CFDA,Feature-level,Parallel fusion,Unitary space},
number = {PART 3},
pages = {356--364},
title = {{Feature-level fusion of Iris and face for personal identification}},
volume = {5553 LNCS},
year = {2009}
}
@article{Johnson2010a,
abstract = {Identification of individuals using biometric information has found great success in many security and law enforcement applications. Up until the present time, most research in the field has been focused on ideal conditions and most available databases are constructed in these ideal conditions. There has been a growing interest in the perfection of these technologies at a distance and in less than ideal conditions, i.e. low lighting, out-of-focus blur, off angles, etc. This paper presents a dataset consisting of face and iris videos obtained at distances of 5 to 25 feet and in conditions of varying quality. The purpose of this database is to set a standard for quality measurement in face and iris data and to provide a means for analyzing biométrie systems in less than ideal conditions. The structure of the dataset as well as a quantified metric for quality measurement based on a 25 subject subset of the dataset is presented.},
author = {Johnson, P. A. and Lopez-Meyer, P. and Sazonova, N. and Hua, F. and Schuckers, S.},
doi = {10.1109/BTAS.2010.5634513},
isbn = {9781424475803},
journal = {IEEE 4th International Conference on Biometrics: Theory, Applications and Systems, BTAS 2010},
title = {{Quality in face and Iris research ensemble (Q-FIRE)}},
year = {2010}
}
@article{sun2015,
abstract = {The state-of-the-art of face recognition has been significantly advanced by the emergence of deep learning. Very deep neural networks recently achieved great success on general object recognition because of their superb learning capacity. This motivates us to investigate their effectiveness on face recognition. This paper proposes two very deep neural network architectures, referred to as DeepID3, for face recognition. These two architectures are rebuilt from stacked convolution and inception layers proposed in VGG net and GoogLeNet to make them suitable to face recognition. Joint face identification-verification supervisory signals are added to both intermediate and final feature extraction layers during training. An ensemble of the proposed two architectures achieves 99.53{\%} LFW face verification accuracy and 96.0{\%} LFW rank-1 face identification accuracy, respectively. A further discussion of LFW face verification result is given in the end.},
archivePrefix = {arXiv},
arxivId = {1502.00873},
author = {Sun, Yi and Liang, Ding and Wang, Xiaogang and Tang, Xiaoou},
eprint = {1502.00873},
title = {{DeepID3: Face Recognition with Very Deep Neural Networks}},
url = {https://arxiv.org/pdf/1502.00873.pdf http://arxiv.org/abs/1502.00873},
year = {2015}
}
@article{Al-Waisy2017a,
abstract = {Multimodal biometrie systems seek to alleviate some of the limitations of unimodal biometrie systems by combining multiple pieces of evidence of the same person in the deeision-making process. In this paper, a novel multimodal biometric identification system is proposed based on fusing the results obtained from both the face and the left and right irises using deep learning approaches. Firstly, the facial features are extracted using a Deep Belief Network (DBN) architecture consisting of 3-layers. The first two RBMs are used as features detectors, while the last one is used as a discriminative model associated with softmax units for the multi-class classification purpose. Secondly, an efficient deep learning system is employed for iris recognition, whose architecture is based on a combination of Convolutional Neural Network (CNN) and Softmax classifier to extract discriminative features from an iris image. Extensive experiments on large-scale challenging databases, including FERET, CASIA V1.0 and MMU1, and SDUMLA-HMT have demonstrated the superiority of the proposed approaches by achieving new state-of-the-art Rank-1 identification rates on all the employed databases.},
author = {Al-Waisy, A S and Qahwaji, R and Ipson, S and Al-Fahdawi, S},
doi = {10.1109/EST.2017.8090417},
isbn = {9781538640173},
journal = {2017 Seventh International Conference on Emerging Security Technologies (EST)},
keywords = {Databases;Face;Feature extraction;Iris recognition},
pages = {163--168},
title = {{A multimodal biometrie system for personal identification based on deep learning approaches}},
year = {2017}
}
@article{Hu2015,
abstract = {Deep learning, in particular Convolutional Neural Network (CNN), has achieved promising results in face recognition recently. However, it remains an open question: why CNNs work well and how to design a 'good' architecture. The existing works tend to focus on reporting CNN architectures that work well for face recognition rather than investigate the reason. In this work, we conduct an extensive evaluation of CNN-based face recognition systems (CNN-FRS) on a common ground to make our work easily reproducible. Specifically, we use public database LFW (Labeled Faces in the Wild) to train CNNs, unlike most existing CNNs trained on private databases. We propose three CNN architectures which are the first reported architectures trained using LFW data. This paper quantitatively compares the architectures of CNNs and evaluate the effect of different implementation choices. We identify several useful properties of CNN-FRS. For instance, the dimensionality of the learned features can be significantly reduced without adverse effect on face recognition accuracy. In addition, traditional metric learning method exploiting CNN-learned features is evaluated. Experiments show two crucial factors to good CNN-FRS performance are the fusion of multiple CNNs and metric learning. To make our work reproducible, source code and models will be made publicly available.},
archivePrefix = {arXiv},
arxivId = {1504.02351},
author = {Hu, Guosheng and Yang, Yongxin and Yi, Dong and Kittler, Josef and Christmas, William and Li, Stan Z and Hospedales, Timothy},
doi = {10.1109/ICCVW.2015.58},
eprint = {1504.02351},
isbn = {9780769557205},
issn = {15505499},
journal = {2015 IEEE International Conference on Computer Vision Workshop (ICCVW)},
month = {dec},
pages = {384--392},
publisher = {IEEE},
title = {{When Face Recognition Meets with Deep Learning: an Evaluation of Convolutional Neural Networks for Face Recognition}},
url = {http://ieeexplore.ieee.org/document/7406407/ http://arxiv.org/abs/1504.02351},
year = {2015}
}
@article{Mellakh2009a,
author = {Mellakh, A and Chaari, A and Guerfi, S and Dhose, J and Colineau, J and Lelandais, S and Petrovska-Delacr{\`{e}}taz, D and Dorizzi, B},
doi = {10.1007/978-3-642-04697-1_3},
isbn = {03029743; 3642046967 (ISBN); 9783642046964 (ISBN)},
issn = {03029743},
journal = {11th International Conference on Advanced Concepts for Intelligent Vision Systems, ACIVS 2009},
keywords = {2D face recognition,Appearance based,Appearance-based algorithms,Computer vision,Database,Database systems,Discriminant analysis,Evaluation campaign,Experimental protocols,Face images,Face recognition,Linear discriminant analysis,Multi-modal,Multimodal database,Principal component analysis,Training sets},
pages = {24--32},
title = {{2D face recognition in the IV2 evaluation campaign}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-70549098348{\&}partnerID=40{\&}md5=0541be27bb29a037c965f0d644b15856},
volume = {5807 LNCS},
year = {2009}
}
@article{Bazrafkan2017,
abstract = {With the increasing imaging and processing capabilities of today's mobile devices, user authentication using iris biometrics has become feasible. However, as the acquisition conditions become more unconstrained and as image quality is typically lower than dedicated iris acquisition systems, the accurate segmentation of iris regions is crucial for these devices. In this work, an end to end Fully Convolutional Deep Neural Network (FCDNN) design is proposed to perform the iris segmentation task for lower-quality iris images. The network design process is explained in detail, and the resulting network is trained and tuned using several large public iris datasets. A set of methods to generate and augment suitable lower quality iris images from the high-quality public databases are provided. The network is trained on Near InfraRed (NIR) images initially and later tuned on additional datasets derived from visible images. Comprehensive inter-database comparisons are provided together with results from a selection of experiments detailing the effects of different tunings of the network. Finally, the proposed model is compared with SegNet-basic, and a near-optimal tuning of the network is compared to a selection of other state-of-art iris segmentation algorithms. The results show very promising performance from the optimized Deep Neural Networks design when compared with state-of-art techniques applied to the same lower quality datasets.},
archivePrefix = {arXiv},
arxivId = {1712.02877},
author = {Bazrafkan, Shabab and Thavalengal, Shejin and Corcoran, Peter},
eprint = {1712.02877},
month = {dec},
title = {{An End to End Deep Neural Network for Iris Segmentation in Unconstraint Scenarios}},
url = {http://arxiv.org/abs/1712.02877},
year = {2017}
}
@article{Redmon2018,
abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
archivePrefix = {arXiv},
arxivId = {1804.02767},
author = {Redmon, Joseph and Farhadi, Ali},
eprint = {1804.02767},
file = {:home/nicstar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon, Farhadi - 2018 - YOLOv3 An Incremental Improvement.pdf:pdf},
month = {apr},
title = {{YOLOv3: An Incremental Improvement}},
url = {http://arxiv.org/abs/1804.02767},
year = {2018}
}
@article{Vivek2012a,
author = {Vivek, S. Arun and Aravinth, J. and Valarmathy, S.},
doi = {10.1109/ICPRIME.2012.6208377},
isbn = {978-1-4673-1039-0},
journal = {International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME-2012)},
keywords = {density based score level,error,feature extraction,fusion,gmm,likelihood ratio test,multimodal biometrics,rates,template,unimodal biometrics},
pages = {387--392},
title = {{Feature extraction for multimodal biometric and study of fusion using Gaussian mixture model}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6208377},
year = {2012}
}
@article{Ma2015,
abstract = {This study presents a novel human-machine interface (HMI) based on both electrooculography (EOG) and electroencephalography (EEG). This hybrid interface works in two modes: an EOG mode recognizes eye movements such as blinks, and an EEG mode detects event related potentials (ERPs) like P300. While both eye movements and ERPs have been separately used for implementing assistive interfaces, which help patients with motor disabilities in performing daily tasks, the proposed hybrid interface integrates them together. In this way, both the eye movements and ERPs complement each other. Therefore, it can provide a better efficiency and a wider scope of application. In this study, we design a threshold algorithm that can recognize four kinds of eye movements including blink, wink, gaze, and frown. In addition, an oddball paradigm with stimuli of inverted faces is used to evoke multiple ERP components including P300, N170, and VPP. To verify the effectiveness of the proposed system, two different online experiments are carried out. One is to control a multifunctional humanoid robot, and the other is to control four mobile robots. In both experiments, the subjects can complete tasks effectively by using the proposed interface, whereas the best completion time is relatively short and very close to the one operated by hand.},
author = {Ma, Jiaxin and Zhang, Yu and Cichocki, Andrzej and Matsuno, Fumitoshi},
doi = {10.1109/TBME.2014.2369483},
isbn = {0018-9294},
issn = {15582531},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {Electroencephalogram (EEG),Electrooculogram (EOG),event-related potential (ERP),human-machine interface (HMI),robot control},
number = {3},
pages = {876--889},
pmid = {25398172},
title = {{A novel EOG/EEG hybrid human-machine interface adopting eye movements and ERPs: Application to robot control}},
volume = {62},
year = {2015}
}
@article{Al-Waisy2017,
author = {Al-Waisy, Alaa S and Qahwaji, Rami and Ipson, Stanley and Al-Fahdawi, Shumoos and Nagem, Tarek A M},
doi = {10.1007/s10044-017-0656-1},
isbn = {0123456789},
issn = {14337541},
journal = {Pattern Analysis and Applications},
keywords = {AdaGrad method,Convolutional Neural Network,Deep learning,Iris recognition,Multimodal biometric systems,Softmax classifier},
number = {0123456789},
pages = {1--20},
publisher = {Springer London},
title = {{A multi-biometric iris recognition system based on a deep learning approach}},
url = {https://doi.org/10.1007/s10044-017-0656-1},
year = {2017}
}
@misc{LiborMasek2003,
annote = {Open Source Matlab Iris Recognition system. Based on Daugmans approach.},
author = {{Libor Masek}, Peter Kovesi},
publisher = {The School of Computer Science and Software Engineering, The University of Western Australia},
title = {{MATLAB Source Code for a Biometric Identification System Based on Iris Patterns}},
url = {http://www.peterkovesi.com/studentprojects/libor/sourcecode.html},
year = {2003}
}
@article{Redmon2016,
abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
archivePrefix = {arXiv},
arxivId = {1612.08242},
author = {Redmon, Joseph and Farhadi, Ali},
eprint = {1612.08242},
file = {:home/nicstar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon, Farhadi - 2016 - YOLO9000 Better, Faster, Stronger.pdf:pdf},
month = {dec},
title = {{YOLO9000: Better, Faster, Stronger}},
url = {http://arxiv.org/abs/1612.08242},
year = {2016}
}
@article{Neves2017,
abstract = {{\textcopyright}2017 IEEE. An error-correcting code (ECC) is a process of adding redundant data to a message, such that it can be recovered by a receiver even if a number of errors are introduced in transmission. Inspired by the principles of ECC, we introduce a method capable of detecting degraded features in biometric signatures by exploiting feature correlation. The main novelty is that, unlike existing biometric cryptosystems, the proposed method works directly on the biometric signature. Our approach performs a redundancy analysis of non-degraded data to build an undirected graphical model (Markov Random Field), whose energy minimization determines the sequence of degraded components of the biometric sample. Experiments carried out in different biometric traits ascertain the improvements attained when disregarding degraded features during the matching phase. Also, we stress that the proposed method is general enough to work in different classification methods, such as CNNs.},
author = {Neves, Joao and Proenca, Hugo},
doi = {10.1109/FG.2017.122},
isbn = {9781509040230},
journal = {Proceedings - 12th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2017 - 1st International Workshop on Adaptive Shot Learning for Gesture Understanding and Production, ASL4GUP 2017, Biometrics in the Wild, Bwild 2017, Heteroge},
pages = {981--986},
title = {{Exploiting Data Redundancy for Error Detection in Degraded Biometric Signatures Resulting from in the Wild Environments}},
year = {2017}
}
@article{Luhadiya2017,
annote = {Really good article with a summery of Iris recognition history and approaches in the introduction part.


Keynotes:

Iris database called CASIA with 756 images of 108 people.

SVM used to classify irises.

Elman Recurrent Neural Netowrk used.},
author = {Luhadiya, Ruchi and Khedkar, Anagha},
doi = {10.1109/ICAECCT.2016.7942619},
isbn = {9781509036622},
journal = {2016 IEEE International Conference on Advances in Electronics, Communication and Computer Technology, ICAECCT 2016},
keywords = {GLCM,Hough circular transform,Iris,Machine learning,Person identification,SVM},
pages = {387--392},
title = {{Iris detection for person identification using multiclass SVM}},
year = {2017}
}
@article{Daugman1993,
author = {Daugman, J.G.},
doi = {10.1109/34.244676},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {11},
pages = {1148--1161},
title = {{High confidence visual recognition of persons by a test of statistical independence}},
url = {http://ieeexplore.ieee.org/document/244676/},
volume = {15},
year = {1993}
}
@article{Hakkinen1993,
abstract = {The primary aim of the study was to determine the best electrode positions for EOG signals in vigilance studies. Two-channel recordings were conducted in analogy to the Rechtschaffen and Kales (1968) system. Twenty electrodes (10 electrode pairs) were compared. Both EOG amplitudes and amplitude asymmetries within an electrode pair were studied. The amplitude of the EOG signal is sensitive to relatively small differences in electrode position. This concerns especially distance from the eye, the direction of eye movement and the effect of the upper eye lid movement. Larger and more symmetrical EOG amplitudes were obtained for different eye movements by placing the electrodes more medially than in the conventionally used system. EOG asymmetry in different electrode positions was dependent on the eye movement direction and even on the starting and end points of a movement with equal angular degrees. Most of the data could be explained by a simple monopolar model when combined with the effects of the upper eye lid movements. The most unexpected finding was that the EOG amplitudes of the horizontal and oblique eye movements were significantly larger when the eye were moving towards an electrode than when they were moving to the opposite direction. {\textcopyright}1993.},
author = {H{\"{a}}kkinen, V and Hirvonen, K and Hasan, J and Kataja, M and V{\"{a}}rri, A and Loula, P and Eskola, H},
doi = {10.1016/0013-4694(93)90111-8},
issn = {00134694},
journal = {Electroencephalography and Clinical Neurophysiology},
keywords = {EOG,Electrode positions,Vigilance},
number = {4},
pages = {294--300},
pmid = {7682933},
title = {{The effect of small differences in electrode position on EOG signals: application to vigilance studies}},
volume = {86},
year = {1993}
}
@article{Wan2017a,
abstract = {State-of-the-art methods for 3D hand pose estimation from depth images require large amounts of annotated training data. We propose to model the statistical relationships of 3D hand poses and corresponding depth images using two deep generative models with a shared latent space. By design, our architecture allows for learning from unlabeled image data in a semi-supervised manner. Assuming a one-to-one mapping between a pose and a depth map, any given point in the shared latent space can be projected into both a hand pose and a corresponding depth map. Regressing the hand pose can then be done by learning a discriminator to estimate the posterior of the latent pose given some depth map. To improve generalization and to better exploit unlabeled depth maps, we jointly train a generator and a discriminator. At each iteration, the generator is updated with the back-propagated gradient from the discriminator to synthesize realistic depth maps of the articulated hand, while the discriminator benefits from an augmented training set of synthesized and unlabeled samples. The proposed discriminator network architecture is highly efficient and runs at 90 FPS on the CPU with accuracies comparable or better than state-of-art on 3 publicly available benchmarks.},
archivePrefix = {arXiv},
arxivId = {1702.03431},
author = {Wan, Chengde and Probst, Thomas and {Van Gool}, Luc and Yao, Angela},
doi = {10.1109/CVPR.2017.132},
eprint = {1702.03431},
journal = {Cvpr2017},
pages = {10},
title = {{Crossing Nets: Dual Generative Models with a Shared Latent Space for Hand Pose Estimation}},
url = {http://arxiv.org/abs/1702.03431},
year = {2017}
}
@article{Kauba2016,
abstract = {—Authentication based on vein patterns is a very promising biometric technique. The most important step is the accurate extraction of the vein pattern from sometimes low quality input images. A single feature extraction technique may fail to correctly extract the vein pattern, entailing bad recognition performance. One of the solutions that can be used to improve recognition results is biometric fusion. A possible fusion strategy is feature level fusion, that is the fusion of several feature extractors' outputs. In our work, we exploited the feature level fusion to improve the quality of the extracted vein patterns and thus the feature extraction accuracy. An experimental study involving different feature extraction techniques (maximum curvature, repeated line tracking, wide line detector, ...) and different fusion techniques (majority voting, weighted average, STAPLE, ...) is conducted on the UTFVP finger-vein data set. The results show that feature level fusion is able to improve the recognition accuracy in terms of the EER over the single feature extraction techniques.},
author = {Kauba, Christof and Uhl, Andreas and Piciucco, Emanuela and Maiorana, Emanuele and Campisi, Patrizio},
doi = {10.1109/BIOSIG.2016.7736908},
isbn = {9783885796541},
issn = {16175468},
journal = {Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)},
title = {{Advanced variants of feature level fusion for finger vein recognition}},
volume = {P-260},
year = {2016}
}
@article{Kumar2016a,
abstract = {We studied the fusion of three biometric authentication modalities, namely, swiping gestures, typing patterns and the phone movement patterns observed during typing or swiping. A web browser was customized to collect the data generated from the aforementioned modalities over four to seven days in an unconstrained environment. Several features were extracted by using sliding window mechanism for each modality and analyzed by using information gain, correlation, and symmetric uncertainty. Finally, five features from windows of continuous swipes, thirty features from windows of continuously typed letters, and nine features from corresponding phone movement patterns while swiping/typing were used to build the authentication system. We evaluated the performance of each modality and their fusion over a dataset of 28 users. The feature-level fusion of swiping and the corresponding phone movement patterns achieved an authentication accuracy of 93.33{\%}, whereas, the score-level fusion of typing behaviors and the corresponding phone movement patterns achieved an authentication accuracy of 89.31{\%}. 1.},
author = {Kumar, Rajesh and Phoha, Vir V. and Serwadda, Abdul},
doi = {10.1109/BTAS.2016.7791164},
isbn = {9781467397339},
journal = {2016 IEEE 8th International Conference on Biometrics Theory, Applications and Systems, BTAS 2016},
title = {{Continuous authentication of smartphone users by fusing typing, swiping, and phone movement patterns}},
year = {2016}
}
@inproceedings{Zhao2015a,
abstract = {This paper proposes a novel and more accurate iris segmentation framework to automatically segment iris region from the face images acquired with relaxed imaging under visible or near-infrared illumination, which provides strong feasibility for applications in surveillance, forensics and the search for missing children, etc. The proposed framework is built on a novel total-variation based formulation which uses l1 norm regularization to robustly suppress noisy texture pixels for the accurate iris localization. A series of novel and robust post processing operations are introduced to more accurately localize the limbic boundaries. Our experimental results on three publicly available databases, i.e., FRGC, UBIRIS.v2 and CASIA.v4-distance, achieve significant performance improvement in terms of iris segmentation accuracy over the state-of-the-art approaches in the literature. Besides, we have shown that using iris masks generated from the proposed approach helps to improve iris recognition performance as well. Unlike prior work, all the implementations in this paper are made publicly available to further advance research and applications in biometrics at-d-distance.},
author = {Zhao, Zijing and Kumar, Ajay},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.436},
isbn = {9781467383912},
issn = {15505499},
month = {dec},
pages = {3828--3836},
publisher = {IEEE},
title = {{An accurate iris segmentation framework under relaxed imaging constraints using total variation model}},
url = {http://ieeexplore.ieee.org/document/7410793/},
volume = {2015 Inter},
year = {2015}
}
@article{Hqwhu,
author = {Hqwhu, E and Vndudkdq, Qvndudnrf and Jwx, Dnjxo and Wu, H G X},
keywords = {convolutional neural,deep learning,network,pupil center estimation},
title = {{Deep learning based estimation of the eye pupil center by using image patch classification}}
}
@article{Aakerberg2017,
abstract = {Object recognition is one of the important tasks in computer vision which has found enormous applications. Depth modality is proven to provide supplementary information to the common RGB modality for object recognition. In this paper, we propose methods to improve the recognition performance of an existing deep learning based RGB-D object recognition model, namely the FusionNet proposed by Eitel et al. First, we show that encoding the depth values as colorized surface normals is beneficial, when the model is initialized with weights learned from training on ImageNet data. Additionally, we show that the RGB stream of the FusionNet model can benefit from using deeper network architectures, namely the 16-layered VGGNet, in exchange for the 8-layered CaffeNet. In combination, these changes improves the recognition performance with 2.2{\%} in comparison to the original FusionNet, when evaluating on the Washington RGB-D Object Dataset.},
author = {Aakerberg, Andreas and Nasrollahi, Kamal and Rasmussen, Christoffer B. and Moeslund, Thomas B.},
doi = {10.5220/0006511501210128},
isbn = {978-989-758-274-5},
journal = {Proceedings of the 9th International Joint Conference on Computational Intelligence},
keywords = {artificial vision,computer vision,convolutional neural networks,deep learning,has found enormous applications,in computer vision which,learning,object recognition is one,of the important tasks,rgb-d,surface normals,transfer},
pages = {121--128},
title = {{Depth Value Pre-Processing for Accurate Transfer Learning based RGB-D Object Recognition}},
url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0006511501210128},
year = {2017}
}
@article{Schiel2002,
abstract = {In this contribution we announce and describe in detail the new multimodal corpus evolving from the publicly funded German SmartKom project. The first release of the corpus (BAS SK-P 1.0) has been finished end of 2001 and will be ready for distribution to the scientific community in July 2002. The SmartKom corpus will be the first of a new generation of Language Resources (LR) designed for a more or less complete data gathering of human-machine communication combining acoustic, visual and tactile input and output modalities. Since the funding of about EU 2 Mio for this LR is 100{\%} public, the corpus will be available without royalties via the Bavarian Archive for Speech Signals (BAS) at the University of Munich.},
author = {Schiel, Florian and Steininger, Silke and T{\"{u}}rk, Ulrich},
journal = {Proceedings of the 3rd Language Resources and Evaluation},
number = {34},
pages = {200--2006},
title = {{The SmartKom Multimodal Corpus at BAS}},
year = {2002}
}
@article{Lv2015,
abstract = {There is an increasing interest in creating pervasive games based on emerging interaction technologies. In order to develop touch-less, interactive and augmented reality games on vision-based wearable device, a touch-less motion interaction technology is designed and evaluated in this work. Users interact with the augmented reality games with dynamic hands/feet gestures in front of the camera, which triggers the interaction event to interact with the virtual object in the scene. Three primitive augmented reality games with eleven dynamic gestures are developed based on the proposed touch-less interaction technology as proof. At last, a comparing evaluation is proposed to demonstrate the social acceptability and usability of the touch-less approach, running on a hybrid wearable framework or with Google Glass, as well as workload assessment, user's emotions and satisfaction.},
author = {Lv, Zhihan and Halawani, Alaa and Feng, Shengzhong and {Ur R{\'{e}}hman}, Shafiq and Li, Haibo},
doi = {10.1007/s00779-015-0844-1},
isbn = {16174909 (ISSN)},
issn = {16174909},
journal = {Personal and Ubiquitous Computing},
keywords = {Augmented reality game,Hand free,Pervasive game,Smartphone game,Touch-less,Wearable device},
number = {3},
pages = {551--567},
title = {{Touch-less interactive augmented reality game on vision-based wearable device}},
volume = {19},
year = {2015}
}
@article{Chowdhury2016a,
author = {Chowdhury, Anurag and Ghosh, Soumyadeep and Singh, Richa and Vatsa, Mayank},
doi = {10.1109/BTAS.2016.7791199},
isbn = {9781467397339},
journal = {2016 IEEE 8th International Conference on Biometrics Theory, Applications and Systems, BTAS 2016},
title = {{RGB-D face recognition via learning-based reconstruction}},
year = {2016}
}
@article{Gopal2018a,
author = {Gopal and Srivastava, Smriti},
doi = {10.1007/s13369-017-2644-6},
issn = {21914281},
journal = {Arabian Journal for Science and Engineering},
keywords = {Feature-level fusion,Multimodal system,Palm–phalanges,Score-level fusion,Unimodal system},
number = {2},
pages = {543--554},
publisher = {Springer Berlin Heidelberg},
title = {{Accurate Human Recognition by Score-Level and Feature-Level Fusion Using Palm–Phalanges Print}},
volume = {43},
year = {2018}
}
@inproceedings{Trokielewicz2016,
author = {Trokielewicz, Mateusz},
booktitle = {2016 IEEE International Conference on Identity, Security and Behavior Analysis (ISBA)},
doi = {10.1109/ISBA.2016.7477233},
isbn = {978-1-4673-9727-8},
month = {feb},
pages = {1--6},
publisher = {IEEE},
title = {{Iris recognition with a database of iris images obtained in visible light using smartphone camera}},
url = {http://ieeexplore.ieee.org/document/7477233/},
year = {2016}
}
@article{Deng2010,
abstract = {Several Human-Machine/Computer Interfaces (HMI/HCI) had been developed in recent years. Some designs were specifically made for people with disabilities such as injured-vertebra, apoplexy or poliomyelitis, Amyotrophic Lateral Sclerosis (ALS), and Motor Neuron Disease, (MND). In this paper, we proposed an eye-movement tracking system. Based on Electro-Oculography (EOG) technology we detected the signal with different directions in eye-movements and then analyzed to understand what they represented about (e.g. horizontal direction or vertical direction). We converted the analog signal to digital signal and then used as the control signals for Human-Computer Interface (HCI). In order to make the system "robust", several applications with EOG-based HCI had been designed. Our preliminary results revealed more than 90{\%} accuracy rate for examining the eye-movement that may become a new useful human-machine user interface in the near future. {\textcopyright}2009 Elsevier Ltd. All rights reserved.},
author = {Deng, Lawrence Y and Hsu, Chun Liang and Lin, Tzu Ching and Tuan, Jui Sen and Chang, Shih Ming},
doi = {10.1016/j.eswa.2009.10.017},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Amyotrophic Lateral Sclerosis,Electro-Oculography (EOG),Eye-movement,Human-Machine/Computer Interface (HMI/HCI),Motor Neuron Disease},
number = {4},
pages = {3337--3343},
publisher = {Elsevier Ltd},
title = {{EOG-based Human-Computer Interface system development}},
url = {http://dx.doi.org/10.1016/j.eswa.2009.10.017},
volume = {37},
year = {2010}
}
@article{LeCun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
isbn = {9780521835688},
issn = {14764687},
journal = {Nature},
keywords = {Computer science,Mathematics and computing},
month = {may},
number = {7553},
pages = {436--444},
pmid = {10463930},
publisher = {Nature Publishing Group},
title = {{Deep learning}},
url = {http://www.nature.com/articles/nature14539},
volume = {521},
year = {2015}
}
@article{Soviany2016,
abstract = {– This paper presents a design approach of a reliable authentication system for mobile applications (such as those within m-Health or m-Banking areas). This means that the biometric data processing should optimize the security performance vs. the computational complexity. The security is given by the combination of fingerprint, iris and voice features that define the multimodal pattern of an individual. The complexity reduction is supported by a reduced feature space, especially for the fingerprint and iris recognition components of the overall system.},
author = {Soviany, Sorin and Săndulescu, Virginia and Puşcoci, Sorin},
isbn = {9781509020478},
journal = {Computers and Artificial Intelligence},
keywords = {-multimodal,biometrics,data fusion,identification},
title = {{A Multimodal Biometric Identification Method for Mobile Applications Security}},
volume = {30},
year = {2016}
}
@article{Kupfer1984,
author = {Kupfer, D J and Ulrich, R F and Coble, P A and Jarnatt, D B and Grochocinski, V and Doman, J and Matthews, G and Borbely, A A},
journal = {Psychiatry Res},
keywords = {ANALYSIS,Depression,Human,REM,Sleep},
pages = {335--343},
title = {{Application of automated REM and slow wave analysis: I normal and depressive subjects}},
volume = {13},
year = {1984}
}
@article{Chen2005a,
abstract = {The recognition accuracy of a single biometric authentication system is often much reduced due to the environment, user mode and physiological defects. In this paper, we combine face and iris features for developing a multimode biometric approach, which is able to diminish the drawback of single biometric approach as well as to improve the performance of authentication system. We combine a face database ORL and iris database CASIA to construct a multimodal biometric experimental database with which we validate the proposed approach and evaluate the multimodal biometrics performance. The experimental results reveal the multimodal biometrics verification is much more reliable and precise than single biometric approach.},
annote = {Face and iris for authentication
uses synthetic multimodal biometric dataset

Shows how to evaluate system.

proves multimodal performs better.},
author = {Chen, Ching-Han and {Te Chu}, Chia},
doi = {10.1007/11608288_76},
isbn = {978-3-540-31621-3},
issn = {03029743},
keywords = {face,iris,multimodal biometrics,wavelet probabilistic neural},
pages = {571--580},
title = {{Fusion of Face and Iris Features for Multimodal Biometrics}},
url = {http://link.springer.com/10.1007/11608288{\_}76},
year = {2005}
}
@inproceedings{Gall2012,
abstract = {Object detection multi class detection Random forests object detection ...},
author = {Gall, Juergen and Razavi, Nima and {Van Gool}, Luc},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-34091-8_11},
isbn = {9783642340901},
issn = {03029743},
keywords = {Hough forest,multi-class object detection,random forest,regression forest},
pages = {243--263},
title = {{An introduction to random forests for multi-class object detection}},
url = {http://link.springer.com/10.1007/978-3-642-34091-8{\_}11},
volume = {7474 LNCS},
year = {2012}
}
@article{Guo2017,
abstract = {In this paper we study face recognition using convolutional neural network. First, we introduced the basic CNN neural network architecture. Second, we modify the traditional neural network and adapt it to another database by fine tuning its parameters. Third, the network architecture is extended to the cross database problem. The CNN is first trained on a large dataset and then tested on another. Experimental results show that the proposed algorithm is suitable for building various real world applications.},
author = {Guo, Mei and Xiao, Min and Gong, Deliang},
doi = {10.1007/978-3-319-69096-4_54},
file = {:home/nicstar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo, Xiao, Gong - 2017 - Face Recognition Using Deep Convolutional Neural Network in Cross-Database Study.pdf:pdf},
journal = {Part of the Advances in Intelligent Systems and Computing book series (AISC, volume 686)},
keywords = {Deep neural network {\'{A}},Face recognition {\'{A}},Image processing},
title = {{Face Recognition Using Deep Convolutional Neural Network in Cross-Database Study}},
url = {https://link-springer-com.zorac.aub.aau.dk/content/pdf/10.1007{\%}2F978-3-319-69096-4{\_}54.pdf},
year = {2017}
}
@article{Galdi2017,
abstract = {FIRE is a Fast Iris REcognition algorithm especially designed for iris recognition on mobile phones under visible-light. It is based on the combination of three classifiers exploiting the iris colour and texture information. Its limited computational time makes FIRE particularly suitable for fast user verification on mobile devices. The high parallelism of the code allows its use also on large databases. FIRE, in its first version, was submitted to the Mobile Iris CHallenge Evaluation part II held in 2016. In this paper, FIRE is further improved: a number of different techniques has been analyzed and the best performing ones have been selected for fusion at score level. Performance are assessed in terms of Recognition Rate (RR), Area Under Receiver Operating Characteristic Curve (AUC), and Equal Error Rate (EER).},
author = {Galdi, Chiara and Dugelay, Jean Luc},
doi = {10.1016/j.patrec.2017.01.023},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Fast iris recognition,MICHE DB,MICHE II,Multi-classifier,Noisy iris recognition,Visible light},
pages = {44--51},
publisher = {Elsevier B.V.},
title = {{FIRE: Fast Iris REcognition on mobile phones by combining colour and texture features}},
url = {http://dx.doi.org/10.1016/j.patrec.2017.01.023},
volume = {91},
year = {2017}
}
@book{Bowyer2016b,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-{\$}\alpha{\$}-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA}for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
annote = {Read only chapter 17 about iris and face fusion. The article gives a nice and clean overview of different multi-biometric systems as well as levels of data abstraction the data fusion can be applied on.

The work presented performs iris and face fusion using multi-sample, multi instance, and multimodal data. it fuses the multi sample, and multi instance together, by simply finding the best match in the variations. this is done as a score level fusion. and it fuses the two modalities by rank-level fusion using Broda count.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Connaughton, Ryan and Bowyer, Kevin W and Flynn, Patrick J},
booktitle = {Handbook of Iris Recognition},
doi = {10.1007/978-1-4471-6784-6},
eprint = {arXiv:1011.1669v3},
isbn = {978-1-4471-6782-2},
issn = {16130073},
pages = {397--415},
pmid = {25246403},
title = {{Chapter 17 Fusion of Face and Iris Biometrics}},
url = {http://link.springer.com/10.1007/978-1-4471-6784-6},
year = {2016}
}
@article{Wijesoma2005,
abstract = {Assistive robots are increasingly being used to improve the quality of the life of disabled or handicapped people. In this paper a complete system is presented that can be used by people with extremely limited peripheral mobility but having the ability for eye motor coordination. The electrooculogram signals (EOG) that results from the eye displacement in the orbit of the subject are processed in real time to interpret intent and hence generate appropriate control signals to the assistive device. The effectiveness of the proposed methodology and the algorithms are demonstrated using a mobile robot for a limited vocabulary},
author = {Wijesoma, W S and Wee, Kang Say Wee Kang Say and Wee, Ong Choon Wee Ong Choon and a.P. Balasuriya and San, Koh Tong San Koh Tong and Soon, Low Kay Soon Low Kay},
doi = {10.1109/ROBIO.2005.246316},
isbn = {0-7803-9315-5},
journal = {2005 IEEE International Conference on Robotics and Biomimetics - ROBIO},
keywords = {- electrooculography,disabled people,eoc,eye movements,severely,wheelchair},
pages = {490--494},
title = {{EOG based control of mobile assistive platforms for the severely disabled}},
year = {2005}
}
@article{Fierrez2018c,
abstract = {The present paper is Part 2 in this series of two papers. In Part 1 we provided an introduction to Multiple Classifier Systems (MCS) with a focus into the fundamentals: basic nomenclature, key elements, architecture, main methods, and prevalent theory and framework. Part 1 then overviewed the application of MCS to the particular field of multimodal biometric person authentication in the last 25 years, as a prototypical area in which MCS has resulted in important achievements. Here in Part 2 we present in more technical detail recent trends and developments in MCS coming from multimodal biometrics that incorporate context information in an adaptive way. These new MCS architectures exploit input quality measures and pattern-specific particularities that move apart from general population statistics, resulting in robust multimodal biometric systems. Similarly as in Part 1, methods here are described in a general way so they can be applied to other information fusion problems as well. Finally, we also discuss here open challenges in biometrics in which MCS can play a key role.},
author = {Fierrez, Julian and Morales, Aythami and Vera-Rodriguez, Ruben and Camacho, David},
doi = {10.1016/j.inffus.2017.12.005},
issn = {15662535},
journal = {Information Fusion},
keywords = {Adaptive,Biometrics,Classifier,Context,Fusion,Multimodal},
number = {November 2017},
pages = {103--112},
publisher = {Elsevier},
title = {{Multiple classifiers in biometrics. Part 2: Trends and challenges}},
url = {https://doi.org/10.1016/j.inffus.2017.12.005},
volume = {44},
year = {2018}
}
@article{Phillips2009,
abstract = {The goal of the Multiple Biometrics Grand Challenge (MBGC) is to improve the performance of face and iris recognition technology from biometric samples acquired under unconstrained conditions. The MBGC is organized into three challenge problems. Each challenge problem re- laxes the acquisition constraints in different directions. In the Portal Challenge Problem, the goal is to recognize people from near-infrared (NIR) and high definition (HD) video as they walk through a portal. Iris recognition can be performed from the NIR video and face recognition from the HD video. The availability of NIR and HD modalities allows for the development of fusion algorithms. The Still Face Challenge Problem has two primary goals. The first is to improve recognition performance from frontal and off angle still face images taken under uncontrolled in- door and outdoor lighting. The second is to improve recognition perfor- mance on still frontal face images that have been resized and compressed, as is required for electronic passports. In the Video Challenge Problem, the goal is to recognize people from video in unconstrained environments. The video is unconstrained in pose, illumination, and camera angle. All three challenge problems include a large data set, experiment descrip- tions, ground truth, and scoring code.},
author = {Phillips, P. Jonathon and Flynn, Patrick J. and Beveridge, J. Ross and Scruggs, W. Todd and O'Toole, Alice J. and Bolme, David and Bowyer, Kevin W. and Draper, Bruce A. and Givens, Geof H. and Lui, Yui Man and Sahibzada, Hassan and Scallan, Joseph A. and Weimer, Samuel},
doi = {10.1007/978-3-642-01793-3_72},
isbn = {3642017924},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {705--714},
title = {{Overview of the multiple biometrics grand challenge}},
volume = {5558 LNCS},
year = {2009}
}
@article{Zhao2017a,
annote = {Good article describing a lot of the state of the art deep learning approaches being researched.

Keynotes:

Not much work with deep learning has been done in iris recognition.

THey Use a Fully Convolutional Network (FCN) and talk about others who ahve used a Convolutional Neural Network (CNN). They also mention a Deep Belief Net (DBN) that others have used. DeepIrisNet is also mentioned and tested with

THey have created their own loss function optimized for iris recognition called Extended Triplet Loss (ETL)

Their network is generalizable to other databases meaning that it doesn't require finetuing as many others do.

Used ND-IRIS, CASIA Iris, IITD Iris and WVU Non-Ideal Iris databases to test on.},
author = {Zhao, Zijing and Kumar, Ajay},
doi = {10.1109/ICCV.2017.411},
isbn = {978-1-5386-1032-9},
journal = {2017 IEEE International Conference on Computer Vision (ICCV)},
pages = {3829--3838},
title = {{Towards More Accurate Iris Recognition Using Deeply Learned Spatially Corresponding Features}},
url = {http://ieeexplore.ieee.org/document/8237673/},
year = {2017}
}
@inproceedings{Misztal2012,
abstract = {A new method for finding the rotation angle in iris images for biometric identification is presented in this paper. The proposed approach is based on Fourier descriptors analysis and algebraic properties of vector rotation in complex space. {\textcopyright} 2012 IFIP International Federation for Information Processing.},
author = {Misztal, Krzysztof and Tabor, Jacek and Saeed, Khalid},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-33260-9-11},
isbn = {9783642332593},
issn = {03029743},
keywords = {Fourier descriptors,Iris pattern recognition,rotation estimation,rotation recovery},
pages = {135--145},
title = {{A new algorithm for rotation detection in iris pattern recognition}},
volume = {7564 LNCS},
year = {2012}
}
@inproceedings{Yin2011,
abstract = {In this paper, the acquisition and content of a new homologous multimodal biometric database are presented. The SDUMLA-HMT database consists of face images from 7 view angles, finger vein images of 6 fingers, gait videos from 6 view angles, iris images from an iris sensor, and fingerprint images acquired with 5 different sensors. The database includes real multimodal data from 106 individuals. In addition to database description, we also present possible use of the database. The database is available to research community through http://mla.sdu.edu.cn/sdumla-hmt.html .},
author = {Yin, Yilong and Liu, Lili and Sun, Xiwei},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-25449-9_33},
isbn = {9783642254482},
issn = {03029743},
keywords = {Biometrics,Face,Finger vein,Fingerprint,Gait,Homologous,Iris,Multi-modal},
pages = {260--268},
title = {{SDUMLA-HMT: A multimodal biometric database}},
volume = {7098 LNCS},
year = {2011}
}
@article{Suk2014,
abstract = {For the last decade, it has been shown that neuroimaging can be a potential tool for the diagnosis of Alzheimer's Disease (AD) and its prodromal stage, Mild Cognitive Impairment (MCI), and also fusion of different modalities can further provide the complementary information to enhance diagnostic accuracy. Here, we focus on the problems of both feature representation and fusion of multimodal information from Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET). To our best knowledge, the previous methods in the literature mostly used hand-crafted features such as cortical thickness, gray matter densities from MRI, or voxel intensities from PET, and then combined these multimodal features by simply concatenating into a long vector or transforming into a higher-dimensional kernel space. In this paper, we propose a novel method for a high-level latent and shared feature representation from neuroimaging modalities via deep learning. Specifically, we use Deep Boltzmann Machine (DBM).22Although it is clear from the context that the acronym DBM denotes "Deep Boltzmann Machine" in this paper, we would clearly indicate that DBM here is not related to "Deformation Based Morphometry"., a deep network with a restricted Boltzmann machine as a building block, to find a latent hierarchical feature representation from a 3D patch, and then devise a systematic method for a joint feature representation from the paired patches of MRI and PET with a multimodal DBM. To validate the effectiveness of the proposed method, we performed experiments on ADNI dataset and compared with the state-of-the-art methods. In three binary classification problems of AD vs. healthy Normal Control (NC), MCI vs. NC, and MCI converter vs. MCI non-converter, we obtained the maximal accuracies of 95.35{\%}, 85.67{\%}, and 74.58{\%}, respectively, outperforming the competing methods. By visual inspection of the trained model, we observed that the proposed method could hierarchically discover the complex latent patterns inherent in both MRI and PET.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Suk, Heung Il and Lee, Seong Whan and Shen, Dinggang},
doi = {10.1016/j.neuroimage.2014.06.077},
eprint = {NIHMS150003},
isbn = {1053-8119},
issn = {10959572},
journal = {NeuroImage},
keywords = {Alzheimer's Disease,Deep boltzmann machine,Mild cognitive impairment,Multimodal data fusion,Shared feature representation},
pages = {569--582},
pmid = {25042445},
publisher = {Elsevier Inc.},
title = {{Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2014.06.077},
volume = {101},
year = {2014}
}
@article{Arslan2017,
abstract = {Biometric systems may be used to create a remote access model on devices, ensure personal data protection, personalize and facilitate the access security. Biometric systems are generally used to increase the security level in addition to the previous authentication methods and they seen as a good solution. Biometry occupies an important place between the areas of daily life of the machine learning. In this study; the techniques, methods, technologies used in biometric systems are researched, machine learning techniques used biometric aplications are investigated for the security perspective, the advantages and disadvantages that these tecniques provide are given. The studies in the literature between 2010-2016 years, used algorithms, technologies, metrics, usage areas, the machine learning techniques used for different biometric systems such as face, palm prints, iris, voice, fingerprint recognition are researched and the studies made are evaluated. The level of security provided by the use of biometric systems by developed using machine learning and disadvantages that arise in the use of these systems are stated in detail in the study. Also, impact on people of biometric methods in terms of ease of use, security and usages areas are examined.},
author = {Arslan, B and Yorulmaz, E and Akca, B and Sagiroglu, S},
doi = {10.1109/ICMLA.2016.183},
isbn = {9781509061662},
journal = {Proceedings - 2016 15th IEEE International Conference on Machine Learning and Applications, ICMLA 2016},
keywords = {Biometric,Face,Fingerprint,Iris,Machine learning,Recognition,Security,Teeth,Voice},
pages = {492--497},
title = {{Security perspective of Biometric recognition and machine learning techniques}},
year = {2017}
}
@article{Galdi2017a,
author = {Galdi, Chiara and Dugelay, Jean Luc},
doi = {10.1109/ICPR.2016.7899626},
isbn = {9781509048472},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
pages = {160--164},
title = {{Fusing iris colour and texture information for fast iris recognition on mobile devices}},
year = {2017}
}
@article{Fierrez2018b,
abstract = {We provide an introduction to Multiple Classifier Systems (MCS) including basic nomenclature and describing key elements: classifier dependencies, type of classifier outputs, aggregation procedures, architecture, and types of methods. This introduction complements other existing overviews of MCS, as here we also review the most prevalent theoretical framework for MCS and discuss theoretical developments related to MCS. The introduction to MCS is then followed by a review of the application of MCS to the particular field of multimodal biometric person authentication in the last 25 years, as a prototypical area in which MCS has resulted in important achievements. This review includes general descriptions of successful MCS methods and architectures in order to facilitate the export of them to other information fusion problems. Based on the theory and framework introduced here, in the companion paper we then develop in more technical detail recent trends and developments in MCS from multimodal biometrics that incorporate context information in an adaptive way. These new MCS architectures exploit input quality measures and pattern-specific particularities that move apart from general population statistics, resulting in robust multimodal biometric systems. Similarly as in the present paper, methods in the companion paper are introduced in a general way so they can be applied to other information fusion problems as well. Finally, also in the companion paper, we discuss open challenges in biometrics and the role of MCS to advance them.},
author = {Fierrez, Julian and Morales, Aythami and Vera-Rodriguez, Ruben and Camacho, David},
doi = {10.1016/j.inffus.2017.12.003},
issn = {15662535},
journal = {Information Fusion},
keywords = {Adaptive,Biometrics,Classifier,Context,Fusion,Multimodal},
number = {November 2017},
pages = {57--64},
publisher = {Elsevier},
title = {{Multiple classifiers in biometrics. part 1: Fundamentals and review}},
url = {https://doi.org/10.1016/j.inffus.2017.12.003},
volume = {44},
year = {2018}
}
@article{Chen2018,
abstract = {In this work, we address the problem of face verification, namely determining whether a pair of face images belongs to the same or different subjects. Previous works often consider solving the problem of face verification in two steps: feature extraction and face recognition, resulting in a fragmented procedure. We argue that these techniques, although working well, fail to explicitly exploit a full end-to-end framework for face verification, which has received much attention and achieved significant improvements recently. In this paper, we propose a novel Joint Bayesian guided metric learning technique for dealing with the face verification task, which well integrates the above two steps of face verification into an end-to-end convolutional neural network (CNN) architecture. In the training stage, an initial neural network, which has the similar architecture with GoogLeNet CNN model, is firstly pre-trained by optimizing classification-based objective functions on the publicly available CASIA WebFace database. Based on constructed face pairs dataset from CASIA WebFace and LFW datasets, we then fine-tune the whole network parameters under the guide of the learned knowledge, which is obtained from the highly successful Joint Bayesian model. This guided learning procedure, which can also be seen as a metric learning technique, can further update network parameters for discriminating face pairs. In the testing process, the outputs by this unified network are discriminated with a threshold value to produce the ultimate prediction for the face verification task. Comprehensive evaluations over the LFW dataset well demonstrate the encouraging face verification performance of our proposed framework.},
author = {Chen, Di and Xu, Chunyan and Yang, Jian and Qian, Jianjun and Zheng, Yuhui and Shen, Linlin},
doi = {10.1016/J.NEUCOM.2017.09.009},
issn = {0925-2312},
journal = {Neurocomputing},
month = {jan},
pages = {560--567},
publisher = {Elsevier},
title = {{Joint Bayesian guided metric learning for end-to-end face verification}},
url = {https://www-sciencedirect-com.zorac.aub.aau.dk/science/article/pii/S0925231217314807{\#}bib0016},
volume = {275},
year = {2018}
}
@article{Xiang2017,
abstract = {Estimating the 6D pose of known objects is important for robots to interact with the real world. The problem is challenging due to the variety of objects as well as the complexity of a scene caused by clutter and occlusions between objects. In this work, we introduce PoseCNN, a new Convolutional Neural Network for 6D object pose estimation. PoseCNN estimates the 3D translation of an object by localizing its center in the image and predicting its distance from the camera. The 3D rotation of the object is estimated by regressing to a quaternion representation. We also introduce a novel loss function that enables PoseCNN to handle symmetric objects. In addition, we contribute a large scale video dataset for 6D object pose estimation named the YCB-Video dataset. Our dataset provides accurate 6D poses of 21 objects from the YCB dataset observed in 92 videos with 133,827 frames. We conduct extensive experiments on our YCB-Video dataset and the OccludedLINEMOD dataset to show that PoseCNN is highly robust to occlusions, can handle symmetric objects, and provide accurate pose estimation using only color images as input. When using depth data to further refine the poses, our approach achieves state-of-the-art results on the challenging OccludedLINEMOD dataset.},
archivePrefix = {arXiv},
arxivId = {1711.00199},
author = {Xiang, Yu and Schmidt, Tanner and Narayanan, Venkatraman and Fox, Dieter},
eprint = {1711.00199},
file = {:home/nicstar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiang et al. - 2017 - PoseCNN A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes.pdf:pdf},
month = {nov},
title = {{PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes}},
url = {https://arxiv.org/abs/1711.00199},
year = {2017}
}
@article{Postelnicu2012,
abstract = {This paper presents an EOG-based (Electrooculography) interface for Human Computer Interface (HCI) purposes. The solution enables the filtering of the recorded signals and identification of characteristic peak amplitudes associated with eye saccades, blinks or winks by using a classifier based on a set of fuzzy logic rules and a deterministic finite automaton. The identified eye saccades were assigned to six low-level commands for navigation purposes. An experiment study was conducted in order to check the accuracy and the performances of the proposed interface compared with three traditional input control interfaces. Experimental results show that the developed interface has good performance and can be used for online communication and control in EOG-based HCI systems or even for first-person navigation metaphors in games industry. {\textcopyright}2012 Elsevier Ltd. All rights reserved.},
author = {Postelnicu, Cristian Cezar and Girbacia, Florin and Talaba, Doru},
doi = {10.1016/j.eswa.2012.03.007},
isbn = {09574174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Deterministic finite automaton,Electrooculography,Fuzzy logic,Human computer interaction,Navigation,Virtual environment},
number = {12},
pages = {10857--10866},
publisher = {Elsevier Ltd},
title = {{EOG-based visual navigation interface development}},
url = {http://dx.doi.org/10.1016/j.eswa.2012.03.007},
volume = {39},
year = {2012}
}
@article{Crossdata2018,
abstract = {In this paper we study face recognition using convolutional neural network. First, we introduced the basic CNN neural network architecture. Second, we modify the traditional neural network and adapt it to another database by fine tuning its parameters. Third, the network architecture is extended to the cross database problem. The CNN is first trained on a large dataset and then tested on another. Experimental results show that the proposed algorithm is suitable for building various real world applications.},
author = {Guo, Mei and Xiao, Min and Gong, Deliang},
doi = {10.1007/978-3-319-69096-4_54},
journal = {Advances in Intelligent Systems and Computing},
keywords = {Deep neural network {\'{A}},Face recognition {\'{A}},Image processing},
title = {{Face Recognition Using Deep Convolutional Neural Network in Cross-Database Study}},
url = {https://link-springer-com.zorac.aub.aau.dk/content/pdf/10.1007{\%}2F978-3-319-69096-4{\_}54.pdf},
year = {2017}
}
@article{Arsalan2017,
author = {Arsalan, Muhammad and Hong, Hyung Gil and Naqvi, Rizwan Ali and Lee, Min Beom and Kim, Min Cheol and Kim, Dong Seop and Kim, Chan Sik and Park, Kang Ryoung},
doi = {10.3390/sym9110263},
issn = {20738994},
journal = {Symmetry},
keywords = {Biometrics,Convolutional neural network (CNN),Iris recognition,Iris segmentation},
number = {11},
title = {{Deep learning-based iris segmentation for iris recognition in visible light environment}},
volume = {9},
year = {2017}
}
@article{Khan2017,
annote = {A good article that uses a new database containing smartphone iris images. Theses are taken in visible light. They use Daugmans approach to localize the iris, then they normalize it. They use wavelets on the image to extract the desired featues and then try to classify them using SVM (97{\%}), KNN (95.1{\%}) and LDA (94.28{\%}).

Keynotes:

A different study has used Sparse Reconstruction Classifier with K-means clustering

A different study obtained 99{\%} accuracy using SVM and Hamming Distance

They tried SVM, K-means, Linear Discrimintant in their own study as classifiers.},
author = {Khan, Fahim Faysal and Akif, Ahnaf and Haque, M A},
isbn = {9781538633748},
pages = {26--28},
title = {{Iris Recognition using Machine Learning from Smartphone Captured Images in Visible Light}},
year = {2017}
}
@article{Daugman2009,
abstract = {This chapter explains the iris recognition algorithms and presents results of 9.1 million comparisons among eye images from trials in Britain, the USA, Japan, and Korea. The key to iris recognition is the failure of a test of statistical independence, which involves so many degrees-of-freedom that this test is virtually guaranteed to be passed whenever the phase codes for two different eyes are compared, but to be uniquely failed when any eye's phase code is compared with another version of itself. The test of statistical independence is implemented by the simple Boolean Exclusive-OR operator (XOR) applied to the 2048 bit phase vectors that encode any two iris patterns, masked (AND'ed) by both of their corresponding mask bit vectors to prevent non iris artifacts from influencing iris comparisons. The XOR operator detects disagreement between any corresponding pair of bits, while the AND operator ensures that the compared bits are both deemed to have been uncorrupted by eyelashes, eyelids, specular reflections, or other noise. The norms of the resultant bit vector and of theAND'ed mask vectors are then measured in order to compute a fractional Hamming Distance as the measure of the dissimilarity between any two irises, whose two phase code bit vectors are denoted {\{}. codeA, codeB{\}} and whose mask bit vectors are denoted {\{}. maskA, maskB{\}}. {\textcopyright}2009 Elsevier Inc. All rights reserved.},
annote = {A chapter about how Iris Recognitions works in general. John Daugman is the creator of IrisCode, a 2D Gabor wavelet-based iris recognition algorithm that is the basis of all publicly deployed automatic iris recognition systems and which has registered more than a billion persons worldwide in government ID programs.

Keywords/phrases:
Near Infra Red (NIR) images can be used for iris capturing.

Gabor wavelets are used for determining the inter and outer edges of an iris.

Often the iris will not be circular because an eyelid will cover it.

Hamming distance is used when comparing two irises.},
author = {Daugman, John},
doi = {10.1016/B978-0-12-374457-9.00025-1},
isbn = {9780123744579},
issn = {10518215},
journal = {The Essential Guide to Image Processing},
pages = {715--739},
pmid = {20810146},
title = {{How Iris Recognition Works}},
year = {2009}
}
@article{Petrovska-Delacretaz2008a,
abstract = {Face recognition finds its place in a large number of applications. They occur in different contexts related to security, entertainment or Internet applications. Reliable face recognition is still a great challenge to computer vision and pattern recognition researchers, and new algorithms need to be evaluated on relevant databases. The publicly available IV2 database allows monomodal and multimodal experiments using face data. Known variabilities, that are critical for the performance of the biometric systems (such as pose, expression, illumination and quality) are present. The face and subface data that are acquired in this database are: 2D audio-video talking-face sequences, 2D stereoscopic data acquired with two pairs of synchronized cameras, 3D facial data acquired with a laser scanner, and iris images acquired with a portable infrared camera. The IV2 database is designed for monomodal and multimodal experiments. The quality of the acquired data is of great importance. Therefore as a first step, and in order to better evaluate the quality of the data, a first internal evaluation was conducted. Only a small amount of the total acquired data was used for this evaluation: 2D still images, 3D scans and iris images. First results show the interest of this database. In parallel to the research algorithms, open-source reference systems were also run for baseline comparisons.},
author = {Petrovska-Delacr{\'{e}}taz, D. and Lelandais, S. and Colineau, J. and Chen, L. and Dorizzi, B. and Ardabilian, M. and Krichen, E. and Mellakh, M. A. and Chaari, A. and Guerfi, S. and D'Hose, J. and Amor, B. Ben},
doi = {10.1109/BTAS.2008.4699323},
isbn = {9781424427307},
journal = {BTAS 2008 - IEEE 2nd International Conference on Biometrics: Theory, Applications and Systems},
pages = {3--9},
title = {{The IV2 multimodal biometric database (including Iris, 2D, 3D, stereoscopic, and talking face data), and the IV2-2007 evaluation campaign}},
volume = {00},
year = {2008}
}
@article{Kim2016,
abstract = {The iris recognition on a mobile phone is different from the conventional iris recognition implemented on a dedicated device in that the computational power of a mobile phone and the space for placing NIR (near infrared) LED (light emitting diode) illuminators and iris camera are limited. This paper raises these issues in detail based on real implementation of an iris recognition system in a mobile phone and proposes some solutions to these issues. An experimental study was conducted to search for the relevant power and wavelength of NIR LED illuminators with their positioning on a phone for capturing a good quality iris image. Subsequently, in view of the disparity between the user's gazing point and the center of the iris camera which causes degradation of acquired iris images, an experiment was performed to locate the appropriate gazing point for good iris image capture. A fast eye detection algorithm was proposed for implementation under the mobile platform with low computational facility. The experiments were conducted on a currently released mobile phone and the results showed promising potential for adoption of iris recognition as a reliable authentication means. As a result, two 850 nm LEDs were selected for iris illumination at 1.1 cm away from the iris camera for the size of a 7 cm × 13.7 cm phone. In the performance, the recognition accuracy was 0.1{\%} EER (equal error rate) and the eye detection rate with the speed of 17.64 ms on a mobile phone was 99.4{\%}.},
annote = {Keynotes:
Contributes with a good NIR mobile algorithm that's fast on mobile phones. They also contribute with an mobile iris database of 500 images that they are willing to share for research. Could not find it online.},
author = {Kim, Dongik and Jung, Yujin and Toh, Kar Ann and Son, Byungjun and Kim, Jaihie},
doi = {10.1016/j.eswa.2016.01.050},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Handheld,Iris recognition,Mobile,Portable,Smartphone},
pages = {328--339},
publisher = {Elsevier Ltd},
title = {{An empirical study on iris recognition in a mobile phone}},
url = {http://dx.doi.org/10.1016/j.eswa.2016.01.050},
volume = {54},
year = {2016}
}
@article{Krizhevsky2017a,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0 {\%} which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2 {\%} achieved by the second-best entry. 1},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
doi = {10.1145/3065386},
eprint = {1102.0183},
isbn = {9781627480031},
issn = {00010782},
journal = {Communications of the ACM},
month = {may},
number = {6},
pages = {84--90},
pmid = {7491034},
publisher = {ACM},
title = {{ImageNet classification with deep convolutional neural networks}},
url = {http://dl.acm.org/citation.cfm?doid=3098997.3065386},
volume = {60},
year = {2017}
}
@article{Cheng2017a,
abstract = {Sea–land segmentation and ship detection are two prevalent research domains for optical remote sensing harbor images and can find many applications in harbor supervision and management. As the spatial resolution of imaging technology improves, traditional methods struggle to perform well due to the complicated appearance and background distributions. In this paper, we unify the above two tasks into a single framework and apply the deep convolutional neural networks to predict pixelwise label for an input. Specifically, an edge aware convolutional network is proposed to parse a remote sensing harbor image into three typical objects, e.g., sea, land, and ship. Two innovations are made on top of the deep structure. First, we design a multitask model by simultaneously training the segmentation and edge detection networks. Hierarchical semantic features from the segmentation network are extracted to learn the edge network. Second, the outputs of edge pipeline are further employed to refine entire model by adding an edge aware regularization, which helps our method to yield very desirable results that are spatially consistent and well boundary located. It also benefits the segmentation of docked ships that are quite challenging for many previous methods. Experimental results on two datasets collected from Google Earth have demonstrated the effectiveness of our approach both in quantitative and qualitative performance compared with state-of-the-art methods.},
author = {Cheng, Dongcai and Meng, Gaofeng and Xiang, Shiming and Pan, Chunhong},
doi = {10.1109/JSTARS.2017.2747599},
issn = {21511535},
journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
keywords = {Edge aware regularization,harbor images,multitask learning,semantic segmentation},
number = {12},
pages = {5769--5783},
title = {{FusionNet: Edge Aware Deep Convolutional Networks for Semantic Segmentation of Remote Sensing Harbor Images}},
volume = {10},
year = {2017}
}
@article{Duguleana2010,
abstract = {This paper proposes a new approach to real-time robot controlling by integrating an Electrooculography (EOG) measuring device within human-robot interaction (HRI). Our study focuses on controlling robots using EOG for fulfilling elementary robot activities such as basic motor movements and environment interaction. A new EOG-based HRI paradigm has been developed on the specific defined problem of eye blinking. The resulted model is tested using biometric capturing equipment. We propose a simple algorithm for real-time identification and processing of signals produced by eyes during blinking phases. We present the experimental setup and the results of the experiment. We conclude by listing further research issues.},
author = {Duguleana, Mihai and Mogan, Gheorghe},
doi = {10.1007/978-3-642-11628-5_37},
isbn = {9783642116278},
issn = {18684238},
journal = {IFIP Advances in Information and Communication Technology},
keywords = {Control,Electrooculography,Eye blink,Human-robot interaction,Robot},
number = {29},
pages = {343--350},
title = {{Using eye blinking for EOG-based robot control}},
volume = {314},
year = {2010}
}
@article{Jesus2017,
author = {Jesus, Rosales Banderas Jose De and Maximo, Lopez Sanchez and Raul, Pinto Elias and Gabriel, Gonzalez Serna},
doi = {10.1109/CSCI.2016.0167},
isbn = {9781509055104},
journal = {Proceedings - 2016 International Conference on Computational Science and Computational Intelligence, CSCI 2016},
keywords = {color calibration,image processing,image stabilizer,iris detection},
pages = {861--864},
title = {{Methodology for Iris Scanning through Smartphones}},
year = {2017}
}
@inproceedings{Tan2014,
abstract = {In this paper, we address the problem of object tracking in intensity images and depth data. We propose a generic framework that can be used either for tracking 2D templates in intensity images or for tracking 3D objects in depth images. To overcome problems like partial occlusions, strong illumination changes and motion blur, that notoriously make energy minimization-based tracking methods get trapped in a local minimum, we propose a learning-based method that is robust to all these problems. We use random forests to learn the relation between the parameters that defines the object's motion, and the changes they induce on the image intensities or the point cloud of the template. It follows that, to track the template when it moves, we use the changes on the image intensities or point cloud to predict the parameters of this motion. Our algorithm has an extremely fast tracking performance running at less than 2 ms per frame, and is robust to partial occlusions. Moreover, it demonstrates robustness to strong illumination changes when tracking templates using intensity images, and robustness in tracking 3D objects from arbitrary viewpoints even in the presence of motion blur that causes missing or erroneous data in depth images. Extensive experimental evaluation and comparison to the related approaches strongly demonstrates the benefits of our method.},
author = {Tan, David Joseph and Ilic, Slobodan},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.157},
isbn = {9781479951178},
issn = {10636919},
month = {jun},
pages = {1202--1209},
publisher = {IEEE},
title = {{Multi-forest tracker: A Chameleon in tracking}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909553},
year = {2014}
}
@article{Khiari-Hili2017a,
author = {Khiari-Hili, Nefissa and Montagne, Christophe and Lelandais, Sylvie and Hamrouni, Kamel},
doi = {10.1109/IPTA.2016.7820954},
isbn = {9781467389105},
journal = {2016 6th International Conference on Image Processing Theory, Tools and Applications, IPTA 2016},
keywords = {Multimodal biometrics,authentication,dynamic weighted sum,face,iris,quality,score fusion},
pages = {1--6},
title = {{Quality dependent multimodal fusion of face and iris biometrics}},
year = {2017}
}
@article{Nguyen2010,
author = {Nguyen, Kien and Fookes, Clinton and Sridharan, Sridha},
doi = {10.1145/1852611.1852635},
isbn = {9781450301053},
journal = {Proceedings of the 2010 Symposium on Information and Communication Technology - SoICT '10},
keywords = {iris recognition,mbgc,signal-level fusion,super-resolution},
number = {November},
pages = {122},
title = {{Robust mean super-resolution for less cooperative NIR iris recognition at a distance and on the move}},
url = {http://portal.acm.org/citation.cfm?doid=1852611.1852635},
year = {2010}
}
@article{Kim2013,
author = {Kim, Myoung Ro and Yoon, Gilwon},
journal = {International Journal of Electrical, Computer, Energetic, Electronic and Communication Engineering},
keywords = {ddr game,eog,eye movement},
number = {10},
pages = {1352--1355},
title = {{Control Signal from EOG Analysis and Its Application}},
volume = {7},
year = {2013}
}
@article{Nam2016a,
abstract = {We propose Dual Attention Networks (DANs) which jointly leverage visual and textual attention mechanisms to capture fine-grained interplay between vision and language. DANs attend to specific regions in images and words in text through multiple steps and gather essential information from both modalities. Based on this framework, we introduce two types of DANs for multimodal reasoning and matching, respectively. The reasoning model allows visual and textual attentions to steer each other during collaborative inference, which is useful for tasks such as Visual Question Answering (VQA). In addition, the matching model exploits the two attention mechanisms to estimate the similarity between images and sentences by focusing on their shared semantics. Our extensive experiments validate the effectiveness of DANs in combining vision and language, achieving the state-of-the-art performance on public benchmarks for VQA and image-text matching.},
archivePrefix = {arXiv},
arxivId = {1611.00471},
author = {Nam, Hyeonseob and Ha, Jung-Woo and Kim, Jeonghee},
doi = {10.1109/CVPR.2017.232},
eprint = {1611.00471},
isbn = {978-1-5386-0457-1},
issn = {1611.00471},
pages = {299--307},
title = {{Dual Attention Networks for Multimodal Reasoning and Matching}},
url = {http://arxiv.org/abs/1611.00471},
year = {2016}
}
@article{Kuehlkamp2016,
abstract = {Iris recognition systems are a mature technology that is widely used throughout the world. In identification (as opposed to verification) mode, an iris to be recognized is typically matched against all N enrolled irises. This is the classic "1-to-N search". In order to improve the speed of large-scale identification, a modified "1-to-First" search has been used in some operational systems. A 1-to-First search terminates with the first below-threshold match that is found, whereas a 1-to-N search always finds the best match across all enrollments. We know of no previous studies that evaluate how the accuracy of 1-to-First search differs from that of 1-to-N search. Using a dataset of over 50,000 iris images from 2,800 different irises, we perform experiments to evaluate the relative accuracy of 1-to-First and 1-to-N search. We evaluate how the accuracy difference changes with larger numbers of enrolled irises, and with larger ranges of rotational difference allowed between iris images. We find that False Match error rate for 1-to-First is higher than for 1-to-N, and the the difference grows with larger number of enrolled irises and with larger range of rotation.},
annote = {The way that iris recognition works is that some kind of filter is applied to localize the iris. 2D Gador filter is often cited. Then that iris is checked against the whole database. This is called 1:N. They check the Hamming Distance between of the bits of the data and then choose the lowest ones as a pair. In 1:First search they do the same except there is is a threshold that that is has to be under to be accepted. If it is under that threshold it is accepted and the search is stopped. Two types of error can occour: a False Match (FM) and False Non-Match (FNM). A false match occurs when two samples from different individuals are declared by the system as a match. A false non-match is when two samples from the same individual fail to be considered as a match by the system},
archivePrefix = {arXiv},
arxivId = {1702.01167},
author = {Kuehlkamp, Andrey and Bowyer, Kevin W},
doi = {10.1109/WACV.2016.7477687},
eprint = {1702.01167},
isbn = {9781509006410},
journal = {2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016},
title = {{An analysis of 1-to-first matching in iris recognition}},
year = {2016}
}
@inproceedings{Or-El2015,
abstract = {The popularity of low-cost RGB-D scanners is increasing on a daily basis. Nevertheless, existing scanners often cannot capture subtle details in the environment. We present a novel method to enhance the depth map by fusing the intensity and depth information to create more detailed range profiles. The lighting model we use can handle natural scene illumination. It is integrated in a shape from shading like technique to improve the visual fidelity of the reconstructed object. Unlike previous efforts in this do- main, the detailed geometry is calculated directly, without the need to explicitly find and integrate surface normals. In addition, the proposed method operates four orders of magnitude faster than the state of the art. Qualitative and quantitative visual and statistical evidence support the im- provement in the depth obtained by the suggested method.},
author = {Or-El, Roy and Rosman, Guy and Wetzler, Aaron and Kimmel, Ron and Bruckstein, Alfred M.},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7299179},
isbn = {9781467369640},
issn = {10636919},
month = {jun},
pages = {5407--5416},
pmid = {7299179},
publisher = {IEEE},
title = {{RGBD-fusion: Real-time high precision depth recovery}},
url = {http://ieeexplore.ieee.org/document/7299179/},
volume = {07-12-June},
year = {2015}
}
@article{Ross2003,
annote = {Nice and clear overview of the four basic modules of the standard biometric system.

Very clear explainations of different aspects, but a bit limited in respect to the separation of methods into categories .},
author = {Ross, Arun and Jain, Anil},
isbn = {1517355931},
keywords = {biometrics,decision tree,face,fingerprints,hand geometry,linear discriminant analysis,multimodal,sum rule,verification},
number = {13},
pages = {2115--2125},
title = {{Information Fusion in Biometrics}},
volume = {24},
year = {2003}
}
@incollection{Bowyer2016,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-{\$}\alpha{\$}-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA}for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bowyer, Kevin W and Hollingsworth, Karen P and Flynn, Patrick J},
booktitle = {Handbook of Iris Recognition},
doi = {10.1007/978-1-4471-6784-6},
eprint = {arXiv:1011.1669v3},
isbn = {978-1-4471-6782-2},
issn = {16130073},
pages = {23--61},
pmid = {25246403},
title = {{Chapter 2 A Survey of Iris Biometrics Research: 2008–2010}},
url = {http://link.springer.com/10.1007/978-1-4471-6784-6},
year = {2016}
}
@book{Wechsler2007,
abstract = {One of the grand challenges for computational intelligence and biometrics is to understand how people process and recognize faces and to develop automated and reliable face recognition systems. Biometrics has become the major component in the complex decision making process associated with security applications. The face detection and authentication challenges addressed include cluttered environments, image variability, occlusion and disguise, and temporal changes all within open set recognition. Reliable Face Recognition Methods: System Design, Implementation and Evaluation comprehensively explores the face recognition problem while drawing inspiration from complementary disciplines such as neurosciences, statistics, signal and image processing, computer vision, and machine learning and pattern recognition. This book also examines the evolution of face recognition research and explores promising new avenues for research and development. Reliable Face Recognition Methods: System Design, Implementation and Evaluation benefits graduate-level students, researchers, and practitioners, as well as government and industry decision makers in the security arena. Ruud Bolle (IBM): "Harry Wechsler's monograph provides a thorough, up-to-date, and in-depth overview of the many advances in the area of face recognition. gives an excellent overview of the issues related to security and privacy when it comes to automated biometrics. In summary, this book has the potential to become a classic. Harry Wechsler is to be commended for undertaking the monumental task of writing this book." John Daugman (Cambridge University, UK): "The book looks excellent. The topic is timely, and the perspective-multi-disciplinary, measured, and objective - is much needed and welcome. I believe that Wechsler's latest book will make a valued contribution to this important field and will become a standard." David Zhang (Hong Kong Polytechnic University, China): "From a system view, this book shows an excellent arrangement, from data collection to face recognition, as well as system performance evaluation and error analysis. This book can serve both as an interdisciplinary text and a research reference. Each chapter provides the background and impetus for understanding the problems discussed." Stan Li (Chinese Academy of Sciences, China): "The book treats the subject of face recognition in a fairly systematic way. The book covers most relevant topics in face recognition, in a well organized way. The information is also useful for experts. The manuscript is easy to read." Tom Huang (University of Illinois, USA): "Harry Wechsler is without doubt one of the leading authorities in face recognition and related topics. We have recently seen a number of excellent edited books on Face Recognition. However, Wechsler's book is the first unified treatment of this important subject. In my opinion, Wechsler is one of only a handful of people in the world who could write such a comprehensive, unified, informative, perceptive, and authoritative book on Face Recognition. Harry Wechsler so far, is the only one of this handful who took the time and effort to realize such a project. The book certainly will have a great positive impact on the biometrics research community. But also, because it looks at Face Recognition from different perspectives, it will be welcomed by non-experts." {\textcopyright}2007 Springer Science+Business Media, LLC. All rights reserved.},
address = {Boston, MA},
author = {Wechsler, Harry},
booktitle = {Reliable Face Recognition Methods: System Design, Impementation and Evaluation},
doi = {10.1007/978-0-387-38464-1},
isbn = {038722372X},
pages = {1--329},
publisher = {Springer US},
title = {{Reliable face recognition methods: System design, implementation and evaluation}},
url = {http://link.springer.com/10.1007/978-0-387-38464-1 https://link-springer-com.zorac.aub.aau.dk/book/10.1007{\%}2F978-0-387-38464-1{\#}about},
year = {2007}
}
@article{Fierrez-Aguilar2003a,
abstract = {The aim of this paper, regarding multimodal biometric verification, is twofold: on the one hand, some score fusion strategies reported in the literature are reviewed and, on the other hand, we compare experimentally a selection of them using as monomodal baseline experts: i) our face verification system based on a global face appearance representation scheme, ii) our minutiaebased fingerprint verification system, and iii) our on-line signature verification system based on HMM modeling of temporal functions, on the MCYT multimodal database. A new strategy is also proposed and discussed in order to generate a multimodal combined score by means of Support Vector Machine (SVM) classifiers from which user-independent and user-dependent fusion schemes are derived and evaluated.},
author = {Fierrez-Aguilar, J and Ortega-Garcia, J and Garcia-Romero, D and Gonzalez-Rodriguez, J},
isbn = {3-540-40302-7},
issn = {03029743},
journal = {Proc. AVBPA},
pages = {1056},
title = {{A Comparative Evaluation of Fusion Strategies for Multimodal Biometric Verification}},
year = {2003}
}
@article{Percy,
author = {Percy, Oad},
journal = {Blekinge Institute of Technology},
pages = {1--48},
title = {{Iris localization using Daugman ' s algorithm}},
url = {http://www.diva-portal.org/smash/get/diva2:831173/FULLTEXT01.pdf}
}
@article{Heide1999,
annote = {General notes:
Review of different eye tracking techniques. From 1999.

EOG:
Used most in clinical research
Pros:
- Non intrusive
- Does not limit field of view
- Can be used while the subject is wearing glasses/lenses
- Can be used with non cooperative subject
- Can be used while eye is closed ( E.G while asleep)

Cons:
- Sensitive to changes in light
- Often contaminated by electrical noise + blinks and eye lids.
- Horisontal range of +-40 deg
- Resolution of about 1-2 deg
- Vertical eye movements are often unreliable and require special configurations
- Torsinal eye movemens (around the line of sight) are not possible

VOG:
Pros:
- Non invasive
- 3D as well.
- Range of +- 40 deg horisontal and +- 30 deg vertical
- spatial resolution of about 0.5 deg

Cons:
Not really any that are relevant anymore},
author = {Heide, W and Koenig, E and Trillenberg, P and K{\"{o}}mpf, D and Zee, D S},
issn = {0424-8155},
journal = {Electroencephalography and clinical neurophysiology. Supplement},
pages = {223--240},
pmid = {10590990},
title = {{Electrooculography: technical standards and applications. The International Federation of Clinical Neurophysiology.}},
volume = {52},
year = {1999}
}
@article{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
isbn = {9781450341448},
issn = {09505849},
journal = {International Conference on Learning Representations (ICRL)},
month = {sep},
pages = {1--14},
pmid = {16873662},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2015}
}
@article{Baltrusaitis2017a,
abstract = {Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.},
archivePrefix = {arXiv},
arxivId = {1705.09406},
author = {Baltru{\v{s}}aitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
doi = {10.1109/TPAMI.2018.2798607},
eprint = {1705.09406},
issn = {0162-8828},
number = {c},
pages = {1--20},
title = {{Multimodal Machine Learning: A Survey and Taxonomy}},
url = {http://arxiv.org/abs/1705.09406},
volume = {8828},
year = {2017}
}
