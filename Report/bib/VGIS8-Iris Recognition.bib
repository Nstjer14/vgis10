Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Rifaee2017,
author = {Rifaee, Mustafa and Abdallah, Mohammad and Okosh, Basem},
file = {::},
journal = {international journal of multimedia {\&} its applications},
number = {April},
title = {{A Short Survey for Iris Images Databases}},
url = {https://www.researchgate.net/publication/316093004{\_}A{\_}Short{\_}Survey{\_}for{\_}Iris{\_}Images{\_}Databases},
year = {2017}
}
@article{Soviany2016,
abstract = {– This paper presents a design approach of a reliable authentication system for mobile applications (such as those within m-Health or m-Banking areas). This means that the biometric data processing should optimize the security performance vs. the computational complexity. The security is given by the combination of fingerprint, iris and voice features that define the multimodal pattern of an individual. The complexity reduction is supported by a reduced feature space, especially for the fingerprint and iris recognition components of the overall system.},
author = {Soviany, Sorin and Săndulescu, Virginia and Puşcoci, Sorin},
file = {::},
isbn = {9781509020478},
journal = {Computers and Artificial Intelligence},
keywords = {-multimodal,biometrics,data fusion,identification},
title = {{A Multimodal Biometric Identification Method for Mobile Applications Security}},
volume = {30},
year = {2016}
}
@article{Bazrafkan2017,
abstract = {With the increasing imaging and processing capabilities of today's mobile devices, user authentication using iris biometrics has become feasible. However, as the acquisition conditions become more unconstrained and as image quality is typically lower than dedicated iris acquisition systems, the accurate segmentation of iris regions is crucial for these devices. In this work, an end to end Fully Convolutional Deep Neural Network (FCDNN) design is proposed to perform the iris segmentation task for lower-quality iris images. The network design process is explained in detail, and the resulting network is trained and tuned using several large public iris datasets. A set of methods to generate and augment suitable lower quality iris images from the high-quality public databases are provided. The network is trained on Near InfraRed (NIR) images initially and later tuned on additional datasets derived from visible images. Comprehensive inter-database comparisons are provided together with results from a selection of experiments detailing the effects of different tunings of the network. Finally, the proposed model is compared with SegNet-basic, and a near-optimal tuning of the network is compared to a selection of other state-of-art iris segmentation algorithms. The results show very promising performance from the optimized Deep Neural Networks design when compared with state-of-art techniques applied to the same lower quality datasets.},
archivePrefix = {arXiv},
arxivId = {1712.02877},
author = {Bazrafkan, Shabab and Thavalengal, Shejin and Corcoran, Peter},
eprint = {1712.02877},
file = {::},
month = {dec},
title = {{An End to End Deep Neural Network for Iris Segmentation in Unconstraint Scenarios}},
url = {http://arxiv.org/abs/1712.02877},
year = {2017}
}
@article{Jesus2017,
author = {Jesus, Rosales Banderas Jose De and Maximo, Lopez Sanchez and Raul, Pinto Elias and Gabriel, Gonzalez Serna},
doi = {10.1109/CSCI.2016.0167},
file = {::},
isbn = {9781509055104},
journal = {Proceedings - 2016 International Conference on Computational Science and Computational Intelligence, CSCI 2016},
keywords = {color calibration,image processing,image stabilizer,iris detection},
pages = {861--864},
title = {{Methodology for Iris Scanning through Smartphones}},
year = {2017}
}
@incollection{Bowyer2016,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bowyer, Kevin W and Hollingsworth, Karen P and Flynn, Patrick J},
booktitle = {Handbook of Iris Recognition},
doi = {10.1007/978-1-4471-6784-6},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {978-1-4471-6782-2},
issn = {16130073},
pages = {23--61},
pmid = {25246403},
title = {{Chapter 2 A Survey of Iris Biometrics Research: 2008–2010}},
url = {http://link.springer.com/10.1007/978-1-4471-6784-6},
year = {2016}
}
@article{Al-Waisy2017,
author = {Al-Waisy, Alaa S. and Qahwaji, Rami and Ipson, Stanley and Al-Fahdawi, Shumoos and Nagem, Tarek A.M.},
doi = {10.1007/s10044-017-0656-1},
file = {::},
isbn = {0123456789},
issn = {14337541},
journal = {Pattern Analysis and Applications},
keywords = {AdaGrad method,Convolutional Neural Network,Deep learning,Iris recognition,Multimodal biometric systems,Softmax classifier},
number = {0123456789},
pages = {1--20},
publisher = {Springer London},
title = {{A multi-biometric iris recognition system based on a deep learning approach}},
url = {https://doi.org/10.1007/s10044-017-0656-1},
year = {2017}
}
@article{Rattani2017,
abstract = {Ocular biometrics encompasses the imaging and use of characteristic features extracted from the eyes for personal recognition. Ocular biometric modalities in visible light have mainly focused on iris, blood vessel structures over the white of the eye (mostly due to conjunctival and episcleral layers), and periocular region around eye. Most of the existing studies on iris recognition use the near infrared spectrum. However, conjunctival vasculature and periocular regions are imaged in the visible spectrum. Iris recognition in the visible spectrum is possible for light color irides or by utilizing special illumination. Ocular recognition in the visible spectrum is an important research area due to factors such as recognition at a distance, suitability for recognition with regular RGB cameras, and adaptability to mobile devices. Further these ocular modalities can be obtained from a single RGB eye image, and then fused together for enhanced performance of the system. Despite these advantages, the state-of-the-art related to ocular biometrics in visible spectrum is not well known. This paper surveys this topic in terms of computational image enhancement, feature extraction, classification schemes and designed hardware-based acquisition set-ups. Future research directions are also enumerated to identify the path forward.},
annote = {A very detalied survey article. They go through a lot of different approaches to iris recognition. They also talk about the other biometrics that can be extracted from a VIS (Visible Spectrum) image like, moles/freckles/nevi. Other patterns can also b extraxted such as conjunctival vaslulature, periocular and retinal biometrics.

UBIRIS is one of the most used publicaly available databses for VIS iris images with noise.},
author = {Rattani, Ajita and Derakhshani, Reza},
doi = {10.1016/j.imavis.2016.11.019},
file = {::},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Biometrics,Conjunctival vasculature,Iris,Mobile biometrics,Ocular biometrics,Periocular biometrics,Visible spectrum},
pages = {1--16},
publisher = {Elsevier B.V.},
title = {{Ocular biometrics in the visible spectrum: A survey}},
url = {http://dx.doi.org/10.1016/j.imavis.2016.11.019},
volume = {59},
year = {2017}
}
@inproceedings{Zhao2015a,
abstract = {This paper proposes a novel and more accurate iris segmentation framework to automatically segment iris region from the face images acquired with relaxed imaging under visible or near-infrared illumination, which provides strong feasibility for applications in surveillance, forensics and the search for missing children, etc. The proposed framework is built on a novel total-variation based formulation which uses l1 norm regularization to robustly suppress noisy texture pixels for the accurate iris localization. A series of novel and robust post processing operations are introduced to more accurately localize the limbic boundaries. Our experimental results on three publicly available databases, i.e., FRGC, UBIRIS.v2 and CASIA.v4-distance, achieve significant performance improvement in terms of iris segmentation accuracy over the state-of-the-art approaches in the literature. Besides, we have shown that using iris masks generated from the proposed approach helps to improve iris recognition performance as well. Unlike prior work, all the implementations in this paper are made publicly available to further advance research and applications in biometrics at-d-distance.},
author = {Zhao, Zijing and Kumar, Ajay},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.436},
file = {::;::},
isbn = {9781467383912},
issn = {15505499},
month = {dec},
pages = {3828--3836},
publisher = {IEEE},
title = {{An accurate iris segmentation framework under relaxed imaging constraints using total variation model}},
url = {http://ieeexplore.ieee.org/document/7410793/},
volume = {2015 Inter},
year = {2015}
}
@article{Saha2017a,
annote = {Not a very good article. but it has some nice references.

Keypoints

It is possible to aquire iris images in multiple way including Near Infrared (NIRD). a simple lense and monochome CCD camera, Adaboost cascade iris detector.

Iris localization is done by Daugman using a 2D Gabor Filter and Fisher Linear Discriminate method.

To help localize the iris despite of eyelashes a 1D rankfilter and histogram filter can be used.},
author = {Saha, Rishmita and Kundu, Mahasweta and Dutta, Madhuparna and Majumder, Rahul and Mukherjee, Debosmita and Pramanik, Sayak and Thakur, Uttam Narendra and Mukherjee, Chiradeep},
file = {::},
isbn = {9781538633717},
journal = {Information Technology, Electronics and Mobile Communication Conference (IEMCON), 2017 8th IEEE Annual},
pages = {685--688},
title = {{A Brief Study on Evolution of Iris Recognition System}},
url = {http://ieeexplore.ieee.org.ezproxy.psu.edu.sa/stamp/stamp.jsp?arnumber=8117234},
year = {2017}
}
@incollection{Bowyer2016b,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
annote = {Read only chapter 17 about iris and face fusion. The article gives a nice and clean overview of different multi-biometric systems as well as levels of data abstraction the data fusion can be applied on. 

The work presented performs iris and face fusion using multi-sample, multi instance, and multimodal data. it fuses the multi sample, and multi instance together, by simply finding the best match in the variations. this is done as a score level fusion. and it fuses the two modalities by rank-level fusion using Broda count.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Connaughton, Ryan and Bowyer, Kevin W and Flynn, Patrick J},
booktitle = {Handbook of Iris Recognition},
doi = {10.1007/978-1-4471-6784-6},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {978-1-4471-6782-2},
issn = {16130073},
pages = {397--415},
pmid = {25246403},
title = {{Chapter 17 Fusion of Face and Iris Biometrics}},
url = {http://link.springer.com/10.1007/978-1-4471-6784-6},
year = {2016}
}
@article{Zhao2017a,
annote = {Good article describing a lot of the state of the art deep learning approaches being researched.

Keynotes:

Not much work with deep learning has been done in iris recognition.

THey Use a Fully Convolutional Network (FCN) and talk about others who ahve used a Convolutional Neural Network (CNN). They also mention a Deep Belief Net (DBN) that others have used. DeepIrisNet is also mentioned and tested with

THey have created their own loss function optimized for iris recognition called Extended Triplet Loss (ETL)

Their network is generalizable to other databases meaning that it doesn't require finetuing as many others do.

Used ND-IRIS, CASIA Iris, IITD Iris and WVU Non-Ideal Iris databases to test on.},
author = {Zhao, Zijing and Kumar, Ajay},
doi = {10.1109/ICCV.2017.411},
file = {::},
isbn = {978-1-5386-1032-9},
journal = {2017 IEEE International Conference on Computer Vision (ICCV)},
pages = {3829--3838},
title = {{Towards More Accurate Iris Recognition Using Deeply Learned Spatially Corresponding Features}},
url = {http://ieeexplore.ieee.org/document/8237673/},
year = {2017}
}
@article{Hqwhua,
author = {Hqwhu, E and Vndudkdq, Qvndudnrf and Jwx, Dnjxo and Wu, H G X},
file = {::},
keywords = {convolutional neural,deep learning,network,pupil center estimation},
title = {{Deep learning based estimation of the eye pupil center by using image patch classification}}
}
@article{Percy,
abstract = {Iris recognition system is a reliable and an accurate biometric system. Localization of the iris borders in an eye image can be considered as a vital step in the iris recognition process. There exist many algorithms to segment the iris. One of the segmentation methods, that is used in many commercial iris biometric systems is an algorithm known as a Daugman's algorithm. The aim of this thesis is to implement this algorithm using MATLAB programming environment. The implemented algorithm was tested on the eye images of different quality, such as the images with partly covered iris or low contrast images. The test results demonstrated that the Daugman's algorithm detects the iris borders in the high quality images with high accuracy. The performance of the algorithm on the lower quality images has been improved by additional preprocessing of these images.},
author = {Percy, Oad and Waqas, Ahmad},
file = {::},
journal = {Blekinge Institute of Technology},
pages = {1--48},
title = {{Iris localization using Daugman ' s algorithm}},
url = {http://www.diva-portal.org/smash/get/diva2:831173/FULLTEXT01.pdf},
year = {2012}
}
@misc{LiborMasek2003,
annote = {Open Source Matlab Iris Recognition system. Based on Daugmans approach.},
author = {{Libor Masek}, Peter Kovesi},
publisher = {The School of Computer Science and Software Engineering, The University of Western Australia},
title = {{MATLAB Source Code for a Biometric Identification System Based on Iris Patterns}},
url = {http://www.peterkovesi.com/studentprojects/libor/sourcecode.html},
year = {2003}
}
@article{Gulmire2012,
author = {Gulmire, Kshamaraj and Ganorkar, Sanjay},
file = {::},
issn = {2278-0181},
number = {5},
pages = {1--5},
title = {{Iris Recognition Using Gabor Wavelet}},
volume = {1},
year = {2012}
}
@article{Arsalan2017,
author = {Arsalan, Muhammad and Hong, Hyung Gil and Naqvi, Rizwan Ali and Lee, Min Beom and Kim, Min Cheol and Kim, Dong Seop and Kim, Chan Sik and Park, Kang Ryoung},
doi = {10.3390/sym9110263},
file = {::},
issn = {20738994},
journal = {Symmetry},
keywords = {Biometrics,Convolutional neural network (CNN),Iris recognition,Iris segmentation},
number = {11},
title = {{Deep learning-based iris segmentation for iris recognition in visible light environment}},
volume = {9},
year = {2017}
}
@article{Zhang2016a,
author = {Zhang, Man and Zhang, Qi and Sun, Zhenan and Zhou, Shujuan and Ahmed, Nasir Uddin},
doi = {10.1109/BTAS.2016.7791191},
file = {::},
isbn = {9781467397339},
journal = {2016 IEEE 8th International Conference on Biometrics Theory, Applications and Systems, BTAS 2016},
title = {{The BTAS∗Competition on Mobile Iris Recognition}},
year = {2016}
}
@article{Kim2016,
abstract = {The iris recognition on a mobile phone is different from the conventional iris recognition implemented on a dedicated device in that the computational power of a mobile phone and the space for placing NIR (near infrared) LED (light emitting diode) illuminators and iris camera are limited. This paper raises these issues in detail based on real implementation of an iris recognition system in a mobile phone and proposes some solutions to these issues. An experimental study was conducted to search for the relevant power and wavelength of NIR LED illuminators with their positioning on a phone for capturing a good quality iris image. Subsequently, in view of the disparity between the user's gazing point and the center of the iris camera which causes degradation of acquired iris images, an experiment was performed to locate the appropriate gazing point for good iris image capture. A fast eye detection algorithm was proposed for implementation under the mobile platform with low computational facility. The experiments were conducted on a currently released mobile phone and the results showed promising potential for adoption of iris recognition as a reliable authentication means. As a result, two 850 nm LEDs were selected for iris illumination at 1.1 cm away from the iris camera for the size of a 7 cm × 13.7 cm phone. In the performance, the recognition accuracy was 0.1{\%} EER (equal error rate) and the eye detection rate with the speed of 17.64 ms on a mobile phone was 99.4{\%}.},
annote = {Keynotes:
Contributes with a good NIR mobile algorithm that's fast on mobile phones. They also contribute with an mobile iris database of 500 images that they are willing to share for research. Could not find it online.},
author = {Kim, Dongik and Jung, Yujin and Toh, Kar Ann and Son, Byungjun and Kim, Jaihie},
doi = {10.1016/j.eswa.2016.01.050},
file = {::},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Handheld,Iris recognition,Mobile,Portable,Smartphone},
pages = {328--339},
publisher = {Elsevier Ltd},
title = {{An empirical study on iris recognition in a mobile phone}},
url = {http://dx.doi.org/10.1016/j.eswa.2016.01.050},
volume = {54},
year = {2016}
}
@article{Luhadiya2017a,
annote = {Really good article with a summery of Iris recognition history and approaches in the introduction part.


Keynotes:

Iris database called CASIA with 756 images of 108 people.

SVM used to classify irises.

Elman Recurrent Neural Netowrk used.},
author = {Luhadiya, Ruchi and Khedkar, Anagha},
doi = {10.1109/ICAECCT.2016.7942619},
file = {::},
isbn = {9781509036622},
journal = {2016 IEEE International Conference on Advances in Electronics, Communication and Computer Technology, ICAECCT 2016},
keywords = {GLCM,Hough circular transform,Iris,Machine learning,Person identification,SVM},
pages = {387--392},
title = {{Iris detection for person identification using multiclass SVM}},
year = {2017}
}
@article{Daugman2009a,
abstract = {This chapter explains the iris recognition algorithms and presents results of 9.1 million comparisons among eye images from trials in Britain, the USA, Japan, and Korea. The key to iris recognition is the failure of a test of statistical independence, which involves so many degrees-of-freedom that this test is virtually guaranteed to be passed whenever the phase codes for two different eyes are compared, but to be uniquely failed when any eye's phase code is compared with another version of itself. The test of statistical independence is implemented by the simple Boolean Exclusive-OR operator (XOR) applied to the 2048 bit phase vectors that encode any two iris patterns, masked (AND'ed) by both of their corresponding mask bit vectors to prevent non iris artifacts from influencing iris comparisons. The XOR operator detects disagreement between any corresponding pair of bits, while the AND operator ensures that the compared bits are both deemed to have been uncorrupted by eyelashes, eyelids, specular reflections, or other noise. The norms of the resultant bit vector and of theAND'ed mask vectors are then measured in order to compute a fractional Hamming Distance as the measure of the dissimilarity between any two irises, whose two phase code bit vectors are denoted {\{}. codeA, codeB{\}} and whose mask bit vectors are denoted {\{}. maskA, maskB{\}}. {\textcopyright} 2009 Elsevier Inc. All rights reserved.},
annote = {A chapter about how Iris Recognitions works in general. John Daugman is the creator of IrisCode, a 2D Gabor wavelet-based iris recognition algorithm that is the basis of all publicly deployed automatic iris recognition systems and which has registered more than a billion persons worldwide in government ID programs.

Keywords/phrases:
Near Infra Red (NIR) images can be used for iris capturing. 

Gabor wavelets are used for determining the inter and outer edges of an iris.

Often the iris will not be circular because an eyelid will cover it.

Hamming distance is used when comparing two irises.},
author = {Daugman, John},
doi = {10.1016/B978-0-12-374457-9.00025-1},
file = {::},
isbn = {9780123744579},
issn = {10518215},
journal = {The Essential Guide to Image Processing},
pages = {715--739},
pmid = {20810146},
title = {{How Iris Recognition Works}},
year = {2009}
}
@article{Arslan2017a,
abstract = {Biometric systems may be used to create a remote access model on devices, ensure personal data protection, personalize and facilitate the access security. Biometric systems are generally used to increase the security level in addition to the previous authentication methods and they seen as a good solution. Biometry occupies an important place between the areas of daily life of the machine learning. In this study; the techniques, methods, technologies used in biometric systems are researched, machine learning techniques used biometric aplications are investigated for the security perspective, the advantages and disadvantages that these tecniques provide are given. The studies in the literature between 2010-2016 years, used algorithms, technologies, metrics, usage areas, the machine learning techniques used for different biometric systems such as face, palm prints, iris, voice, fingerprint recognition are researched and the studies made are evaluated. The level of security provided by the use of biometric systems by developed using machine learning and disadvantages that arise in the use of these systems are stated in detail in the study. Also, impact on people of biometric methods in terms of ease of use, security and usages areas are examined.},
author = {Arslan, B. and Yorulmaz, E. and Akca, B. and Sagiroglu, S.},
doi = {10.1109/ICMLA.2016.183},
file = {::},
isbn = {9781509061662},
journal = {Proceedings - 2016 15th IEEE International Conference on Machine Learning and Applications, ICMLA 2016},
keywords = {Biometric,Face,Fingerprint,Iris,Machine learning,Recognition,Security,Teeth,Voice},
pages = {492--497},
title = {{Security perspective of Biometric recognition and machine learning techniques}},
year = {2017}
}
@article{Proenca2017a,
abstract = {The effectiveness of current iris recognition systems de-pends on the accurate segmentation and parameterisation of the iris boundaries, as failures at this point misalign the coefficients of the biometric signatures. This paper de-scribes IRINA, an algorithm for Iris Recognition that is ro-bust against INAccurately segmented samples, which makes it a good candidate to work in poor-quality data. The pro-cess is based in the concept of " corresponding " patch be-tween pairs of images, that is used to estimate the posterior probabilities that patches regard the same biological region, even in case of segmentation errors and non-linear texture deformations. Such information enables to infer a free-form deformation field (2D registration vectors) between images, whose first and second-order statistics provide effective bio-metric discriminating power. Extensive experiments were carried out in four datasets (CASIA-IrisV3-Lamp, CASIA-IrisV4-Lamp, CASIA-IrisV4-Thousand and WVU) and show that IRINA not only achieves state-of-the-art performance in good quality data, but also handles effectively severe seg-mentation errors and large differences in pupillary dilation / constriction.},
author = {Proenca, Hugo and Neves, Joao C.},
doi = {10.1109/CVPR.2017.714},
file = {::},
isbn = {978-1-5386-0457-1},
journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {6747--6756},
title = {{IRINA: Iris Recognition (Even) in Inaccurately Segmented Data}},
url = {http://ieeexplore.ieee.org/document/8100197/},
year = {2017}
}
@article{Galdi2017a,
author = {Galdi, Chiara and Dugelay, Jean Luc},
doi = {10.1109/ICPR.2016.7899626},
file = {::},
isbn = {9781509048472},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
pages = {160--164},
title = {{Fusing iris colour and texture information for fast iris recognition on mobile devices}},
year = {2017}
}
@article{Neves2017a,
abstract = {{\textcopyright} 2017 IEEE. An error-correcting code (ECC) is a process of adding redundant data to a message, such that it can be recovered by a receiver even if a number of errors are introduced in transmission. Inspired by the principles of ECC, we introduce a method capable of detecting degraded features in biometric signatures by exploiting feature correlation. The main novelty is that, unlike existing biometric cryptosystems, the proposed method works directly on the biometric signature. Our approach performs a redundancy analysis of non-degraded data to build an undirected graphical model (Markov Random Field), whose energy minimization determines the sequence of degraded components of the biometric sample. Experiments carried out in different biometric traits ascertain the improvements attained when disregarding degraded features during the matching phase. Also, we stress that the proposed method is general enough to work in different classification methods, such as CNNs.},
author = {Neves, Joao and Proenca, Hugo},
doi = {10.1109/FG.2017.122},
file = {::},
isbn = {9781509040230},
journal = {Proceedings - 12th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2017 - 1st International Workshop on Adaptive Shot Learning for Gesture Understanding and Production, ASL4GUP 2017, Biometrics in the Wild, Bwild 2017, Heteroge},
pages = {981--986},
title = {{Exploiting Data Redundancy for Error Detection in Degraded Biometric Signatures Resulting from in the Wild Environments}},
year = {2017}
}
@article{Abate2017,
abstract = {The increasing popularity of smartphones amongst the population laid the basis for a wide range of applications aimed at security and privacy protection. Very modern mobile devices have recently demonstrated the feasibility of using a camera sensor to access the system without typing any alphanumerical password. In this work, we present a method that implements iris recognition in the visible spectrum through unsupervised learning by means of Self Organizing Maps (SOM). The proposed method uses a SOM network to cluster iris features at pixel level. The discriminative feature map is obtained by using RGB data of the iris combined with the statistical descriptors of kurtosis and skewness. An experimental analysis on MICHE-I and UBIRISv1 datasets demonstrates the strengths and weaknesses of the algorithm, which has been specifically designed to require low processing power in compliance with the limited capability of common mobile devices.},
author = {Abate, Andrea F. and Barra, Silvio and Gallo, Luigi and Narducci, Fabio},
doi = {10.1016/j.patrec.2017.02.002},
file = {::},
isbn = {0000000000},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Iris recognition,Mobile biometric recognition,Statistical descriptors,Unsupervised learning},
pages = {37--43},
pmid = {11341202},
publisher = {Elsevier B.V.},
title = {{Kurtosis and skewness at pixel level as input for SOM networks to iris recognition on mobile devices}},
volume = {91},
year = {2017}
}
@article{Kuehlkamp2016a,
abstract = {Iris recognition systems are a mature technology that is widely used throughout the world. In identification (as opposed to verification) mode, an iris to be recognized is typically matched against all N enrolled irises. This is the classic "1-to-N search". In order to improve the speed of large-scale identification, a modified "1-to-First" search has been used in some operational systems. A 1-to-First search terminates with the first below-threshold match that is found, whereas a 1-to-N search always finds the best match across all enrollments. We know of no previous studies that evaluate how the accuracy of 1-to-First search differs from that of 1-to-N search. Using a dataset of over 50,000 iris images from 2,800 different irises, we perform experiments to evaluate the relative accuracy of 1-to-First and 1-to-N search. We evaluate how the accuracy difference changes with larger numbers of enrolled irises, and with larger ranges of rotational difference allowed between iris images. We find that False Match error rate for 1-to-First is higher than for 1-to-N, and the the difference grows with larger number of enrolled irises and with larger range of rotation.},
annote = {The way that iris recognition works is that some kind of filter is applied to localize the iris. 2D Gador filter is often cited. Then that iris is checked against the whole database. This is called 1:N. They check the Hamming Distance between of the bits of the data and then choose the lowest ones as a pair. In 1:First search they do the same except there is is a threshold that that is has to be under to be accepted. If it is under that threshold it is accepted and the search is stopped. Two types of error can occour: a False Match (FM) and False Non-Match (FNM). A false match occurs when two samples from different individuals are declared by the system as a match. A false non-match is when two samples from the same individual fail to be considered as a match by the system},
archivePrefix = {arXiv},
arxivId = {1702.01167},
author = {Kuehlkamp, Andrey and Bowyer, Kevin W.},
doi = {10.1109/WACV.2016.7477687},
eprint = {1702.01167},
file = {::},
isbn = {9781509006410},
journal = {2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016},
title = {{An analysis of 1-to-first matching in iris recognition}},
year = {2016}
}
@inproceedings{Trokielewicz2016,
author = {Trokielewicz, Mateusz},
booktitle = {2016 IEEE International Conference on Identity, Security and Behavior Analysis (ISBA)},
doi = {10.1109/ISBA.2016.7477233},
isbn = {978-1-4673-9727-8},
month = {feb},
pages = {1--6},
publisher = {IEEE},
title = {{Iris recognition with a database of iris images obtained in visible light using smartphone camera}},
url = {http://ieeexplore.ieee.org/document/7477233/},
year = {2016}
}
@article{Daugman1993,
author = {Daugman, J.G.},
doi = {10.1109/34.244676},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {11},
pages = {1148--1161},
title = {{High confidence visual recognition of persons by a test of statistical independence}},
url = {http://ieeexplore.ieee.org/document/244676/},
volume = {15},
year = {1993}
}
@article{Karpathy2016a,
abstract = {These notes accompany the Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition.},
author = {Karpathy, Andrej},
journal = {Stanford University},
pages = {1--21},
title = {{Convolutional Neural Networks for Visual Recognition}},
url = {http://cs231n.github.io/classification/},
year = {2016}
}
@article{Ribeiro2017a,
author = {Ribeiro, Eduardo and Uhl, Andreas and Alonso-Fernandez, Fernando and Farrugia, Reuben A.},
doi = {10.23919/EUSIPCO.2017.8081595},
file = {::},
isbn = {978-0-9928626-7-1},
journal = {2017 25th European Signal Processing Conference (EUSIPCO)},
pages = {2176--2180},
title = {{Exploring deep learning image super-resolution for iris recognition}},
url = {http://ieeexplore.ieee.org/document/8081595/},
volume = {2},
year = {2017}
}
@article{Uka2017a,
annote = {Keynotes:

CASIA (Most used database) and IIT Delhi Iris Database used.

Hough Transofrm algorithm used to detect boundaries},
author = {Uka, Arban and Ro{\c{c}}i, Albana and Ko{\c{c}}, Oktay},
file = {::},
isbn = {9781509038435},
journal = {IEEE EUROCON 2017 -17th International Conference on Smart Technologies},
keywords = {encoding,equal error rate,segmentation},
number = {July},
pages = {6--8},
title = {{Improved Segmentation Algorithm and Further Optimization for Iris Recognition}},
year = {2017}
}
@article{Nielsen2015,
abstract = {Neural Networks and Deep Learning is a free online book. The book will teach you about: Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data Deep learning, a powerful set of techniques for learning in neural networks Neural networks and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing. This book will teach you many of the core concepts behind neural networks and deep learning.},
author = {Nielsen, Michael},
journal = {Determination Press},
title = {{Neural Networks and Deep Learning}},
url = {http://neuralnetworksanddeeplearning.com/index.html},
year = {2015}
}
@article{Elrefaei2017,
author = {Elrefaei, Lamiaa A. and Hamid, Doaa H. and Bayazed, Afnan A. and Bushnak, Sara S. and Maasher, Shaikhah Y.},
doi = {10.1007/s11042-017-5049-3},
file = {::},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {Biometrics,Deep Sparse Filter,Hough transform,Iris recognition,Visible Light},
pages = {1--25},
publisher = {Multimedia Tools and Applications},
title = {{Developing Iris Recognition System for Smartphone Security}},
year = {2017}
}
@article{B2017,
author = {B, Qi Wang and Su, Xia and Cai, Zhenlin and Zhang, Xiangde},
doi = {10.1007/978-3-319-69923-3},
file = {::},
isbn = {978-3-319-69922-6},
keywords = {joint bayesian,mobile iris recognition,ordinal measures},
number = {3},
pages = {401--410},
title = {{Biometric Recognition}},
url = {http://link.springer.com/10.1007/978-3-319-69923-3},
volume = {10568},
year = {2017}
}
@article{Khan2017a,
annote = {A good article that uses a new database containing smartphone iris images. Theses are taken in visible light. They use Daugmans approach to localize the iris, then they normalize it. They use wavelets on the image to extract the desired featues and then try to classify them using SVM (97{\%}), KNN (95.1{\%}) and LDA (94.28{\%}).

Keynotes:

A different study has used Sparse Reconstruction Classifier with K-means clustering

A different study obtained 99{\%} accuracy using SVM and Hamming Distance

They tried SVM, K-means, Linear Discrimintant in their own study as classifiers.},
author = {Khan, Fahim Faysal and Akif, Ahnaf and Haque, M A},
file = {::},
isbn = {9781538633748},
pages = {26--28},
title = {{Iris Recognition using Machine Learning from Smartphone Captured Images in Visible Light}},
year = {2017}
}
@article{DeMarsico2018,
abstract = {Mobile biometrics technologies are nowadays the new frontier for secure use of data and services, and are considered particularly important due to the massive use of handheld devices in the entire world. Among the biometric traits with potential to be used in mobile settings, the iris/ocular region is a natural candidate, even considering that further advances in the technology are required to meet the operational requirements of such ambitious environments. Aiming at promoting these advances, we organized the Mobile Iris Challenge Evaluation (MICHE)-I contest. This paper presents a comparison of the performance of the participant methods by various Figures of Merit (FoMs). A particular attention is devoted to the identification of the image covariates that are likely to cause a decrease in the performance levels of the compared algorithms. Among these factors, interoperability among different devices plays an important role. The methods (or parts of them) implemented by the analyzed approaches are classified into segmentation (S), which was the main target of MICHE-I, and recognition (R). The paper reports both the results observed for either S or R, and also for different recombinations (S+R) of such methods. Last but not least, we also present the results obtained by multi-classifier strategies.},
author = {{De Marsico}, Maria and Nappi, Michele and Narducci, Fabio and Proen{\c{c}}a, Hugo},
doi = {10.1016/j.patcog.2017.08.028},
file = {::},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Biometric algorithm fusion,Evaluation,Mobile Iris Recognition},
pages = {286--304},
publisher = {Elsevier Ltd},
title = {{Insights into the results of MICHE I - Mobile Iris CHallenge Evaluation}},
volume = {74},
year = {2018}
}
@article{Jung2017,
abstract = {Finding the accurate position of an eye is crucial for mobile iris recognition system in order to extract the iris region quickly and correctly. Unfortunately, this is very difficult to accomplish when a person is wearing eyeglasses because of the interference from the eyeglasses. This paper proposes an eye detection method that is robust to eyeglass interference in mobile environment. The proposed method comprises two stages: eye candidate generation and eye validation. In the eye candidate generation stage, multi-scale window masks consisting of 2 × 3 subblocks are used to generate all image blocks possibly containing an eye image. In the ensuing eye validation stage, two methods are employed to determine which blocks actually contain true eye images and locate their precise positions as well: the first method searches for the glint of an NIR illuminator on the pupil region. If this first method fails, the next method computes the intensity difference between the assumed pupil and its surrounding region using multi-scale 3 × 3 window masks. Experimental results show that the proposed method detects the eye position more accurately and quickly than competing methods in the presence of interference from eyeglass frames.},
author = {Jung, Yujin and Kim, Dongik and Son, Byungjun and Kim, Jaihie},
doi = {10.1016/j.eswa.2016.09.036},
file = {::},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Eye detection,Eye validation,Eyeglasses,Iris detection,Iris recognition,Mobile},
pages = {178--188},
publisher = {Elsevier Ltd},
title = {{An eye detection method robust to eyeglasses for mobile iris recognition}},
url = {http://dx.doi.org/10.1016/j.eswa.2016.09.036},
volume = {67},
year = {2017}
}
@article{Galdi2017,
abstract = {FIRE is a Fast Iris REcognition algorithm especially designed for iris recognition on mobile phones under visible-light. It is based on the combination of three classifiers exploiting the iris colour and texture information. Its limited computational time makes FIRE particularly suitable for fast user verification on mobile devices. The high parallelism of the code allows its use also on large databases. FIRE, in its first version, was submitted to the Mobile Iris CHallenge Evaluation part II held in 2016. In this paper, FIRE is further improved: a number of different techniques has been analyzed and the best performing ones have been selected for fusion at score level. Performance are assessed in terms of Recognition Rate (RR), Area Under Receiver Operating Characteristic Curve (AUC), and Equal Error Rate (EER).},
author = {Galdi, Chiara and Dugelay, Jean Luc},
doi = {10.1016/j.patrec.2017.01.023},
file = {::},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Fast iris recognition,MICHE DB,MICHE II,Multi-classifier,Noisy iris recognition,Visible light},
pages = {44--51},
publisher = {Elsevier B.V.},
title = {{FIRE: Fast Iris REcognition on mobile phones by combining colour and texture features}},
url = {http://dx.doi.org/10.1016/j.patrec.2017.01.023},
volume = {91},
year = {2017}
}
@article{Lee2017,
abstract = {In recent years, the iris recognition system has been gaining increasing acceptance for applications such as access control and smartphone security. When the images of the iris are obtained under unconstrained conditions, an issue of undermined quality is caused by optical and motion blur, off-angle view (the user's eyes looking somewhere else, not into the front of the camera), specular reflection (SR) and other factors. Such noisy iris images increase intra-individual variations and, as a result, reduce the accuracy of iris recognition. A typical iris recognition system requires a near-infrared (NIR) illuminator along with an NIR camera, which are larger and more expensive than fingerprint recognition equipment. Hence, many studies have proposed methods of using iris images captured by a visible light camera without the need for an additional illuminator. In this research, we propose a new recognition method for noisy iris and ocular images by using one iris and two periocular regions, based on three convolutional neural networks (CNNs). Experiments were conducted by using the noisy iris challenge evaluation-part II (NICE.II) training dataset (selected from the university of Beira iris (UBIRIS).v2 database), mobile iris challenge evaluation (MICHE) database, and institute of automation of Chinese academy of sciences (CASIA)-Iris-Distance database. As a result, the method proposed by this study outperformed previous methods.},
author = {Lee, Min Beom and Hong, Hyung Gil and Park, Kang Ryoung},
doi = {10.3390/s17122933},
file = {::},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Convolutional neural network,Iris and periocular,Noisy iris and ocular image},
number = {12},
pmid = {29258217},
title = {{Noisy ocular recognition based on three convolutional neural networks}},
volume = {17},
year = {2017}
}
