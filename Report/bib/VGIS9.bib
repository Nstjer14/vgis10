Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{tless,
abstract = {We introduce T-LESS, a new public dataset for estimating the 6D pose, i.e. translation and rotation, of texture-less rigid objects. The dataset features thirty industry-relevant objects with no significant texture and no discriminative color or reflectance properties. The objects exhibit symmetries and mutual similarities in shape and/or size. Compared to other datasets, a unique property is that some of the objects are parts of others. The dataset includes training and test images that were captured with three synchronized sensors, specifically a structured-light and a time-of-flight RGB-D sensor and a high-resolution RGB camera. There are approximately 39K training and 10K test images from each sensor. Additionally, two types of 3D models are provided for each object, i.e. a manually created CAD model and a semi-automatically reconstructed one. Training images depict individual objects against a black background. Test images originate from twenty test scenes having varying complexity, which increases from simple scenes with several isolated objects to very challenging ones with multiple instances of several objects and with a high amount of clutter and occlusion. The images were captured from a systematically sampled view sphere around the object/scene, and are annotated with accurate ground truth 6D poses of all modeled objects. Initial evaluation results indicate that the state of the art in 6D object pose estimation has ample room for improvement, especially in difficult cases with significant occlusion. The T-LESS dataset is available online at cmp.felk.cvut.cz/t-less.},
archivePrefix = {arXiv},
arxivId = {1701.05498},
author = {Hodaň, Tom{\'{a}}{\v{s}} and Haluza, Pavel and Obdrzalek, {\v{S}}t{\v{e}}p{\'{a}}n and Matas, Jiř{\'{i}} and Lourakis, Manolis and Zabulis, Xenophon},
booktitle = {Proceedings - 2017 IEEE Winter Conference on Applications of Computer Vision, WACV 2017},
doi = {10.1109/WACV.2017.103},
eprint = {1701.05498},
file = {::},
isbn = {9781509048229},
issn = {2472-6737},
pages = {880--888},
title = {{T-LESS: An RGB-D dataset for 6D pose estimation of texture-less objects}},
url = {http://cmp.felk.cvut.cz/{~}hodanto2/data/hodan2017tless.pdf},
year = {2017}
}
@article{Redmon2018,
abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
archivePrefix = {arXiv},
arxivId = {1804.02767},
author = {Redmon, Joseph and Farhadi, Ali},
eprint = {1804.02767},
file = {:C$\backslash$:/Users/Niclas/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon, Farhadi - 2018 - YOLOv3 An Incremental Improvement.pdf:pdf},
month = {apr},
title = {{YOLOv3: An Incremental Improvement}},
url = {http://arxiv.org/abs/1804.02767},
year = {2018}
}
@article{d2s,
abstract = {We introduce the Densely Segmented Supermarket (D2S) dataset, a novel benchmark for instance-aware semantic segmentation in an industrial domain. It contains 21 000 high-resolution images with pixel-wise labels of all object instances. The objects comprise groceries and everyday products from 60 categories. The benchmark is designed such that it resembles the real-world setting of an automatic checkout, inventory, or warehouse system. The training images only contain objects of a single class on a homogeneous background, while the validation and test sets are much more complex and diverse. To further benchmark the robustness of instance segmentation methods, the scenes are acquired with different lightings, rotations, and backgrounds. We ensure that there are no ambiguities in the labels and that every instance is labeled comprehensively. The annotations are pixel-precise and allow using crops of single instances for articial data augmentation. The dataset covers several challenges highly relevant in the field, such as a limited amount of training data and a high diversity in the test and validation sets. The evaluation of state-of-the-art object detection and instance segmentation methods on D2S reveals significant room for improvement.},
author = {Follmann, Patrick and B{\"{o}}ttger, Tobias and H{\"{a}}rtinger, Philipp and K{\"{o}}nig, Rebecca and Ulrich, Markus},
file = {::},
journal = {European Conference on Computer Vision (ECCV)},
keywords = {industrial application,instance segmentation,segmentation dataset},
pages = {569--585},
title = {{MVTec D2S: Densely Segmented Supermarket Dataset}},
url = {www.mvtec.com},
year = {2018}
}
@article{Dexnet3,
abstract = {Vacuum-based end effectors are widely used in industry and are often preferred over parallel-jaw and multifinger grippers due to their ability to lift objects with a single point of contact. Suction grasp planners often target planar surfaces on point clouds near the estimated centroid of an object. In this paper, we propose a compliant suction contact model that computes the quality of the seal between the suction cup and local target surface and a measure of the ability of the suction grasp to resist an external gravity wrench. To characterize grasps, we estimate robustness to perturbations in end-effector and object pose, material properties, and external wrenches. We analyze grasps across 1,500 3D object models to generate Dex-Net 3.0, a dataset of 2.8 million point clouds, suction grasps, and grasp robustness labels. We use Dex-Net 3.0 to train a Grasp Quality Convolutional Neural Network (GQ-CNN) to classify robust suction targets in point clouds containing a single object. We evaluate the resulting system in 350 physical trials on an ABB YuMi fitted with a pneumatic suction gripper. When evaluated on novel objects that we categorize as Basic (prismatic or cylindrical), Typical (more complex geometry), and Adversarial (with few available suction-grasp points) Dex-Net 3.0 achieves success rates of 98{\$}\backslash{\%}{\$}, 82{\$}\backslash{\%}{\$}, and 58{\$}\backslash{\%}{\$} respectively, improving to 81{\$}\backslash{\%}{\$} in the latter case when the training set includes only adversarial objects. Code, datasets, and supplemental material can be found at http://berkeleyautomation.github.io/dex-net .},
archivePrefix = {arXiv},
arxivId = {1709.06670},
author = {Mahler, Jeffrey and Matl, Matthew and Liu, Xinyu and Li, Albert and Gealy, David and Goldberg, Ken},
doi = {10.1109/ICRA.2018.8460887},
eprint = {1709.06670},
file = {::},
isbn = {978-1-5386-3081-5},
month = {sep},
title = {{Dex-Net 3.0: Computing Robust Robot Vacuum Suction Grasp Targets in Point Clouds using a New Analytic Model and Deep Learning}},
url = {http://arxiv.org/abs/1709.06670},
year = {2017}
}
@misc{Vincent2018,
abstract = {How British supermarket Ocado is using robots to make online grocery shopping faster},
author = {Vincent, James},
booktitle = {The verge},
title = {{WELCOME TO THE AUTOMATED WAREHOUSE OF THE FUTURE}},
url = {https://www.theverge.com/2018/5/8/17331250/automated-warehouses-jobs-ocado-andover-amazon},
urldate = {2019-01-04},
year = {2018}
}
@article{Mahler2017,
abstract = {To reduce data collection time for deep learning of robust robotic grasp plans, we explore training from a synthetic dataset of 6.7 million point clouds, grasps, and analytic grasp metrics generated from thousands of 3D models from Dex-Net 1.0 in randomized poses on a table. We use the resulting dataset, Dex-Net 2.0, to train a Grasp Quality Convolutional Neural Network (GQ-CNN) model that rapidly predicts the probability of success of grasps from depth images, where grasps are specified as the planar position, angle, and depth of a gripper relative to an RGB-D sensor. Experiments with over 1,000 trials on an ABB YuMi comparing grasp planning methods on singulated objects suggest that a GQ-CNN trained with only synthetic data from Dex-Net 2.0 can be used to plan grasps in 0.8sec with a success rate of 93{\%} on eight known objects with adversarial geometry and is 3x faster than registering point clouds to a precomputed dataset of objects and indexing grasps. The Dex-Net 2.0 grasp planner also has the highest success rate on a dataset of 10 novel rigid objects and achieves 99{\%} precision (one false positive out of 69 grasps classified as robust) on a dataset of 40 novel household objects, some of which are articulated or deformable. Code, datasets, videos, and supplementary material are available at http://berkeleyautomation.github.io/dex-net .},
archivePrefix = {arXiv},
arxivId = {1703.09312},
author = {Mahler, Jeffrey and Liang, Jacky and Niyaz, Sherdil and Laskey, Michael and Doan, Richard and Liu, Xinyu and Ojea, Juan Aparicio and Goldberg, Ken},
doi = {10.15607/RSS.2017.XIII.058},
eprint = {1703.09312},
file = {::},
isbn = {978-0-9923747-3-0},
issn = {0929-5593},
month = {mar},
pmid = {23641116},
title = {{Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics}},
url = {http://arxiv.org/abs/1703.09312},
year = {2017}
}
@techreport{Zhao,
abstract = {Due to object detection's close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles which combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy and optimization function, etc. In this paper, we provide a review on deep learning based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely Convolutional Neural Network (CNN). Then we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network based learning systems.},
archivePrefix = {arXiv},
arxivId = {1807.05511v1},
author = {Zhao, Zhong-Qiu and Zheng, Peng and Xu, Shou-Tao and Wu, Xindong},
eprint = {1807.05511v1},
file = {::},
keywords = {Index Terms-deep learning,neural network,object detection},
title = {{Object Detection with Deep Learning: A Review}},
url = {https://arxiv.org/pdf/1807.05511.pdf},
year = {2018}
}
@inproceedings{Gall2012,
abstract = {Object detection multi class detection Random forests object detection ...},
author = {Gall, Juergen and Razavi, Nima and {Van Gool}, Luc},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-34091-8_11},
isbn = {9783642340901},
issn = {03029743},
keywords = {Hough forest,multi-class object detection,random forest,regression forest},
pages = {243--263},
title = {{An introduction to random forests for multi-class object detection}},
url = {http://link.springer.com/10.1007/978-3-642-34091-8{\_}11},
volume = {7474 LNCS},
year = {2012}
}
@inproceedings{Tan2014,
abstract = {In this paper, we address the problem of object tracking in intensity images and depth data. We propose a generic framework that can be used either for tracking 2D templates in intensity images or for tracking 3D objects in depth images. To overcome problems like partial occlusions, strong illumination changes and motion blur, that notoriously make energy minimization-based tracking methods get trapped in a local minimum, we propose a learning-based method that is robust to all these problems. We use random forests to learn the relation between the parameters that defines the object's motion, and the changes they induce on the image intensities or the point cloud of the template. It follows that, to track the template when it moves, we use the changes on the image intensities or point cloud to predict the parameters of this motion. Our algorithm has an extremely fast tracking performance running at less than 2 ms per frame, and is robust to partial occlusions. Moreover, it demonstrates robustness to strong illumination changes when tracking templates using intensity images, and robustness in tracking 3D objects from arbitrary viewpoints even in the presence of motion blur that causes missing or erroneous data in depth images. Extensive experimental evaluation and comparison to the related approaches strongly demonstrates the benefits of our method.},
author = {Tan, David Joseph and Ilic, Slobodan},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.157},
isbn = {9781479951178},
issn = {10636919},
month = {jun},
pages = {1202--1209},
publisher = {IEEE},
title = {{Multi-forest tracker: A Chameleon in tracking}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909553},
year = {2014}
}
@article{Yilmaz2006,
abstract = {The goal of this article is to review the state-of-the-art tracking methods, classify them into different categories , and identify new trends. Object tracking, in general, is a challenging problem. Difficulties in tracking objects can arise due to abrupt object motion, changing appearance patterns of both the object and the scene, nonrigid object structures, object-to-object and object-to-scene occlusions, and camera motion. Tracking is usually performed in the context of higher-level applications that require the location and/or shape of the object in every frame. Typically, assumptions are made to constrain the tracking problem in the context of a particular application. In this survey, we categorize the tracking methods on the basis of the object and motion representations used, provide detailed descriptions of representative methods in each category, and examine their pros and cons. Moreover, we discuss the important issues related to tracking including the use of appropriate image features, selection of motion models, and detection of objects.},
author = {{Acm Reference Format: Yilmaz}, A and Javed, O and Shah, M},
doi = {10.1145/1177352.1177355},
file = {::},
journal = {ACM Comput. Surv},
keywords = {I48 [Image Processing and Computer Vision]: Scene,contour evolution,feature selection,object detection,object representation,point tracking,shape tracking},
pages = {45},
title = {{Object tracking: A survey}},
url = {http://doi.acm.org/10.1145/1177352.1177355},
volume = {38},
year = {2006}
}
@misc{Olsen2018,
abstract = {H{\o}je danske l{\o}nninger, global konkurrence og stigende krav fra forbrugerne tvinger danske netbutikker til hele tiden at optimere arbejdsgangene, hvis de ikke vil g{\aa} bagud af dansen. Flere ser det robotstyrede lager som en l{\o}sning, viser en analyse fra interesseorganisationen Foreningen for Dansk Internethandel. Proshop inviterede robotterne indenfor i begyndelsen af maj.},
address = {Horsens},
author = {Olsen, Line},
booktitle = {Horsens Folkeblad},
month = {nov},
title = {{Fuldautomatisk robotlager: Nu tager det syv minutter at pakke en pakke}},
url = {https://hsfo.dk/erhverv/Fuldautomatisk-robotlager-Nu-tager-det-syv-minutter-at-pakke-en-pakke/artikel/199254},
year = {2018}
}
@article{Leal-Taixe2017,
archivePrefix = {arXiv},
arxivId = {1704.02781},
author = {Leal-Taix{\'{e}}, Laura and Milan, Anton and Schindler, Konrad and Cremers, Daniel and Reid, Ian and Roth, Stefan},
eprint = {1704.02781},
file = {::},
month = {apr},
title = {{Tracking the Trackers: An Analysis of the State of the Art in Multiple Object Tracking}},
url = {https://arxiv.org/abs/1704.02781},
year = {2017}
}
@article{Redmon2015,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
archivePrefix = {arXiv},
arxivId = {1506.02640},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
eprint = {1506.02640},
file = {:C$\backslash$:/Users/Niclas/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon et al. - 2015 - You Only Look Once Unified, Real-Time Object Detection.pdf:pdf},
month = {jun},
title = {{You Only Look Once: Unified, Real-Time Object Detection}},
url = {http://arxiv.org/abs/1506.02640},
year = {2015}
}
@inproceedings{Or-El2015,
abstract = {The popularity of low-cost RGB-D scanners is increasing on a daily basis. Nevertheless, existing scanners often cannot capture subtle details in the environment. We present a novel method to enhance the depth map by fusing the intensity and depth information to create more detailed range profiles. The lighting model we use can handle natural scene illumination. It is integrated in a shape from shading like technique to improve the visual fidelity of the reconstructed object. Unlike previous efforts in this do- main, the detailed geometry is calculated directly, without the need to explicitly find and integrate surface normals. In addition, the proposed method operates four orders of magnitude faster than the state of the art. Qualitative and quantitative visual and statistical evidence support the im- provement in the depth obtained by the suggested method.},
author = {Or-El, Roy and Rosman, Guy and Wetzler, Aaron and Kimmel, Ron and Bruckstein, Alfred M.},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7299179},
isbn = {9781467369640},
issn = {10636919},
month = {jun},
pages = {5407--5416},
pmid = {7299179},
publisher = {IEEE},
title = {{RGBD-fusion: Real-time high precision depth recovery}},
url = {http://ieeexplore.ieee.org/document/7299179/},
volume = {07-12-June},
year = {2015}
}
@article{Xiang2017,
abstract = {Estimating the 6D pose of known objects is important for robots to interact with the real world. The problem is challenging due to the variety of objects as well as the complexity of a scene caused by clutter and occlusions between objects. In this work, we introduce PoseCNN, a new Convolutional Neural Network for 6D object pose estimation. PoseCNN estimates the 3D translation of an object by localizing its center in the image and predicting its distance from the camera. The 3D rotation of the object is estimated by regressing to a quaternion representation. We also introduce a novel loss function that enables PoseCNN to handle symmetric objects. In addition, we contribute a large scale video dataset for 6D object pose estimation named the YCB-Video dataset. Our dataset provides accurate 6D poses of 21 objects from the YCB dataset observed in 92 videos with 133,827 frames. We conduct extensive experiments on our YCB-Video dataset and the OccludedLINEMOD dataset to show that PoseCNN is highly robust to occlusions, can handle symmetric objects, and provide accurate pose estimation using only color images as input. When using depth data to further refine the poses, our approach achieves state-of-the-art results on the challenging OccludedLINEMOD dataset.},
archivePrefix = {arXiv},
arxivId = {1711.00199},
author = {Xiang, Yu and Schmidt, Tanner and Narayanan, Venkatraman and Fox, Dieter},
eprint = {1711.00199},
file = {::},
month = {nov},
title = {{PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes}},
url = {https://arxiv.org/abs/1711.00199},
year = {2017}
}
@article{Jund2016,
abstract = {With the increasing performance of machine learning techniques in the last few years, the computer vision and robotics communities have created a large number of datasets for benchmarking object recognition tasks. These datasets cover a large spectrum of natural images and object categories, making them not only useful as a testbed for comparing machine learning approaches, but also a great resource for bootstrapping different domain-specific perception and robotic systems. One such domain is domestic environments, where an autonomous robot has to recognize a large variety of everyday objects such as groceries. This is a challenging task due to the large variety of objects and products, and where there is great need for real-world training data that goes beyond product images available online. In this paper, we address this issue and present a dataset consisting of 5,000 images covering 25 different classes of groceries, with at least 97 images per class. We collected all images from real-world settings at different stores and apartments. In contrast to existing groceries datasets, our dataset includes a large variety of perspectives, lighting conditions, and degrees of clutter. Overall, our images contain thousands of different object instances. It is our hope that machine learning and robotics researchers find this dataset of use for training, testing, and bootstrapping their approaches. As a baseline classifier to facilitate comparison, we re-trained the CaffeNet architecture (an adaptation of the well-known AlexNet) on our dataset and achieved a mean accuracy of 78.9{\%}. We release this trained model along with the code and data splits we used in our experiments.},
archivePrefix = {arXiv},
arxivId = {1611.05799},
author = {Jund, Philipp and Abdo, Nichola and Eitel, Andreas and Burgard, Wolfram},
eprint = {1611.05799},
file = {::},
isbn = {3838361334},
month = {nov},
title = {{The Freiburg Groceries Dataset}},
url = {http://arxiv.org/abs/1611.05799},
year = {2016}
}
@article{Redmon2016,
abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
archivePrefix = {arXiv},
arxivId = {1612.08242},
author = {Redmon, Joseph and Farhadi, Ali},
eprint = {1612.08242},
file = {::},
month = {dec},
title = {{YOLO9000: Better, Faster, Stronger}},
url = {http://arxiv.org/abs/1612.08242},
year = {2016}
}
@misc{Perez2018,
author = {Perez, Sarah},
booktitle = {Tech Crunch},
pages = {0--1},
title = {{Walmart pilots a grocery-picking robot to fulfill customers' online orders}},
url = {https://techcrunch.com/2018/08/03/walmart-pilots-a-grocery-picking-robot-to-fulfill-customers-online-orders/?guccounter=1},
urldate = {2019-01-04},
year = {2018}
}
