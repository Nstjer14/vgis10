\section{Iris Recognition}
\label{sec:Iris_Recognition_Research}
Modern iris recognition was first introduced in an article by \cite{Daugman1993} discussing the security of using iris for identity recognition. Here an outline of how to do recognition was laid out and even though the field has been extensively improved since \cite{Daugman1993}, the general methodology is more or less the same solution as the one Daugman proposed. A modern system is typically composed of image acquisition, segmentation and normalisation, feature extraction and matching. In the following sections each module is briefly elaborated upon respectively.

\subsection{Image Acquistion}
The images acquired are often taken in the \gls{nir} spectrum, which is ranging between 700 nm and 900 nm in wavelength. In this band, the melanin in the iris, which is the substance that gives the iris its colour e.g. brown or blue, is typically less prominent making the unique structure in the iris very distinct. To acquire usable \gls{nir} images, the user has to be in the millimetre range of the \gls{nir} camera. In the visible light, the melanin is much more prominent and thus makes is harder to detect the structure. The band of visible light has many names in the literature, namely Visible Spectrum (VIS), Visible Wavelength (VW), and \gls{vl}. In this report \gls{vl} will be used. 

While \gls{nir} is beneficial in some cases, other useful features can be observed in the \gls{vl} that cannot be seen with \gls{nir}. These can include the moles, freckles and conjunctival vasculature, which can help in making more accurate recognition systems. To make iris recognitions systems comparable with each other some publicly available databases are often used. \cite{Rifaee2017}  give an outline of some of the free databases that are used. A comprehensive table summarising the visual properties, statistics and type of subject used in various databases are tabulated in \autoref{fig:Iris_database_1} and \autoref{fig:Iris_database_2}. Most of the databases available are \gls{nir} images with CASIA being one of the most used databases. For \gls{vl} images, UBIRIS is the most commonly used. This contains more realistic samples of real world data as the images contain noise in the form of eyelid obstruction, eyelash obstruction, glare, motion blur, out-of-focus or poor focused iris, partial iris and specular reflection \citep{Rattani2017}. The database has also been used in Noisy Iris Challenge Evaluation (NICE) I. The increasing usage of mobile devices also lays ground for an opportunity to integrate iris recognition as a biometric trait for verifying the identity of the user. For this purpose a competition, Mobile Iris CHallenge Evaluation (MICHE), is made to compare the state of the art mobile iris recognition. For this competition the MICHE database is provided, which is claimed to be a better database for mobile systems than UBIRIS \citep{DeMarsico2018}. The database contains noisy iris images taken with a Samsung Galaxy IV, iPhone 5, and Samsung Galaxy Tablet II. The noise includes noise from both artificial and natural light sources during acquisition, motion blur, occlusion due to eyelids, glasses, eyelashes, hair, or shadows, which can naturally occur when a user is trying to unlock a phone using their iris. 

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/Iris_Database_tabel_1.png} 
\caption{A table depicting the contents of freely available iris image databases  \citep{Rifaee2017}}
\label{fig:Iris_database_1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/Iris_Database_tabel_2.png} 
\caption{A table depicting the different kinds of noise present in the images in the various databases \citep{Rifaee2017}}
\label{fig:Iris_database_2}
\end{figure}


\subsection{Segmentation and Normalisation}
The segmentation of an iris in iris recognition systems detects the iris and finds the pupillary boundary and the limbus boundary as well as the eyelids and eyelashes that can cause noise on the image. The boundaries along with other parts of the iris are shown in \autoref{fig:iris_naming}. The approach used for segmentation depends, among other things, on the wavelength of the image; \gls{nir} or \gls{vl}. They both have some common challenges to them. Often the eyelids cover a small part of the iris, causing the limbus boundary of the iris to not be circular or elliptical. Eyelashes can also cause a similar disturbance as they sometimes cover parts of the iris. Likewise, poor lighting can make it extremely difficult to detect the boundaries. Specular reflections in the iris may also cause difficulties as they can lie on the iris boundary or close to. Most systems also require a great deal of user cooperation as an off angle iris, motion blur, or glasses or contact lenses can make it even more troublesome to detect the boundaries. This can especially be the case for iris recognition on a smart phone, as it cannot be expected that the user knows how to acquire a good iris image and the mobility allows the user to vary the acquisition angle unrestrictedly. 

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/iris_naming.png} 
\caption{A close up \gls{nir} image of an iris depicting the different parts of the iris along with their names \citep{Bowyer2016b}}
\label{fig:iris_naming}
\end{figure}

Two commonly observed approaches for segmentation in \gls{nir} band images are Daugman's approach and Hough Transform \citep{Daugman1993, Saha2017,Rattani2017,Khan2017a,Luhadiya2017,Uka2017}. Daugman's approach consists of using a Gaussian filter on the image to attenuate the effect of noise and eliminate undesired weak edges like the boundaries within the iris while keeping the strong edges like iris boundaries and eyelid boundaries. An integro-differential operator is then used as a circular edge detector. It is then used iteratively to find the pupillary boundary and the limbus boundary. Hough Transform on the other hand is a histogram based model fitting approach. An edge map of the input image is generated using a gradient-based edge detector, then a voting procedure is applied on the thresholded edge map to determine the parameters for a contour that best fits a circle. This operation gives an approximate edge map of the iris boundary. Lastly, the segmented iris is often normalised using Daugman's Rubber Sheet Model, which maps every point in the segmented region from cartesian to polar coordinates. An open source MATLAB implementation based on an updated version of Daugman's approach is a commonly used tool \citep{Percy}. There exist other methods for different circumstances, but Daugman's approach and Hough Transform are the two most commonly used. These methods can also be used on \gls{vl} images if the images are converted from RGB to grey scale images \citep{Bowyer2016}. 

\subsection{Feature Extraction and Classification}
There are multiple ways that the features can be extracted from the segmented iris. The most commonly used in the literature is a 2D Gabor filter which is a linear filter used for edge detection \citep{Daugman1993}. The Hamming distance is then used as a way to classify the iris. The Hamming distance is a measurement of how many bit flips a piece of data needs to undergo to match another piece of data. The bits of the extracted features are then measured against the whole database and the pair with the lowest score is a match. This is called "1-to-N search". As the database gets larger and larger the computation time also grows as it will have to search through the whole database. That's why \cite{Kuehlkamp2016} have suggested using a "1-to-first search" instead to improve the speed of the search. Here a threshold is chosen and as soon as a match has been found below the threshold it will stop the search. Other approaches to the categorisation have been proposed using machine learning. \cite{Khan2017a} proposed using \gls{svm}, \gls{knn} and \gls{lda} with respective test accuracies of 97\%, 95.1\%, 94.28\%. In comparisons the  commercial systems ranged from 94.57\% to  99.67\% with VeriEye having the lowest accuracy and IriCore having the highest accuracy \citep{Khan2017a} . 

\subsection{Deep Learning Methods}
According to \cite{Zhao2017a} research in neural networks within iris recognition is still very new and not much has been done. Some of the work that has been done has used \gls{cnn} and a \gls{dbn}. \cite{Al-Waisy2017} used a common \gls{cnn} architecture to extract features and classify a segmented and normalised iris image. The segmentation was done using Circular Hough Transform (CHT), while normalisation was done using Daugman's rubber sheet method. They named the network IrisConvNet and the architecture was inspired by the Spoofnet as is shown in \autoref{fig:Al_Waisy2017_CNN_model}. This was the general architecture and they tried different numbers of maps and layers to find the best architecture. In general a $3\times3$ input kernal was used on a $64\times64$ or $128\times128$ pixel input image to create feature maps followed by $2\times2$ max pooling, $5\times5$ convolution, $2\times2$ max pooling, $5\times5$ convolution, $2\times2$ max pooling, $5\times5$ convolution, and finally two fully connected layers connected to a softmax regression classifier layer. A ReLU activation function is applied on top of the convolutional and fully connected layers as this results in faster training without sacrificing accuracy. The AdaGrad algorithm was used for training the network. Three databases were used; SDUMLA-HMT, CASIA-Iris-V3 and IIT Dehli (IITD). IrisConvNet scored an accuracy of 99.82\% at categorising CASIA-Iris-V3. 

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/Al_Waisy2017_CNN_model} 
\caption{Example of a \gls{cnn} architecture  from \cite{Al-Waisy2017}}
\label{fig:Al_Waisy2017_CNN_model}
\end{figure}


\cite{Zhao2017a} claim that the problem with traditional networks are that they are database specific and not very generalisable. They created a \gls{fcn} using a loss function they created specifically for iris networks called Extended Triplet Loss (ETL). They claim this network is more generalisable than the previous networks and it shows superior results compared with IrisCode, which is Daugman's segmentation and normalisation approach. A \gls{fcn} differs from a \gls{cnn} in that there are no fully connected layers, only convolutional layers, max pooling etc. \cite{Zhao2017a} proposed an architecture called UniNet, which is shown in \autoref{fig:Zhao2017_CNN_model}. It consists of two \gls{fcn}s; FeatNet and MaskNet. The network takes an iris that has been segmented and normalised using a recent approach \citep{Zhao2015a}. The segmented iris has a resolution of $64\times512$ pixels. The image is then fed through FeatNet and MaskNet. FeatNet extracts the iris features while MaskNet masks the non-iris part of the image. e.g an eyelid that occludes. Three of these UniNet networks are then trained in parallel in Triplet-based network architecture, which uses the ETL loss function. They used four databases to train the networks; ND-IRIS-0405, CASIA V4, IITD and WVU Non-ideal Iris Database. It was then compared with a \gls{cnn} network based on VGG-16 that used softmax, a \gls{cnn} with triplet, FeatNet only, and DeepIrisNet, which is a \gls{cnn} that is proposed directly for iris recognition. The results are shown in \autoref{fig:Zhao2017_CNN_results}, where UniNet outperforms the other nets and FeatNet is the by far worst performing network, which suggests that MaskNet is needed for the network to perform well. 

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/Zhao2017_CNN_model} 
\caption{Example of a \gls{cnn} architecture  from \cite{Zhao2017a}}
\label{fig:Zhao2017_CNN_model}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/Zhao2017_CNN_results} 
\caption{Results from the different networks tested on the ND-IRIS-0405 database. \textit{Ours} is UniNet and FCN + Tripletloss is FeatNet \citep{Zhao2017a}}
\label{fig:Zhao2017_CNN_results}
\end{figure}


Another promising approach by \cite{Bazrafkan2017}, which targets iris segmentation in low-quality consumer images, obtained from smart phones, is using \gls{spdnn} for generating iris maps from low quality iris images.  In \gls{spdnn} several deep networks are merged into a single model. This way it is possible to include different network designs and combine their strengths. They combined four \gls{fcn}s of different architectures, which used a variety of kernel sizes and layers. An example of one of the \gls{fcn}s has 12 convolutional layers in this order; $3\times3$, $5\times5$, $7\times7$, $9\times9$, $11\times11$, $13\times13$, $15\times15$, $13\times13$,$11\times11$, $9\times9$, $7\times7$, $5\times5$, and an output layer that is $3\times3$. It was trained by using the databases Bath800, CASIA Thousand, UBIRIS and MobBio. They contained both \gls{nir} and \gls{vl} images. Extensive noise was also added to the training images in the form of blur and lower contrast to generate degraded versions of the high quality images and utilise the databases better by creating more samples. When applied on the UBIRIS database, in order to compare with other state of the art segmentation techniques, it achieved the highest accuracy of 99.30\% among all of the techniques, which all achieved accuracies of at least 98.10\%.\\

Both face and iris are commonly used individually for identification. However, the combination of iris and face in one classification system has been subject to a great amount of attention among researchers during the last decade \citep{Al-Waisy2017a}. Section~\ref{sec:info_fuse} investigates and outlines methods used for fusion of  information, which is necessary for creating multi modal biometric systems.  









%\cite{Rifaee2017} gives an outline of commercially free databases often used in research.\autoref{fig:Iris_database_1} and \autoref{fig:Iris_database_2} show the different contents of the databases. Most of the databases are \gls{nir} images with CASIA being one most used databases in the literature.  For images in the visible light UBRIS is often used . These contain more "real world" data as the images contain more noise in the form of eyelids obstruction, eyelash obstruction, glare, motion blur, out-of-focus or poor focused iris, partial iris and spec- ular reflection \citep{Rattani2017}. The database has also been used in Noisy Iris Challenge Evaluation (NICE) I. For iris recognition system on mobile platforms the Mobile Iris CHallenge Evaluation (MICHE) database can be used.  The database contains noisy images taken withe Galaxy Samsung IV, Iphone5 and Galaxy Tablet II. The noise includes noise from both artificial and natural light sources during acquisition, motion blur, occlusion due to eyelids/glasses/eyelashes/hair/shadows which can naturally occur when a user is trying to unlock a phone using their iris. 


%. It was also the foundation for IrisCode which is a commercially developed iris recognition algorithm by John Daugman. In 2016 a handbook for iris recognition by \cite{Bowyer2016b} was published giving an outline of the whole process of iris recognition. In general Near Infra-Red (NIR) images of an iris are used but other type of images can also be used.  \cite{Khan2017a} show how to use Daugmans methodology on iris images taken with a smartphone in visible light. They use Daugmans Integro-differential operator to localise the bounds of the iris. Then they suppress the eyelids the eyelids which often cover parts of the iris by using an approach inspired by Masek. Afterwards the image is normalised by using the homogeneous rubber sheet model by Daugman. Then eyelashes are removed from the image and feature extraction is done by using 2D Gabor Waveletts. This approach to extract features in one way or another can be seen in multiple state of the art iris recognition systems and research papers e.g \citep{Luhadiya2017a,Uka2017a,Kuehlkamp2016a}, \cite{Kuehlkamp2016a}. After the features have been extracted they are compared/categorised. Traditionally a "1-to-N search" has been done. Here the Hamming distance of features of the scanned iris are compared to the whole database and it is classified with the iris that has the least distance. \cite{Kuehlkamp2016a} have suggested using a "1-to-first search" instead to improve the speed of the search. Here a threshold is chosen and as soon as a match has been found below the threshold it will stop the search. Other approaches to the categorisation have been proposed using machine learning. \cite{Khan2017a} proposed using Support Vector Machines (SVM), K-Nearest-Neighbours(KNN) and Linear Discriminant Analysis (LDA) with respective test accuracies of 97\%, 95.1\%, 94.28\%. \cite{Zhao2017b} proposed using deep learning to classify. They claim that research in neural networks with iris recognition is still very new and not much has been done. But some of the work that has been done have used Convolutional Neural Networks (CNN) and a Deep Belief Network (DBN). They claim that the problem with traditional networks are that they are very database specific on not very generalisable. \cite{Zhao2017b} created a Fully Convolutional Network (FCN) using a loss function they created specifically for iris networks called Extended Triplet Loss (ETL). They claim this network is more generalisable than the previous networks. 




