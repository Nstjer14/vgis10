\section{Network Fusion}
To be able to combine iris and face verification, a combination of the results of the two networks is made.  This is done by having two individual streams that are merged at their last \gls{fc} layer by concatenating the layers along their longest dimension. The merged layer can then be fed into other \gls{fc} layers and further to a classification layer. This is done using a softmax layer after the \gls{fc} layer \citep{Eitel2015}. This should in theory increase the accuracy in ID verification in a potential instance where verification is required. The architecture used is shown in \autoref{fig:net_fusion}. The two \gls{fc} has 5020 neurons. This is to have the same dimensions as the concatenated layer. To see if the new merged structure performs better than the iris and face \gls{cnn}s separately, the same hyper parameters are used as earlier in this chapter.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{merged_net_overview}
	\caption{Overview of how the two networks are fused}
	\label{fig:net_fusion}
\end{figure}

The parameters set for classification in the fused network are shown in \autoref{tab:merged_net}

\begin{table}[h]
	\centering
	\caption{Parameters set for the classification in the fused network}
	\label{tab:merged_net}
	\begin{tabular}{lrrrr}
		\textbf{Layer Type} & \textbf{Feature Map Size} & \textbf{Kernel/Pool Size} & \textbf{Activation} & \textbf{Other} \\ \hline
		\multicolumn{5}{l}{Iris Recognition and Face Recognition Networks Outputs}                                        \\
		\rowcolor{lightGrey}
		Concatenate         &                           &                           &                     &                \\
		Dense               & $5120$                    &                           & ReLU                &                \\
		\rowcolor{lightGrey} 
		Dropout             &                           &                           &                     & $0.5$          \\
		Dense               & $5120$                    &                           & ReLU                &                \\
		\rowcolor{lightGrey} 
		Dropout             &                           &                           &                     & $0.5$          \\
		Dense               & Amount of Classes         &                           &  Softmax            &               
	\end{tabular}
\end{table}

\clearpage
\subsection{Multimodal Database}
The fusion net should be trained with a multimodal biometric database. Ideally the database used in this project should consist of face and iris images obtained from the same subjects captured with the same camera complying with the requirements set in \autoref{ch:req}. 

However, even though literature suggests that chimeric databases are less adequate than genuine multimodal biometric databases, the multimodal database used during the work with information fusion is synthetically created. As \autoref{sec:multi_modal_data} mentions there are only a limited amount of multimodal biometric databases available containing both iris and face data, and, to the extend of the knowledge gained from the research presented in \autoref{sec:info_fuse}, there is only one of the databases, which is obtained using mobile devices namely the MobBio database. For this database the Asus Transformer Pad TF 300T is used, which has a camera of 8MP, and is thus comparable to the iPhone 5s used for the caption of the Warsaw-BioBase used for the iris identification methods presented in \autoref{BasicM} and \autoref{sec:cnn_iris_rec} \citep{Sequeira2014}. However, despite several attempts of contacting the authors of the database it was not possible to establish communication or gain access to the database. Therefore the available way to obtain a multimodal database with mobile device images is to create it synthetically. Furthermore as the goal is to compare the performance of the fused \gls{cnn}s with the individual \gls{cnn}s on the face and iris data respectively, it is desirable to test on the same data. Therefore a databased was created by combining the \gls{lfw} and the Warsaw-BioBase databases. 

The database is created by combining iris classes arbitrarily with an face class for as many classes as there are classes available from both the iris and the face dataset. Before combining the datasets the classes with ten or less samples are discarded. The new samples are made by giving the iris image and the face image the same label. The new samples are then data augmented as described in \autoref{sec:cnn_iris_rec}

\subsection{Results}
The data augmented chimeric database consists of 20640 samples across 91 classes. 70\% is used for training, 15\% for validation and 15\% for testing. The network is trained for 140 epochs and achieved a test accuracy of 81.17\%

\begin{figure}[H]
	\centering
	\begin{subfigure}{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{merged_acc_81_17_acc}
		\caption{Face recognition \gls{cnn} accuracy progression through epochs}
		\label{fig:merged_acc}
	\end{subfigure}
	\begin{subfigure}{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{merged_acc_81_17_loss}
		\caption{Face recognition \gls{cnn} loss progression through epochs}
		\label{fig:merged_loss}
	\end{subfigure}
	\caption{Accuracy and loss progression for the merged \gls{cnn}. Achieved 81.17\% accuracy on the test set}
	\label{fig:merged_graphs}
\end{figure}

\noindent As the merged \gls{cnn} using the chimeric data performs worse by almost 20\% something is wrong. To investigate how the separate iris and face \gls{cnn}s would perform on the chimeric data a second test is made. This is done by discarding either the iris or the face respectively from the generated sample. This produced the following result with the same 70, 15, 15 split.  The iris \gls{cnn} achieves 68.94\% accuracy on the test set, \autoref{fig:iris_cnn_68_graphs}. The VGG face \gls{cnn} achieves 77.78\% accuracy on the test set, \autoref{fig:vgg_77_graphs}. When all networks use the same data, the merged network seems to outperform both the VGG face \gls{cnn} and the iris \gls{cnn} although they both achieve above 99\% accuracy when trained on the data before the it is fused to the chimeric data.

\begin{figure}[H]
	\centering
	\begin{subfigure}{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{iris_cnn_bad_68_94_acc}
		\caption{Iris \gls{cnn} accuracy progression through epochs}
		\label{fig:iris_cnn_68_acc}
	\end{subfigure}
	\begin{subfigure}{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{iris_cnn_bad_68_94_loss}
		\caption{Iris \gls{cnn} loss progression through epochs}
		\label{fig:iris_cnn_68_loss}
	\end{subfigure}
	\caption{Accuracy and loss progression for the iris \gls{cnn} trained on the chimeric data. Achieved 68.94\% accuracy on the test set}
	\label{fig:iris_cnn_68_graphs}
\end{figure}


\begin{figure}[H]
	\centering
	\begin{subfigure}{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{vgg16_bad_77_88_acc}
		\caption{Face recognition \gls{cnn} accuracy progression through epochs}
		\label{fig:vgg_77_acc}
	\end{subfigure}
	\begin{subfigure}{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{vgg16_bad_77_88_loss}
		\caption{Face recognition \gls{cnn} loss progression through epochs}
		\label{fig:vgg_77_loss}
	\end{subfigure}
	\caption{Accuracy and loss progression for the VGG face \gls{cnn} trained on the chimeric data. Achieved 77.78\% accuracy on the test set }
	\label{fig:vgg_77_graphs}
\end{figure}

